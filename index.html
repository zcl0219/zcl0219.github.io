<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/avatar-32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/avatar-16.png">
  <link rel="mask-icon" href="/images/emoji-smile.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Code:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"zcl0219.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":null,"show_result":true},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Doing the tough things sets winners apart from losers">
<meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://zcl0219.github.io/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Doing the tough things sets winners apart from losers">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="GGBond">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://zcl0219.github.io/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Hexo</title>
  







<link href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap" rel="stylesheet">

  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Hexo</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="GGBond"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">GGBond</p>
  <div class="site-description" itemprop="description">Doing the tough things sets winners apart from losers</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">33</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="GGBond">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="Doing the tough things sets winners apart from losers">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/06/22/Multi-Agent-AI/" class="post-title-link" itemprop="url">Multi-Agent AI</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-06-22 21:04:54" itemprop="dateCreated datePublished" datetime="2025-06-22T21:04:54+08:00">2025-06-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-07-06 21:51:08" itemprop="dateModified" datetime="2025-07-06T21:51:08+08:00">2025-07-06</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <ol>
<li><p>引言</p>
<p>q1：多智能体强化学习系统特点</p>
<p>智能体数量非单个</p>
<p>智能体之间存在一定联系：合作、对抗、合作+对抗（多智能体多动机）</p>
<p>智能体不仅需要考虑到环境，还需要考虑到其他智能体的行动</p>
<p>学习环境动态变化</p>
</li>
<li><p>博弈论</p>
<p>minmax</p>
<p>nash equilibrium</p>
<p><img src="image-20250622214102079.png" alt=""></p>
<p><img src="image-20250622215001166.png" alt="1-2"></p>
<p><img src="image-20250622215250842.png" alt="1-3"></p>
<p>上图说明了制定出合理的策略需要智能体之间communication</p>
<p><img src="image-20250622215526657.png" alt="1-4"></p>
<p>局部最优并不代表全局最优</p>
<p><img src="image-20250622221246041.png" alt="1-5"></p>
<p><img src="image-20250622221913906.png" alt="1-6"></p>
<p>对彼此的最佳对策被称为纳什均衡</p>
<p><img src="image-20250622222258083.png" alt="1-7"></p>
<p>纳什均衡可以有多个</p>
<p><img src="image-20250622222540677.png" alt="1-8"></p>
<p>博弈定义</p>
<p><img src="image-20250622223016230.png" alt="1-9"></p>
<p>占优策略：对于一个玩家来说，拥有一个策略a，不论其他玩家选择任何策略，选择策略a都会使得他的效用u最大，这个策略a就被称为占优策略</p>
<p><img src="image-20250622223222044.png" alt="1-10"></p>
<p>n-玩家纳什均衡定义</p>
<p>q2：是不是所有博弈情况都存在纳什均衡？</p>
</li>
<li><p>混合策略纳什均衡</p>
<p><img src="image-20250622225345099.png" alt="1-11"></p>
</li>
</ol>
<h2 id="2-1-重复博弈"><a href="#2-1-重复博弈" class="headerlink" title="2.1 重复博弈"></a>2.1 重复博弈</h2><p><img src="image-20250623201854412.png" alt="2-1"></p>
<ul>
<li><p>为什么要研究重复博弈？</p>
<p>生活中我们参与的许多战略互动都是持续进行的，比如，我们会与相同的人重复互动等。重复博弈理论提供了一个框架用以研究这种重复行为</p>
</li>
</ul>
<p><img src="image-20250623202222337.png" alt="2-2"></p>
<ul>
<li>重复博弈定义：同一个基础博弈（阶段博弈）被相同的参与者多次重复进行。换句话说，在重复博弈中，一个标准式博弈被同样的参与者反复进行。</li>
</ul>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623202435065.png" alt="image-20250623202435065"></p>
<ul>
<li><p>惩罚的威胁：</p>
<p>惩罚的威胁是理解重复博弈引入折扣因子的关键概念。基本思想是参与者可能会因为<strong>“惩罚”的“威胁”</strong>而被阻止利用其短期优势，这种威胁的直观效果就是会降低其长期收益。</p>
<p>通过未来惩罚的威胁来维持合作。参与者在做决策时不仅要考虑当前收益，还要考虑背叛行为可能招致的未来惩罚，从而在长期利益考量下选择合作而非短期的机会主义行为。</p>
</li>
</ul>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623202916496.png" alt="image-20250623202916496"></p>
<ul>
<li><p>重复博弈：囚徒困境</p>
<p>在单次博弈中，背叛是占优策略，但在重复博弈中，未来惩罚的威胁可能使合作成为可能。这为分析现实中的长期合作关系提供了理论基础。</p>
</li>
</ul>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623203105804.png" alt="image-20250623203105804"></p>
<ul>
<li><p>囚徒困境有一个唯一的纳什均衡：每个参与者都选D（背叛）</p>
</li>
<li><p>现在引入冷酷触发策略：</p>
<ul>
<li><p>只要对方参与者选择C，就选择C；</p>
</li>
<li><p>如果在任何时期对方参与者选择D，那么在<strong>此后的每个时期</strong>都选择D</p>
</li>
</ul>
</li>
<li><p>另一个参与者应该怎么做？</p>
<ul>
<li>只要她对未来收益的重视程度与当前收益相比不是太小，她最好在每个时期都选择C</li>
</ul>
</li>
<li><p>冷酷触发策略的工作原理：通过”一旦背叛，永远惩罚”的威胁来维持合作。关键在于参与者对未来收益的重视程度（贴现因子）必须足够高，使得长期合作的收益超过短期背叛的收益。</p>
</li>
</ul>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623203455003.png" alt="image-20250623203455003"></p>
<ul>
<li>每次都选择C的策略是应对冷酷触发策略最好的策略</li>
<li>为什么？<ul>
<li>如果她在每个时期都选择C，那么每个时期的结果都是(C,C)，她在每个时期获得收益2</li>
<li>如果她在某个时期转向D，那么她在该时期获得收益3，在此后的每个时期获得收益1</li>
<li>然而，只要她对未来收益的重视程度与当前收益相比不是太小，收益流(3,1,1,…)对她来说比收益流(2,2,2,…)更差 • 因此她最好在每个时期都选择C</li>
</ul>
</li>
<li>为什么冷酷触发策略能够维持合作：虽然背叛能带来一次性的更高收益（3 vs 2），但随后的永久惩罚（每期收益1）使得总体收益低于持续合作（每期收益2）。关键条件是参与者必须足够重视未来收益。</li>
</ul>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623203658761.png" alt="image-20250623203658761"></p>
<ul>
<li>这张幻灯片指出了重复博弈中的多重均衡问题。除了通过冷酷触发策略维持的合作均衡外，还存在”总是背叛”的均衡。在这种均衡中，由于对方无论如何都会背叛，自己也最好选择背叛，这与单次囚徒困境的结果相同。这说明重复博弈可能存在多个纳什均衡，既有合作的也有非合作的。</li>
</ul>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623203835329.png" alt="image-20250623203835329"></p>
<ul>
<li><p>两个关键问题：</p>
<ul>
<li><p><strong>耐心程度的量化</strong>：要维持合作均衡，参与者需要多重视未来收益？这涉及贴现因子的临界值计算。</p>
</li>
<li><p><strong>均衡结果的多样性</strong>：除了完全合作(C,C)和完全背叛(D,D)之外，还有哪些策略组合和结果可以构成纳什均衡？</p>
</li>
</ul>
</li>
</ul>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623204324562.png" alt="image-20250623204324562"></p>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623204526705.png" alt="image-20250623204526705"></p>
<ul>
<li>通过冷酷触发策略结合折扣因子来解释折扣因子的边界取值。分别是一直选择合作（C）以及中途换选择（D），分别计算其策略收益，最后比较收益值，即可计算出折扣因子的边界值。</li>
<li><strong>当δ &lt; 1/2时，”一直背叛”是纳什均衡</strong>：<ul>
<li>无论δ值如何，”一直背叛”策略<strong>总是纳什均衡</strong>：<ul>
<li>如果玩家1总是背叛，玩家2的最佳响应是总是背叛（因为如果玩家2合作，支付为0；如果背叛，支付为1）。</li>
<li>同样，如果玩家2总是背叛，玩家1的最佳响应也是总是背叛。</li>
<li>支付为每期(1,1)，现值为11−δ1−<em>δ</em>1。</li>
<li>没有玩家能通过单方面改变策略（如尝试合作）获得更高支付，因为合作会被立即剥削（支付0），且未来收益折现后不足以补偿。</li>
</ul>
</li>
<li>这个均衡总是存在，但它导致<strong>帕累托低效的结果</strong>（支付(1,1)低于合作时的(2,2)）。</li>
</ul>
</li>
</ul>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623204705478.png" alt="image-20250623204705478"></p>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623204723342.png" alt="image-20250623204723342"></p>
<ul>
<li>有限步惩罚策略与“以牙还牙”策略计算折扣因子边界值思想与冷酷触发策略计算类似，这里不再详细解释。</li>
</ul>
<h2 id="2-2-扩展形式博弈"><a href="#2-2-扩展形式博弈" class="headerlink" title="2.2 扩展形式博弈"></a>2.2 扩展形式博弈</h2><p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623210315029.png" alt="image-20250623210315029"></p>
<ul>
<li><p>上述内容对比了策略型博弈和扩展式博弈的核心区别，并定义了一种特定类型的扩展式博弈<strong>（多阶段可观测行动博弈）</strong>。以下是我对关键点的理解：</p>
<ol>
<li><p><strong>核心区别 (Sequentiality &amp; Information):</strong></p>
<ul>
<li><strong>策略型博弈 (战略式博弈):</strong> 强调<strong>同时决策</strong>。玩家在不知道对手选择的情况下做出一次性决策（如石头剪刀布、静态 Cournot 模型）。收益矩阵是其主要表示形式。</li>
<li><strong>扩展式博弈 (扩展式):</strong> 强调<strong>行动的先后顺序 (序列性)</strong> 和<strong>信息结构</strong>。玩家在不同时间点行动，并且后行动的玩家可能（但不一定）能观察到先行动玩家的选择（如象棋、动态 Stackelberg 模型、序贯议价）。博弈树是其核心表示工具。</li>
</ul>
</li>
<li><p><strong>关注类型 (Multi-stage with Observed Actions):</strong></p>
<ul>
<li><strong>多阶段 (Multi-stage):</strong> 博弈过程被划分为不同的阶段。</li>
<li><strong>可观测行动 (Observed Actions):</strong> 这是定义中<strong>最关键的信息假设</strong>。它意味着在每个阶段开始时，<strong>所有玩家都完全知道之前所有阶段中所有玩家选择的所有行动</strong>。这被称为<strong>完美信息 (Perfect Information)</strong>，但PPT的表述更一般化，因为它允许同一阶段内的玩家<strong>同时行动 (Simultaneous Moves)</strong>。</li>
<li><strong>完美信息 vs. 可观测行动：</strong> “完美信息”通常指在<em>每个决策点</em>，玩家确切知道之前发生的<em>所有</em>行动（即知道整个历史，知道当前处于哪个决策节点）。PPT定义的“具有可观测行动的多阶段博弈”在阶段之间是完美信息的（玩家知道之前所有阶段的所有行动），但在一个阶段内部，如果存在同时行动，则在该阶段内行动时，玩家可能不知道同阶段其他玩家的<em>即时</em>选择（但在下一阶段开始前，这些行动会被揭示）。这比严格的“完美信息”博弈（要求每个决策点都无同时行动且完全知晓历史）范围更广。</li>
</ul>
</li>
<li><p><strong>表示工具 (Game Trees):</strong></p>
<ul>
<li>博弈树是表示扩展式博弈最直观的方式。树根代表起点，树枝代表玩家可能的行动，树节点代表决策点（轮到哪个或哪些玩家行动），树叶代表终点（对应收益/结果）。</li>
<li>它天然地刻画了<strong>行动顺序</strong>和可能的<strong>路径 (历史)</strong>。</li>
</ul>
</li>
<li><p><strong>关键概念 (Histories):</strong></p>
<ul>
<li><strong>历史记录 (Histories)</strong> 是扩展式博弈模型中的<strong>基础构件</strong>。一个历史就是一个从博弈开始到某个时间点为止，所有玩家按顺序采取的行动的序列。</li>
<li>每个决策点（博弈树节点）都对应一个<strong>唯一的历史</strong>，该历史描述了到达该节点所经过的路径。</li>
<li>玩家的<strong>信息集 (Information Sets)</strong> 通常由其无法区分的一组历史（节点）来定义。但在PPT定义的“具有可观测行动的多阶段博弈”中，由于行动完全可观测，每个信息集通常只包含一个节点（即玩家总是确切知道自己处于哪个决策点），除非在同一个阶段内存在同时行动（此时玩家可能不知道同阶段对手的<em>即时</em>选择）。</li>
</ul>
</li>
</ol>
</li>
</ul>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623211203839.png" alt="image-20250623211203839"></p>
<ul>
<li><p><strong>玩家角色：</strong></p>
<ul>
<li><strong>玩家1 - 进入者 (Entrant):</strong> 考虑是否进入一个新市场（通常是一个已有在位企业的市场）。</li>
<li><strong>玩家2 - 在位者 (Incumbent):</strong> 是市场现有的主导企业，对进入者的行动做出反应。</li>
</ul>
</li>
<li><p><strong>行动顺序与信息：</strong></p>
<ul>
<li>这是一个<strong>序贯博弈</strong>：玩家1先行动，玩家2后行动。</li>
<li><strong>关键信息假设：</strong> 玩家2在做出决策（容纳还是斗争）之前，<strong>完全观察到了玩家1的选择</strong>（进入或不进入）。这意味着玩家2知道博弈进行到了哪个决策点（即玩家1选了哪个行动）。</li>
<li>这符合之前定义的“<strong>具有可观测行动</strong>”的多阶段博弈。在这里，只有一个阶段玩家1行动，紧接着一个阶段玩家2行动，且玩家2的行动是基于完全知晓玩家1行动的情况下做出的。</li>
</ul>
</li>
<li><p><strong>博弈树表示：</strong></p>
<ul>
<li>这个例子非常适合用<strong>博弈树</strong>来表示：<ul>
<li><strong>根节点 (Root):</strong> 玩家1的决策点（进入 / 不进入）。</li>
<li><strong>中间节点 (Decision Nodes):</strong> 玩家1选择“进入”后，会到达玩家2的决策点（容纳 / 斗争）。玩家1选择“不进入”后，博弈直接结束。</li>
<li><strong>叶节点 / 终点节点 (Terminal Nodes / Leaves):</strong> 代表博弈结束的点，标有收益向量 ((x, y))。每个叶节点对应一个<strong>完整的历史</strong>（行动序列）和最终的收益结果。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>收益：</strong></p>
<ul>
<li>收益 ((x, y)) 的具体数值决定了博弈的结果和均衡。虽然没有给出具体数字，但典型的设定可能是：<ul>
<li><code>(不进入)</code>: 进入者收益为0（无成本无收入），在位者收益较高（维持垄断利润）。</li>
<li><code>(进入, 容纳)</code>: 进入者获得正利润（但低于垄断利润），在位者利润下降（但仍为正，因为共享市场）。</li>
<li><code>(进入, 斗争)</code>: 进入者亏损（因在位者发起价格战等），在位者也亏损（价格战成本）。虽然进入者损失可能更大，但斗争通常对双方都不利，是两败俱伤的结果。</li>
</ul>
</li>
<li>玩家2（在位者）的决策取决于哪种行动（容纳或斗争）在给定玩家1已进入的前提下，能给他带来更高的收益 (y)。</li>
</ul>
</li>
<li><p>要分析这个博弈的均衡（例如，子博弈精炼纳什均衡），就需要具体设定收益值，并逆向归纳求解玩家2在观察到“进入”后的最优反应，以及玩家1预测到玩家2的最优反应后，最初是否选择“进入”。</p>
</li>
</ul>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623212809384.png" alt="image-20250623212809384"></p>
<ul>
<li>扩展型博弈的一些基本符号定义</li>
</ul>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623213442013.png" alt="image-20250623213442013"></p>
<ul>
<li><p><strong>纯策略 (Pure Strategy):</strong></p>
<ul>
<li>在扩展式博弈中，纯策略不仅仅是玩家在博弈开始时的一个单一选择。它是一个<strong>完整的行动计划</strong>。</li>
<li>策略必须规定玩家在<strong>博弈的每一个可能阶段 (k)</strong>，面对<strong>每一个可能到达该阶段的历史路径 (hᵏ ∈ Hᵏ)</strong> 时，他会选择哪个可用的行动 (sᵢᵏ(hᵏ) ∈ Sᵢ(hᵏ)。</li>
<li><strong>为什么需要这么复杂？</strong> <ul>
<li>因为博弈是序贯的，并且玩家在决策时可能面临不同的局面（由不同的历史 hᵏ 描述）。一个完整的策略必须说明玩家在 <em>所有可能遇到的情况</em> 下会怎么做，即使某些情况在博弈实际进行中可能不会发生（如果玩家遵循这个策略的话）。</li>
</ul>
</li>
<li>形式上，玩家 i 的策略 sᵢ 是一个<strong>函数集合</strong> {sᵢ⁰, sᵢ¹, …, sᵢᴷ}，其中每个函数 sᵢᵏ 将阶段 k 的<em>历史集合 Hᵏ</em> 映射到该玩家在该历史下<em>可用的行动集合 Sᵢ(Hᵏ)</em> 中的一个具体行动。</li>
</ul>
</li>
<li><p><strong>策略组合生成博弈路径:</strong></p>
<ul>
<li>当所有玩家都选定他们的纯策略 (s₁, s₂, …, sₗ) 后，博弈的实际进行路径就被唯一确定了。</li>
<li>路径是通过<strong>递归应用</strong>所有玩家的策略函数来生成的：<ul>
<li><strong>阶段 0:</strong> 从初始历史 h⁰ = ∅ 开始。所有玩家根据他们的策略 sᵢ⁰(∅) 选择行动，形成行动组合 a⁰ = (s₁⁰(∅), s₂⁰(∅), …, sₗ⁰(∅))。阶段 0 后的历史变为 h¹ = a⁰。</li>
<li><strong>阶段 1:</strong> 面对历史 h¹ = a⁰。所有玩家根据他们的策略 sᵢ¹(a⁰) 选择行动，形成行动组合 a¹ = (s₁¹(a⁰), s₂¹(a⁰), …, sₗ¹(a⁰))。阶段 1 后的历史变为 h² = (a⁰, a¹)。</li>
<li><strong>后续阶段:</strong> 以此类推，直到最终阶段 K。阶段 K 后的历史 hᴷ⁺¹ = (a⁰, a¹, …, aᴷ) 就是终端历史。</li>
</ul>
</li>
<li>策略组合 s <strong>完全决定了</strong>终端历史 hᴷ⁺¹。</li>
</ul>
</li>
<li><p><strong>收益 (Payoffs):</strong></p>
<ul>
<li>玩家的收益取决于博弈的最终<strong>结果 (outcome)</strong>，即终端历史 hᴷ⁺¹。</li>
<li>每个玩家 i 有一个<strong>效用函数 (utility function)</strong> uᵢ，该函数将每个可能的终端历史 hᴷ⁺¹ 映射到一个实数，表示玩家 i 在该结果下获得的收益（或效用）。</li>
<li>由于策略组合 s 决定了终端历史 hᴷ⁺¹，因此我们也可以说策略组合 s 决定了每个玩家的收益，记作 uᵢ(s)。uᵢ(s) 本质上是 uᵢ 在由 s 决定的那个特定终端历史 hᴷ⁺¹ 上的取值。</li>
</ul>
</li>
<li><p>扩展式博弈中的核心概念：</p>
<ul>
<li><strong>纯策略:</strong> 是玩家针对<strong>所有可能历史</strong>制定的完整应变计划，表现为一组映射函数 {sᵢᵏ}，每个函数为特定阶段 k 的每个可能历史 hᵏ 指定一个行动。</li>
<li><strong>策略组合决定路径:</strong> 当所有玩家选定策略后，博弈的路径（行动序列 a⁰, a¹, …, aᴷ）和最终的终端历史 hᴷ⁺¹ 就被策略函数递归地、确定性地生成。</li>
<li><strong>收益定义在结果上:</strong> 玩家的收益由终端历史（博弈的最终结果）决定。效用函数 uᵢ 量化了玩家对每个可能结果的偏好。策略组合 s 通过决定终端历史 hᴷ⁺¹ 来间接决定每个玩家的收益 uᵢ(s)。</li>
</ul>
</li>
<li><p>理解纯策略的这种“完备应变计划”性质对于分析扩展式博弈的均衡（如子博弈精炼纳什均衡）至关重要，因为它要求玩家即使在“偏离均衡路径”的历史下（即如果博弈意外地到达了那里），也要指定一个行动。</p>
</li>
</ul>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623214724248.png" alt="image-20250623214724248"></p>
<ul>
<li><strong>关键总结 (PPT最后一句):</strong></li>
</ul>
<blockquote>
<p>一个玩家的策略规定了该玩家在其<strong>轮到行动的每一个历史</strong>（例如，玩家2在<code>&#123;C&#125;</code>之后或<code>&#123;D&#125;</code>之后）处所选择的行动。</p>
</blockquote>
<ul>
<li>玩家 1 的策略只需要规定在唯一的历史 <code>∅</code> 处选择 <code>C</code> 或 <code>D</code>。</li>
<li>玩家 2 的策略必须规定在 <em>两个</em> 可能的历史 <code>&#123;C&#125;</code> 和 <code>&#123;D&#125;</code> 处分别选择什么行动（<code>E/F</code> 和 <code>G/H</code> 的组合）。</li>
</ul>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624002522709.png" alt="image-20250624002522709"></p>
<ul>
<li>在给定的扩展式博弈及其转换后的标准式（策略型）博弈中，策略组合（L, RL）不是纳什均衡（Nash equilibrium）。以下基于提供的收益矩阵和纳什均衡的定义，逐步解释原因。</li>
</ul>
<ul>
<li><p><strong>博弈的收益矩阵</strong>：</p>
<p>| 玩家1 \ 玩家2 | LL   | LR   | RL   | RR   |<br>| ——————- | —— | —— | —— | —— |<br>| <strong>L</strong>         | 3,2  | 3,2  | 2,3  | 2,3  |<br>| <strong>R</strong>         | 4,1  | 0,1  | 4,1  | 0,1  |</p>
</li>
<li><p><strong>玩家2的策略含义</strong>（RL策略）：</p>
<ul>
<li>RL = 如果玩家1选择L，则玩家2选择R；如果玩家1选择R，则玩家2选择L。</li>
</ul>
</li>
<li><p><strong>策略组合（L, RL）的含义</strong>：</p>
<ul>
<li>玩家1选择L。</li>
<li>玩家2选择RL策略（因此，当玩家1选L时，玩家2选R）。</li>
<li>实际发生的行动路径：玩家1选L → 玩家2选R。</li>
<li>对应收益：从收益矩阵中，行L、列RL的单元格为（2,3），即玩家1收益为2，玩家2收益为3。</li>
</ul>
</li>
<li><h3 id="纳什均衡的定义"><a href="#纳什均衡的定义" class="headerlink" title="纳什均衡的定义"></a>纳什均衡的定义</h3><ul>
<li>纳什均衡要求：在给定其他玩家的策略下，没有任何一个玩家能通过单方面改变自己的策略而获得更高的收益。也就是说：</li>
<li>玩家1的策略必须是对玩家2策略的最优响应（best response）。</li>
<li>玩家2的策略必须是对玩家1策略的最优响应。<br>如果任何一个玩家有激励偏离当前策略，则该组合不是纳什均衡。</li>
</ul>
</li>
<li><h3 id="为什么（L-RL）不是纳什均衡？"><a href="#为什么（L-RL）不是纳什均衡？" class="headerlink" title="为什么（L, RL）不是纳什均衡？"></a>为什么（L, RL）不是纳什均衡？</h3><p>在策略组合（L, RL）下，玩家1有激励单方面改变策略。具体分析如下：</p>
</li>
</ul>
<ol>
<li><p><strong>给定玩家2的策略RL，玩家1的收益比较</strong>：</p>
<ul>
<li>如果玩家1保持选择L（当前策略）：<ul>
<li>玩家2的RL策略规定：当玩家1选L时，玩家2选R。</li>
<li>因此，收益为（2,3），玩家1获得收益<strong>2</strong>。</li>
</ul>
</li>
<li>如果玩家1单方面改为选择R：<ul>
<li>玩家2的RL策略规定：当玩家1选R时，玩家2选L（因为RL策略在玩家1选R时对应选L）。</li>
<li>因此，行动路径为（R, L），收益为（4,1）（从收益矩阵的行R、列RL单元格可得）。</li>
<li>玩家1获得收益<strong>4</strong>。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>玩家1的激励分析</strong>：</p>
<ul>
<li>玩家1的收益从<strong>2</strong>（选L）变为<strong>4</strong>（选R），收益增加（4 &gt; 2）。</li>
<li>因此，玩家1有严格激励（strict incentive）偏离策略L，改为选择R。因为收益更高，且这是单方面改变（玩家2的策略RL保持不变）。</li>
</ul>
</li>
<li><p><strong>玩家2的响应（虽不必要，但完整性分析）</strong>：</p>
<ul>
<li>在（L, RL）下，玩家2的收益为3（给定玩家1选L，玩家2选R）。</li>
<li>如果玩家1保持选L，玩家2改变策略（如改为LL、LR或RR）：<ul>
<li>例如，改为LL：当玩家1选L时，玩家2选L，收益为2（行L、列LL单元格为（3,2），玩家2收益2 &lt; 3）。</li>
<li>改为LR：当玩家1选L时，玩家2选L，收益为2（行L、列LR单元格为（3,2），玩家2收益2 &lt; 3）。</li>
<li>改为RR：当玩家1选L时，玩家2选R，收益为3（与当前相同）。</li>
<li>因此，玩家2无严格激励偏离RL（因为改变策略要么收益降低，要么不变），但这不是关键，因为玩家1的偏离已足够破坏均衡。</li>
</ul>
</li>
</ul>
</li>
</ol>
<ul>
<li><h3 id="为什么PPT中列出的（R-RL）是纳什均衡，而（L-RL）不是？"><a href="#为什么PPT中列出的（R-RL）是纳什均衡，而（L-RL）不是？" class="headerlink" title="为什么PPT中列出的（R, RL）是纳什均衡，而（L, RL）不是？"></a>为什么PPT中列出的（R, RL）是纳什均衡，而（L, RL）不是？</h3><ul>
<li><p><strong>（R, RL）是纳什均衡</strong>（如PPT所列）：</p>
<ul>
<li>玩家1选R，玩家2选RL（当玩家1选R时，玩家2选L）。</li>
<li>收益为（4,1）。</li>
<li>给定玩家2的RL策略，玩家1：选R收益4，选L收益2（因为如果玩家1选L，玩家2选R，收益2），4 &gt; 2，因此玩家1无激励偏离。</li>
<li>给定玩家1选R，玩家2：任何改变（如改为LL、LR或RR），在玩家1选R时收益均为1（无严格增加），因此玩家2无激励偏离。</li>
<li>所以（R, RL）满足纳什均衡条件（弱均衡）。</li>
</ul>
</li>
<li><p><strong>（L, RL）不是纳什均衡</strong>：</p>
<ul>
<li>如上所述，玩家1有严格激励偏离L到R，因此不满足“无玩家有激励偏离”的条件。</li>
</ul>
</li>
</ul>
</li>
<li><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li><strong>（L, RL）不是纳什均衡，因为玩家1可以通过单方面改变策略（从L到R）将收益从2提高到4。</strong> 给定玩家2的RL策略，玩家1选择L不是最优响应。</li>
<li>这体现了纳什均衡的核心要求：每个玩家的策略必须是对其他玩家策略的最优响应。在扩展式博弈中，这种分析也揭示了为什么有些策略组合在标准式中看似可行，但因动态不一致而被排除（如这里玩家1的偏离激励）。</li>
</ul>
</li>
</ul>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624003714695.png" alt="image-20250624003714695"></p>
<p>这张PPT探讨了进入博弈的均衡合理性，通过扩展式和标准式表示揭示了纳什均衡的局限性。以下是详细分析：</p>
<hr>
<h3 id="博弈结构与收益"><a href="#博弈结构与收益" class="headerlink" title="博弈结构与收益"></a>博弈结构与收益</h3><h4 id="扩展式表示（博弈树）："><a href="#扩展式表示（博弈树）：" class="headerlink" title="扩展式表示（博弈树）："></a>扩展式表示（博弈树）：</h4><ul>
<li><strong>玩家1（进入者）</strong>：先行动，选择：<ul>
<li><strong>In</strong>（进入市场）</li>
<li><strong>Out</strong>（不进入市场）→ 收益 (1,2)</li>
</ul>
</li>
<li><strong>玩家2（在位者）</strong>：观察到进入者行动后选择：<ul>
<li><strong>A</strong>（容纳）→ 收益 (2,1)</li>
<li><strong>F</strong>（斗争）→ 收益 (0,0)</li>
</ul>
</li>
</ul>
<h4 id="标准式表示（收益矩阵）："><a href="#标准式表示（收益矩阵）：" class="headerlink" title="标准式表示（收益矩阵）："></a>标准式表示（收益矩阵）：</h4><div class="table-container">
<table>
<thead>
<tr>
<th>进入者 \ 在位者</th>
<th>容纳 (A)</th>
<th>斗争 (F)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>进入 (In)</strong></td>
<td>(2,1)</td>
<td>(0,0)</td>
</tr>
<tr>
<td><strong>不进入 (Out)</strong></td>
<td>(1,2)</td>
<td>(1,2)</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="纳什均衡分析"><a href="#纳什均衡分析" class="headerlink" title="纳什均衡分析"></a>纳什均衡分析</h3><ol>
<li><p><strong>(In, A)</strong>：</p>
<ul>
<li>进入者收益：2（若改为Out，收益1&lt;2）→ <strong>无偏离激励</strong></li>
<li>在位者收益：1（若改为F，收益0&lt;1）→ <strong>无偏离激励</strong><br>✅ <strong>是纳什均衡</strong></li>
</ul>
</li>
<li><p><strong>(Out, F)</strong>：</p>
<ul>
<li>进入者收益：1（若改为In，在位者选F则收益0&lt;1）→ <strong>无偏离激励</strong></li>
<li>在位者收益：2（无论选A或F，收益均为2）→ <strong>无偏离激励</strong><br>✅ <strong>是纳什均衡（弱均衡）</strong></li>
</ul>
</li>
</ol>
<hr>
<h3 id="均衡合理性检验"><a href="#均衡合理性检验" class="headerlink" title="均衡合理性检验"></a>均衡合理性检验</h3><h4 id="为什么-Out-F-不合理？"><a href="#为什么-Out-F-不合理？" class="headerlink" title="为什么 (Out, F) 不合理？"></a>为什么 <strong>(Out, F)</strong> 不合理？</h4><p>尽管 <strong>(Out, F)</strong> 是纳什均衡，但它在动态博弈中<strong>不可信</strong>（缺乏子博弈精炼性）：</p>
<ol>
<li><p><strong>在位者的空威胁</strong>：</p>
<ul>
<li>在位者声称“若你进入，我会斗争（F）”，但若进入者真的选择 <strong>In</strong>：<ul>
<li>在位者选 <strong>A</strong> 收益为 <strong>1</strong></li>
<li>在位者选 <strong>F</strong> 收益为 <strong>0</strong>（更差）<br>→ 理性在位者实际会选择 <strong>A</strong>，而非 <strong>F</strong>。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>进入者的理性决策</strong>：</p>
<ul>
<li>进入者知道在位者的威胁不可信（一旦进入，在位者必选 <strong>A</strong>）。</li>
<li>因此进入者应选择 <strong>In</strong>（收益2 &gt; 选Out的收益1）。</li>
</ul>
</li>
</ol>
<h4 id="逆向归纳验证："><a href="#逆向归纳验证：" class="headerlink" title="逆向归纳验证："></a>逆向归纳验证：</h4><ol>
<li>若进入者选 <strong>In</strong>，在位者在子博弈中：<ul>
<li>选 <strong>A</strong> → 收益 <strong>1</strong></li>
<li>选 <strong>F</strong> → 收益 <strong>0</strong><br>→ 最优选择是 <strong>A</strong>。</li>
</ul>
</li>
<li>进入者预判：<ul>
<li>选 <strong>In</strong> → 收益 <strong>2</strong>（因在位者会选A）</li>
<li>选 <strong>Out</strong> → 收益 <strong>1</strong><br>→ 最优选择是 <strong>In</strong>。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><div class="table-container">
<table>
<thead>
<tr>
<th>均衡</th>
<th>合理性</th>
<th>原因</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>(In, A)</strong></td>
<td>✅ 合理</td>
<td>威胁可信，符合序贯理性</td>
</tr>
<tr>
<td><strong>(Out, F)</strong></td>
<td>❌ 不合理</td>
<td>依赖不可信的空威胁（动态不一致）</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>核心问题</strong>：纳什均衡在扩展式博弈中可能包含<strong>不可信威胁</strong>，需通过<strong>子博弈精炼纳什均衡</strong>（逆向归纳法）剔除不合理的均衡。</li>
<li><strong>本例唯一合理均衡</strong>：<strong>(In, A)</strong>，即进入者进入，在位者容纳。</li>
</ul>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624004513150.png" alt="image-20250624004513150"></p>
<ul>
<li>回顾匹配硬币博弈（matching pennies game）的两阶段扩展式版本。</li>
<li>在这个博弈中，存在两个<strong>真子博弈（proper subgames）</strong> 以及博弈本身（它也是一个子博弈），因此总共有<strong>三个子博弈</strong>。</li>
</ul>
<blockquote>
<p><strong>定义：</strong> 在一个（完美信息）博弈树中，<strong>每一个节点</strong>，连同从该节点到达之后所剩余的博弈部分，被称为一个<strong>子博弈（subgame）</strong>。<br>即，每一个<strong>非终端历史（non-terminal history）</strong> ( h ) 都对应一个子博弈。</p>
</blockquote>
<hr>
<p><strong>关键概念理解：</strong></p>
<ol>
<li><p><strong>子博弈 (Subgame):</strong></p>
<ul>
<li>子博弈是<strong>原博弈的一部分</strong>，它起始于博弈树中的<strong>某个单一决策节点</strong>（该节点代表一个特定的非终端历史 ( h^k )），并<strong>包含该节点之后的所有后续决策节点、行动分支和终端节点</strong>。</li>
<li>它本身必须构成一个<strong>完整的、独立的博弈</strong>，拥有明确的起点（该决策节点）、后续行动规则和最终收益。</li>
<li>子博弈继承了原博弈的所有规则（玩家、行动、信息、收益函数）。</li>
</ul>
</li>
<li><p><strong>非终端历史 (Non-terminal History):</strong></p>
<ul>
<li>指那些<strong>不是博弈最终结果</strong>的历史 ( h^k )（即 ( k &lt; K+1 )，其中 ( K+1 ) 是终端历史的索引）。</li>
<li>每个这样的历史 ( h^k ) 都标志着博弈进行到了一个<strong>尚未结束的决策点</strong>。</li>
<li><strong>关键联系：</strong> 每个非终端历史 ( h^k ) 都<strong>唯一确定了一个子博弈的起始点</strong>。这个子博弈就是从历史 ( h^k ) 所对应的那个决策节点开始的剩余游戏部分。</li>
</ul>
</li>
<li><p><strong>子博弈的数量:</strong></p>
<ul>
<li>在<strong>完美信息（perfect information）</strong> 博弈（即每个信息集只包含一个节点）中，子博弈的数量等于<strong>非终端决策节点的数量</strong>。</li>
<li>示例中提到匹配硬币两阶段博弈有<strong>三个子博弈</strong>：<ul>
<li><strong>子博弈 1：</strong> 起始于<strong>玩家1的第一个决策节点</strong>（对应初始历史 ( h^0 = \emptyset )）。<strong>这就是整个博弈本身。</strong></li>
<li><strong>子博弈 2：</strong> 起始于<strong>玩家2在玩家1选择“正面”之后的决策节点</strong>（对应历史 ( h^1 = (Heads) )）。</li>
<li><strong>子博弈 3：</strong> 起始于<strong>玩家2在玩家1选择“反面”之后的决策节点</strong>（对应历史 ( h^1 = (Tails) )）。</li>
</ul>
</li>
<li>后两个是<strong>真子博弈（proper subgames）</strong>，因为它们严格包含在原博弈之内且不等于原博弈。</li>
</ul>
</li>
<li><p><strong>为什么子博弈概念重要？</strong></p>
<ul>
<li><strong>均衡精炼：</strong> 子博弈概念是定义<strong>子博弈精炼纳什均衡（Subgame Perfect Nash Equilibrium, SPNE）</strong> 的核心。SPNE 要求均衡策略不仅在原博弈上构成纳什均衡，而且在<strong>每一个可能的子博弈</strong>上也构成纳什均衡。这旨在剔除那些依赖<strong>不可信威胁或承诺</strong>的纳什均衡（例如之前进入博弈中的 <code>(Out, F)</code> 均衡）。</li>
<li><strong>分析工具：</strong> 子博弈结构使得我们可以使用<strong>逆向归纳法（backward induction）</strong> 来求解完美信息有限博弈的 SPNE。我们从最小的子博弈（最接近终点的）开始求解，将其解（收益或最优行动）代入其父节点，再逐步倒推回根节点。</li>
<li><strong>模块化分析：</strong> 允许将大型复杂博弈分解为更小的、可管理的子部分进行分析。</li>
</ul>
</li>
<li><p><strong>子博弈求解方法解析</strong></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/199055190">https://zhuanlan.zhihu.com/p/199055190</a></p>
</li>
</ol>
<h2 id="2-3-势博弈"><a href="#2-3-势博弈" class="headerlink" title="2.3 势博弈"></a>2.3 势博弈</h2><p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624194410787.png" alt="image-20250624194410787"></p>
<ul>
<li><strong>请注意，对于所有厂商 i 和所有 q−i&gt;0 的情况：</strong><ul>
<li>ui(qi,q−i)−ui(qi′,q−i)&gt;0 <strong>当且仅当 (iff)</strong> Φ(qi,q−i)−Φ(qi′,q−i)&gt;0 ，此条件对于所有 qi,qi′&gt;0 均成立。</li>
<li>说明：<ul>
<li>这条关键的陈述指出，在其他厂商产量不变的情况下，厂商 i 从产量 qi′ 转换到 qi 能否增加自身利润，与这个转换能否增加函数 Φ 的值，两者是完全等价的。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>这张PPT的核心思想是为古诺竞争模型引入一个称为<strong>“势函数”(Potential Function)</strong> 的概念，并证明此模型是一个<strong>“势博弈”(Potential Game)</strong>。</p>
<ol>
<li><p>什么是古诺竞争？</p>
<p>这是一个经典的经济学模型，描述了在一个寡占市场中，几家厂商如何进行产量竞争。每家厂商都假设其竞争对手的产量是固定的，然后选择能让自己利润最大化的产量。当所有厂商都达到一个状态，即没有任何一家厂商可以单方面改变产量来增加自己的利润时，市场就达到了纳什均衡 (Nash Equilibrium)。</p>
</li>
<li><p>为什么要引入奇怪的函数 Φ？</p>
<p>支付函数 ui 非常直观，它就是厂商 i 的利润。然而，函数 Φ 看起来很抽象，它将“所有厂商产量的乘积”与“单位利润”相乘，并没有直接的经济学意义。</p>
<p>它的真正价值在于其数学性质。投影片的最后一条结论揭示了这个性质：任何单一厂商 i 调整策略 (产量 qi) 所带来的自身利润变化方向，都和函数 Φ 的变化方向完全一致。</p>
</li>
<li><p>这意味着什么？(核心理解)</p>
<p>这意味着，原本一个复杂的多人博弈问题（每个厂商都在最大化自己的 ui），可以被转化为一个相对简单的<strong>“寻找单一函数 Φ 最大值”</strong>的问题。</p>
<ul>
<li><strong>简化分析</strong>：我们不再需要同时追踪 I 个不同厂商的利润函数，只需要分析一个共同的“势函数”Φ 即可。</li>
<li><strong>保证均衡存在</strong>：在势博弈中，势函数 Φ 的局部最大值点对应着该博弈的纳什均衡。因为一个有界的连续函数必然存在最大值，这就为证明古诺均衡的存在性提供了一条优雅的路径。</li>
<li><strong>收敛性</strong>：势博弈还有一个重要特性，即如果玩家们轮流进行“最优反应”（每次都选择当下能让自己利润最大化的策略），这个过程最终一定会收敛到一个纳什均衡。函数 Φ 就像一个山坡，玩家们的每一步调整都像是在爬坡，最终必然会走到一个山顶（均衡点）。</li>
</ul>
</li>
</ol>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624200511139.png" alt="image-20250624200511139"></p>
<p>这两张PPT分别介绍了<strong>序数势博弈</strong>和<strong>精确势博弈</strong>。</p>
<ol>
<li><p>与“序数势”的核心区别</p>
<p>理解“精确势”的关键在于将其与上一张幻灯片中的“序数势”进行对比：</p>
<ul>
<li><strong>序数势 (Ordinal Potential)</strong>：只要求参与者收益变化的<strong>方向</strong>与势函数变化的<strong>方向</strong>一致。<ul>
<li>通俗地说：“只要我换策略能多赚钱，<code>Φ</code> 的值就一定会变大。” 它不关心你多赚了1块钱还是100块钱。</li>
</ul>
</li>
<li><strong>精确势 (Exact Potential)</strong>：要求参与者收益变化的<strong>确切数值</strong>与势函数变化的<strong>确切数值</strong>完全相等。<ul>
<li>通俗地说：“如果我换策略能多赚10块钱，<code>Φ</code> 的值就也必须不多不少，正好增加10。”</li>
</ul>
</li>
</ul>
</li>
<li><p>更严格的条件</p>
<p>显然，“精确势”是一个比“序数势”严格得多的条件。如果一个博弈是精确势博弈，那么它必然也是一个序数势博弈（因为如果两个改变量的数值相等，它们的正负号必然相同）。但反过来不成立，很多序数势博弈并不能满足精确势的苛刻条件。</p>
</li>
<li><p>指正幻灯片中的笔误</p>
<p>需要指出，幻灯片的最后一行存在一个明显的笔误。它写着 “G is called an exact potential game if it admits an ordinal potential.”（如果一个博弈拥有一个序数势，它就被称为一个精确势博弈）。这在逻辑上是错误的。</p>
<ul>
<li><strong>正确表述应为</strong>：“如果一个博弈 G 拥有一个<strong>精确势函数 (exact potential)</strong>，那么它就被称为一个<strong>精确势博弈</strong>。”</li>
</ul>
</li>
</ol>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624201207684.png" alt="image-20250624201207684"></p>
<p>这张幻灯片通过一个具体的2x2矩阵博弈（囚徒困境的一个变体）的例子，非常直观地展示了“势函数”是如何运作的。这张幻灯片的核心目的是让我们<strong>验证</strong>所给出的矩阵 <code>P</code> 是否真的是博弈 <code>G</code> 的一个势函数。</p>
<p>我们可以通过检验定义来验证。让我们看看当某个参与者单方面改变策略时，他个人收益的变化量是否与势函数 <code>P</code> 的变化量相匹配。</p>
<p><strong>1. 验证过程</strong></p>
<p>我们分别检查行参与者（玩家1）和列参与者（玩家2）的决策。</p>
<ul>
<li><p><strong>检验玩家1（行选择者）</strong></p>
<ul>
<li><p>当玩家2选择“左”时</p>
<p>：玩家1在“上”（收益1）和“下”（收益0）之间选择。</p>
<ul>
<li>收益变化: u1(上, 左)−u1(下, 左)=1−0=1</li>
<li>势函数变化: P(上, 左)−P(下, 左)=4−3=1</li>
<li>两者<strong>完全相等</strong>。</li>
</ul>
</li>
<li><p>当玩家2选择“右”时</p>
<p>：玩家1在“上”（收益9）和“下”（收益6）之间选择。</p>
<ul>
<li>收益变化: u1(上, 右)−u1(下, 右)=9−6=3</li>
<li>势函数变化: P(上, 右)−P(下, 右)=3−0=3</li>
<li>两者<strong>完全相等</strong>。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>检验玩家2（列选择者）</strong></p>
<ul>
<li><p>当玩家1选择“上”时</p>
<p>：玩家2在“左”（收益1）和“右”（收益0）之间选择。</p>
<ul>
<li>收益变化: u2(上, 左)−u2(上, 右)=1−0=1</li>
<li>势函数变化: P(上, 左)−P(上, 右)=4−3=1</li>
<li>两者<strong>完全相等</strong>。</li>
</ul>
</li>
<li><p>当玩家1选择“下”时</p>
<p>：玩家2在“左”（收益9）和“右”（收益6）之间选择。</p>
<ul>
<li>收益变化: u2(下, 左)−u2(下, 右)=9−6=3</li>
<li>势函数变化: P(下, 左)−P(下, 右)=3−0=3</li>
<li>两者<strong>完全相等</strong>。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>2. 结论</strong></p>
<p>由于<strong>任何</strong>参与者单方面改变策略所带来的收益变化，都<strong>精确地等于</strong>势函数 <code>P</code> 中对应数值的变化，因此，矩阵 <code>P</code> 是博弈 <code>G</code> 的一个<strong>精确势函数 (exact potential function)</strong>。这个博弈 <code>G</code> 是一个<strong>精确势博弈</strong>。</p>
<p><strong>3. 势函数的威力：寻找纳什均衡</strong></p>
<p>这个例子的美妙之处在于，它展示了势函数如何简化均衡的寻找过程。寻找博弈 <code>G</code> 的纯策略纳什均衡，现在等价于寻找势函数矩阵 <code>P</code> 的“稳定点”。</p>
<p>观察势函数矩阵 P：</p>
<p>P=(4330)</p>
<p>矩阵中的最大值是4，位于（上，左）位置。让我们看看这个点是否稳定：</p>
<ul>
<li>如果从 <code>P=4</code>（上，左）出发，玩家1单方面移到“下”，<code>P</code>的值会从4变成3，他不会移动。</li>
<li>如果从 <code>P=4</code>（上，左）出发，玩家2单方面移到“右”，<code>P</code>的值会从4变成3，他也不会移动。</li>
</ul>
<p>因为没有任何一方有动机从（上，左）这个位置离开，所以它是一个纳什均衡。这对应于原博弈 <code>G</code> 中的（1,1）这个结果。这个例子清晰地表明，<strong>势函数的局部最大值（在此例中也是全局最大值）对应着博弈的一个纳什均衡</strong>。这正是势函数在博弈分析中的核心价值。</p>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624201541524.png" alt="image-20250624201541524"></p>
<p>有限序数势博弈的结果是有限的，势函数会为每一个博弈策略赋予一个值，因此必然存在一个最大值，这个最大值就是纳什均衡点。</p>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624202953517.png" alt="image-20250624202953517"></p>
<h4 id="1-核心结论"><a href="#1-核心结论" class="headerlink" title="1. 核心结论"></a>1. 核心结论</h4><p>结论是最后一条：当厂商 <code>i</code> 单方面将自己的产量从 <code>q&#39;_i</code> 变为 <code>q_i</code> 时，他<strong>个人利润的变化量</strong>，与我们构造的那个看起来很复杂的函数 <code>Φ*</code> 的<strong>变化量是完全相等的</strong>。这正是“精确势函数”的定义。</p>
<h4 id="2-数学证明（为什么这个结论成立？）"><a href="#2-数学证明（为什么这个结论成立？）" class="headerlink" title="2. 数学证明（为什么这个结论成立？）"></a>2. 数学证明（为什么这个结论成立？）</h4><p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624203148105.png" alt="image-20250624203148105"></p>
<p>最后比较两个Δ即可</p>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624204837963.png" alt="image-20250624204837963"></p>
<p><strong>标题：有限序数势博弈中的简单动态 (Simple Dynamics in Finite Ordinal Potential Games)</strong></p>
<p>定义 (Definition)</p>
<p>策略空间 S 中的一条路径 (path) 是一个策略向量序列 (s0,s1,⋯)，其中任意两个连续的策略只在一个坐标上不同（即，每一次恰好只有一名参与者改变了他的策略）。</p>
<p>一条<strong>改进路径 (improvement path)</strong> 是一条路径 (s0,s1,⋯)，它满足：</p>
<ul>
<li>uik(sk)&lt;uik(sk+1)，其中策略 sk 和 sk+1 在第 ik 个坐标上不同。换句话说，对于那个改变了策略的参与者来说，他的收益得到了改善。</li>
</ul>
<p>一条改进路径可以被认为是<strong>“短视参与者” (myopic players)</strong> 动态生成的结果。</p>
<hr>
<p>这张幻灯片将我们的视角从静态的“均衡分析”转向了动态的“演化过程分析”。它为我们描述“一个博弈是如何一步步演变的”提供了正式的语言。这对于理解系统如何达到稳定状态至关重要。</p>
<h4 id="1-“路径”与“改进路径”的通俗解释"><a href="#1-“路径”与“改进路径”的通俗解释" class="headerlink" title="1. “路径”与“改进路径”的通俗解释"></a>1. “路径”与“改进路径”的通俗解释</h4><ul>
<li><strong>路径 (Path)</strong>：你可以把它想象成一盘棋的“棋谱”或一个游戏的“回合记录”。它记录了博弈状态是如何一步步变化的，其核心规则是<strong>“一次只动一个”</strong>。在 sk 这一步，只有一名参与者会改变他的行动，从而进入 sk+1 状态。这是一种分析动态过程的合理简化。</li>
<li><strong>改进路径 (Improvement Path)</strong>：这是一种特殊的、由理性驱动的路径。它不仅记录了系统的演变，还说明了<strong>为什么</strong>会这样演变。每一步的发生，都是因为某个参与者发现“如果我单方面改变策略，我的收益会立刻增加”，于是他就这么做了。这完美地描述了那些只顾眼前利益的参与者的行为。</li>
</ul>
<h4 id="2-“短视参与者”-Myopic-Players-的概念"><a href="#2-“短视参与者”-Myopic-Players-的概念" class="headerlink" title="2. “短视参与者” (Myopic Players) 的概念"></a>2. “短视参与者” (Myopic Players) 的概念</h4><p>这是理解“改进路径”背后行为动机的关键。</p>
<ul>
<li><strong>“短视”</strong>意味着参与者们并不深谋远虑，他们不会去预测对手的对手的反应。</li>
<li>他们的决策逻辑非常简单：“在当前这个局面下，我有没有一个别的选择能让我马上赚得更多？”</li>
<li>如果答案是“有”，那么某个参与者就会采取行动，从而推动整个系统沿着“改进路径”向前走一步。这个过程可以被看作是一系列“更优反应 (better response)”的链式反应。</li>
</ul>
<h4 id="3-为什么要在势博弈中讨论这个？（核心洞见）"><a href="#3-为什么要在势博弈中讨论这个？（核心洞见）" class="headerlink" title="3. 为什么要在势博弈中讨论这个？（核心洞见）"></a>3. 为什么要在势博弈中讨论这个？（核心洞见）</h4><p>定义这些概念的最终目的，是为了引出势博弈最强大的性质之一：<strong>动态收敛性</strong>。</p>
<p>我们可以设想一下：</p>
<ol>
<li><strong>在普通博弈中</strong>：一条“改进路径”可能会没完没了地走下去，甚至可能陷入一个循环（比如A动、B动、C动，结果又回到了A动之前的局面），永远无法达到一个稳定的纳什均衡。</li>
<li><strong>但在势博弈中</strong>：奇迹发生了。我们知道，在势博弈中，只要某个参与者 <code>i</code> 的收益 <code>uᵢ</code> 增加了，全局的势函数 <code>Φ</code> 的值也<strong>必须增加</strong>。</li>
<li>因此，在势博弈里，<strong>每一条“改进路径”都必然是一条“势函数 <code>Φ</code> 值不断增加的路径”</strong>。</li>
<li>在一个<strong>有限</strong>博弈中，势函数 <code>Φ</code> 的可能取值是有限的，它必然有一个最大值。<code>Φ</code> 的值不可能无限地增加下去。</li>
</ol>
<p><strong>结论</strong>：在有限势博弈中，任何“改进路径”走不了几步就<strong>必然会停止</strong>。它会在哪里停下呢？它会停在一个任何人都无法再单方面改善自己收益的地方——而这个地方，根据定义，<strong>就是一个纯策略纳什均衡</strong>。</p>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624205950398.png" alt="image-20250624205950398"></p>
<p><strong>标题：有限精确势博弈的特征描述 (Characterization of Finite Exact Potential Games)</strong></p>
<ul>
<li><p>对于一条有限路径 γ=(s0,s1,…,sN)，我们令：</p>
<p>I(γ)=i=1∑N(umi(si)−umi(si−1))</p>
<p>其中，mi 指代在路径的第 i 步改变其策略的参与者。</p>
</li>
<li><p>如果 s0=sN，则路径 γ=(s0,…,sN) 是<strong>闭合的 (closed)</strong>。如果在此外对于每一个 0≤l&lt;k≤N−1 都有 sl\\=sk，那么它就是一条<strong>简单闭合路径 (simple closed path)</strong>。</p>
</li>
</ul>
<p>定理 (Theorem)</p>
<p>一个博弈 G 是一个精确势博弈，当且仅当对于所有有限简单闭合路径 γ，都有 I(γ) = 0。此外，只检验长度为4的简单闭合路径就足够了。</p>
<hr>
<p>这个数学定理，它为我们提供了一个<strong>“试金石”</strong>，用来检验任何一个有限博弈到底是不是一个“精确势博弈”，而无需我们去猜测或构造那个势函数 <code>Φ</code>。</p>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624211206447.png" alt="image-20250624211206447"></p>
<p><strong>标题：网络拥堵博弈 (Network Congestion Games)</strong></p>
<ul>
<li>一个包含 n 个用户的<strong>有向图 (directed graph)</strong> G=(V,E)，</li>
<li>图 G 中的每一条<strong>边 (edge)</strong> e 都有一个<strong>延迟函数 (delay function)</strong> fₑ，</li>
<li>用户 i 的<strong>策略 (Strategy)</strong> 是选择一条从<strong>起点 (source)</strong> sᵢ 到<strong>终点 (destination)</strong> tᵢ 的<strong>路径 (path)</strong> Aᵢ，</li>
<li>一条路径的延迟是该路径上所有边的延迟之和，</li>
<li>每个用户都想通过选择最佳路径来<strong>最小化 (minimize)</strong> 其自身的延迟。</li>
</ul>
<hr>
<p>这张幻灯片介绍了一类在现实世界中应用极其广泛的博弈模型——<strong>网络拥堵博弈</strong>。这是理解交通堵塞、互联网数据包路由、供应链物流等众多问题的核心理论框架。</p>
<h4 id="1-博弈的核心要素"><a href="#1-博弈的核心要素" class="headerlink" title="1. 博弈的核心要素"></a>1. 博弈的核心要素</h4><p>这个模型非常直观，它完美地捕捉了“拥堵”现象的本质：</p>
<ul>
<li><strong>参与者 (Players)</strong>：n 个用户（比如，n 位司机）。</li>
<li><strong>策略 (Strategies)</strong>：每个司机可以选择的路线（例如，从家 <code>sᵢ</code> 到公司 <code>tᵢ</code> 的不同路径）。</li>
<li><strong>成本 (Cost)</strong>：每个司机在路上花费的时间，即“延迟”。</li>
</ul>
<p>这个博弈最关键、最有趣的地方在于成本（延迟）的计算方式。一条路（边 <code>e</code>）的延迟<code>fₑ</code><strong>不是一个固定的数，而是一个函数</strong>。它的值取决于有多少人同时在使用这条路。</p>
<h4 id="2-博弈的内在冲突"><a href="#2-博弈的内在冲突" class="headerlink" title="2. 博弈的内在冲突"></a>2. 博弈的内在冲突</h4><p>每个司机都想自私地选择一条“最快”的路。但正是因为所有人都这么想，才导致了问题的产生：</p>
<ul>
<li>如果有一条近路看起来最快，所有司机可能都会涌向这条路。</li>
<li>结果，这条路变得极度拥堵，它的延迟函数 <code>fₑ(x)</code> 因为 <code>x</code>（用户数）变得很大而给出一个非常高的延迟值。</li>
<li>这条所谓的“最快”的路，实际上可能比其他更长但无人问津的路要慢得多。</li>
</ul>
<p><strong>每个人的最优选择都依赖于其他所有人的选择</strong>。我的决策影响你的成本，你的决策也影响我的成本。这就是博弈的核心所在。</p>
<h4 id="3-与“势博弈”的深刻联系（核心洞见）"><a href="#3-与“势博弈”的深刻联系（核心洞见）" class="headerlink" title="3. 与“势博弈”的深刻联系（核心洞见）"></a>3. 与“势博弈”的深刻联系（核心洞见）</h4><p>这个模型最惊人的特性是，<strong>网络拥堵博弈是“精确势博弈”的经典范例</strong>。这是由学者 Rosenthal 在1973年发现的里程碑式成果。</p>
<p>存在一个全局的势函数 Φ（通常称为罗森塔尔势函数），它能够完美地刻画整个系统的动态。这个势函数的定义方式非常巧妙：</p>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624211304691.png" alt="image-20250624211304691"></p>
<p>其中，xe 是当前选择了边 e 的总用户数。这个公式的含义是，把网络中每一条边的“从第1个用户到第 xe 个用户的延迟依次加起来”，然后再把所有边的这个值汇总。</p>
<p>可以被严格证明：<strong>当任何一个用户 i 单方面改变自己的路径时，他个人延迟的变化量，与这个全局势函数 <code>Φ</code> 的变化量是完全相等的！</strong></p>
<h4 id="4-重要推论"><a href="#4-重要推论" class="headerlink" title="4. 重要推论"></a>4. 重要推论</h4><p>既然拥堵博弈是精确势博弈，那么我们之前讨论过的所有优美性质就都可以应用在这里：</p>
<ol>
<li><strong>均衡必然存在</strong>：任何一个网络拥堵博弈都<strong>至少存在一个纯策略纳什均衡</strong>。在交通模型里，这被称为“瓦德罗普均衡 (Wardrop equilibrium)”。这意味着，总会存在一种稳定的交通分配格局，在这种格局下，没有单个司机可以通过单方面改变路线来缩短自己的通勤时间。</li>
<li><strong>动态必然收敛</strong>：如果司机们是“短视的”（例如，每天根据昨天的路况尝试寻找更快的路），这个不断调整、学习的过程<strong>必然会收敛到一个稳定的均衡状态</strong>，而不会永久地混乱或振荡下去。</li>
</ol>
<h2 id="3-1-零和博弈及纳什均衡计算"><a href="#3-1-零和博弈及纳什均衡计算" class="headerlink" title="3.1 零和博弈及纳什均衡计算"></a>3.1 零和博弈及纳什均衡计算</h2><p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624232921182.png" alt="image-20250624232921182"></p>
<p><strong>练习：斯塔克尔伯格双寡头模型 (Exercise: The Stackelberg model of Duopoly)</strong></p>
<ul>
<li>斯塔克尔伯格双寡头模型 (1934):<ul>
<li>一个参与者，被称为主导者或<strong>领导者 (leader)</strong>，首先行动，并且该参与者的选择结果在另一位参与者（<strong>跟随者, follower</strong>）做出选择之前就已告知对方。</li>
<li>例如，通用汽车公司（General Motors）在美国历史上的某些时期，就曾在汽车行业中扮演了如此主导的角色。</li>
</ul>
</li>
</ul>
<p><em>（来源：Game Theory, Second Edition, 2014. Thomas S. Ferguson）</em></p>
<p><strong>标题：练习：斯塔克尔伯格双寡头模型</strong></p>
<ul>
<li>厂商1首先选择一个生产数量 q1，其单位成本为 c。</li>
<li>这个数量会被告知厂商2，然后厂商2再选择自己的生产数量 q2，其单位成本同样为 c。</li>
<li>之后，市场的单位价格 P 由以下公式决定： P(Q)={a−Q0if 0≤Q≤aif Q&gt;a=(a−Q)+ 其中 Q=q1+q2，并且 a 是一个常数。</li>
<li>参与者们获得如下支付（利润）： u1(q1,q2)=q1P(q1+q2)−cq1=q1(a−q1−q2)+−cq1 u2(q1,q2)=q2P(q1+q2)−cq2=q2(a−q1−q2)+−cq2 其中单位成本 c&lt;a。</li>
</ul>
<p>斯塔克尔伯格模型是产业组织理论中一个基石性的模型。它与我们之前讨论的古诺模型（Cournot model）最大的不同在于，它将“同时行动”改为了<strong>“序贯行动” (Sequential Moves)</strong>，从而引入了<strong>先手优势 (First-mover Advantage)</strong> 的概念。</p>
<h4 id="1-与古诺模型的根本区别"><a href="#1-与古诺模型的根本区别" class="headerlink" title="1. 与古诺模型的根本区别"></a>1. 与古诺模型的根本区别</h4><ul>
<li><strong>古诺模型</strong>：两家厂商<strong>同时</strong>决定产量，谁也不知道对方会生产多少，是一个静态的、猜对手心思的博弈。</li>
<li><strong>斯塔克尔伯格模型</strong>：两家厂商有明确的行动顺序。一家是“领导者”（先动），另一家是“跟随者”（后动）。领导者率先公布自己的产量，跟随者在<strong>观察到</strong>领导者的产量后再决定自己的最优产量。这是一个动态博弈。</li>
</ul>
<h4 id="2-如何求解？——-逆向归纳法-Backward-Induction"><a href="#2-如何求解？——-逆向归纳法-Backward-Induction" class="headerlink" title="2. 如何求解？—— 逆向归纳法 (Backward Induction)"></a>2. 如何求解？—— 逆向归纳法 (Backward Induction)</h4><p>对于这种有先后顺序的博弈，标准的解法是“逆向归纳”，即从后往前推。</p>
<p><strong>第一步：求解跟随者（厂商2）的问题</strong></p>
<p>我们先站在厂商2的角度。此时，厂商1的产量 q1 已经确定，是一个已知的数字。厂商2的目标是选择自己的产量 q2 来最大化自身利润 u2。</p>
<p>q2max u2(q1,q2)=q2(a−q1−q2)−cq2</p>
<p>为了求最大值，我们对 q2 求导并令其为0：</p>
<p>∂q2∂u2=a−q1−2q2−c=0</p>
<p>解出 q2，我们就得到了厂商2的反应函数 (Reaction Function)：</p>
<p>q2∗(q1)=2a−c−q1</p>
<p>这个函数告诉我们：不论领导者厂商1生产多少 (q1)，跟随者厂商2的最优应对策略是什么。</p>
<p><strong>第二步：求解领导者（厂商1）的问题</strong></p>
<p>厂商1非常“聪明”，它完全知道厂商2会如何根据它的 q1 来做出反应。因此，厂商1在做决策时，会把厂商2的反应函数直接代入自己的利润公式中，以此来预测自己选择不同 q1 的最终后果。</p>
<p>厂商1的利润函数变为：</p>
<p>u1(q1)=q1(a−q1−q2∗(q1))−cq1=q1(a−q1−2a−c−q1)−cq1</p>
<p>化简括号内的部分：</p>
<p>u1(q1)=q1(2a−c−q1)−cq1=2aq1−cq1−q12−cq1</p>
<p>厂商1的目标是选择 q1 来最大化这个新的利润函数。我们对 q1 求导并令其为0：</p>
<p>∂q1∂u1=2a−c−2q1=0⟹a−c−2q1=0</p>
<p>解出领导者厂商1的最优产量：</p>
<p>q1∗=2a−c</p>
<p><strong>第三步：得出最终均衡结果</strong></p>
<p>将厂商1的最优产量代入厂商2的反应函数，得到厂商2的产量：</p>
<p>q2∗=2a−c−q1∗=2a−c−(a−c)/2=2(a−c)/2=4a−c</p>
<p><strong>斯塔克尔伯格均衡解为：(领导者产量 q1∗=2a−c， 跟随者产量 q2∗=4a−c)</strong></p>
<h4 id="3-先手优势"><a href="#3-先手优势" class="headerlink" title="3. 先手优势"></a>3. 先手优势</h4><ul>
<li>在斯塔克尔伯格均衡中，领导者产量(2a−c)是跟随者(4a−c)的两倍。</li>
<li>我们可以对比一下古诺均衡的结果：在古诺模型中，两家厂商的产量相同，均为 3a−c。</li>
<li>比较可知：q1Stackelberg(2a−c)&gt;qCournot(3a−c)&gt;q2Stackelberg(4a−c)。</li>
<li>这意味着，通过率先行动并承诺一个较高的产量，领导者可以有效地“挤压”跟随者的市场空间，迫使跟随者选择一个较低的产量，从而为自己攫取更高的市场份额和利润。这就是<strong>先手优势</strong>的体现。</li>
</ul>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624233936853.png" alt="image-20250624233936853"></p>
<p><strong>零和博弈 (Zero-Sum Games)</strong></p>
<p>极小化极大定理 (Minimax Theorem) (约翰·冯·诺伊曼, 1928):</p>
<p>对于每一个具有有限个纯策略的两人零和博弈，都存在一个适用于各方参与者的混合策略和一个价值 V，使得：</p>
<ul>
<li>给定参与者2的策略，参与者1可能获得的最佳支付为 V。</li>
<li>给定参与者1的策略，参与者2可能获得的最佳支付为 -V。</li>
</ul>
<p>策略存在的部分是<strong>纳什定理的一个特例</strong>，也是其先驱。</p>
<p>这基本上是说，参与者1可以保证自己获得<strong>至少 V</strong> 的支付，而参与者2可以保证自己获得<strong>至少 -V</strong> 的支付。如果双方都采取最优策略，这恰好就是他们将得到的结果。</p>
<p>它之所以被称为“极小化极大 (minimax)”，是因为参与者是通过一种试图<strong>最小化 (minimize) 对手可能获得的最大 (maximum) 支付</strong>的策略来获得这个价值的。我们稍后会再回到这一点。</p>
<p><strong>定义</strong>：价值 V 被称为该博弈的<strong>价值 (value)</strong>（或回报、支付）。</p>
<p><strong>例如</strong>：石头剪刀布的价值是0；假设参与者2采取最优策略（以1/3的概率出每一种手势），参与者1能期望获得的最好结果是0的支付。</p>
<hr>
<p>这张幻灯片介绍了博弈论的奠基性概念之一——<strong>两人零和博弈</strong>，以及该领域第一个里程碑式的定理——冯·诺伊曼的<strong>极小化极大定理</strong>。</p>
<h4 id="1-什么是零和博弈？"><a href="#1-什么是零和博弈？" class="headerlink" title="1. 什么是零和博弈？"></a>1. 什么是零和博弈？</h4><p>首先，零和博弈指的是在一个博弈中，所有参与者的收益（或亏损）加起来永远等于零。这意味着，<strong>一方的所得，必然是另一方的所失</strong>。这是一个纯粹冲突、完全竞争的模型，没有任何合作共赢的可能。经典的例子包括：</p>
<ul>
<li><strong>棋类游戏</strong>：如象棋、围棋，一方赢就是另一方输。</li>
<li><strong>石头剪刀布</strong>：一方赢一分，另一方就输一分。</li>
<li><strong>竞技体育</strong>：大多数只有两方对阵的比赛。</li>
</ul>
<h4 id="2-“极小化极大”定理的通俗解释"><a href="#2-“极小化极大”定理的通俗解释" class="headerlink" title="2. “极小化极大”定理的通俗解释"></a>2. “极小化极大”定理的通俗解释</h4><p>这个定理解决了一个核心问题：在这样你死我活的纯冲突中，“理性”的策略是什么？冯·诺伊曼给出了一个天才的答案，其思考逻辑如下：</p>
<p><strong>从参与者1（P1）的角度（最大化最小值, Maximin）</strong>：</p>
<ol>
<li>P1必须假设P2是完全理性的，并且会尽一切努力损害P1的利益。</li>
<li>P1会思考：“对于我可能采取的每一个策略，P2都会用对我最不利的方式来回应。我先列出每一种策略下，我最坏会得到什么结果（我的<strong>最小</strong>收益）。”</li>
<li>“然后，在所有这些‘最坏结果’中，我选择那个能让我得到最好结果的策略。”</li>
<li>这个过程，就是<strong>最大化自己的最小保证收益 (Maximize a minimum payoff)</strong>，简称 <strong>Maximin</strong>。</li>
</ol>
<p><strong>从参与者2（P2）的角度（极小化极大, Minimax）</strong>：</p>
<ol>
<li>P2同样假设P1会尽全力损害自己。</li>
<li>P2会思考：“对于P1的每一个策略，我最坏会损失多少（即P1能获得的最大收益）？”</li>
<li>“然后，我选择一个策略，能让P1可能获得的最大收益变得最小。”</li>
<li>这个过程，就是<strong>最小化自己的最大可能损失 (Minimize a maximum loss)</strong>，简称 <strong>Minimax</strong>。</li>
</ol>
<p><strong>定理的“奇迹”</strong>：冯·诺伊曼证明，对于任何两人零和博弈，P1通过“最大化最小值”策略能保证得到的收益 <code>V</code>，与P2通过“极小化极大”策略能保证让P1得到的收益 <code>V</code>，是<strong>完全同一个数值</strong>！这个 <code>V</code> 就是该博弈的“价值”。</p>
<p>这意味着，这类纯冲突博弈存在一个绝对理性的、稳定的解。双方的最佳策略将会在这个点上交汇。</p>
<h4 id="3-混合策略的重要性"><a href="#3-混合策略的重要性" class="headerlink" title="3. 混合策略的重要性"></a>3. 混合策略的重要性</h4><p>这个定理的成立，往往需要<strong>混合策略</strong>的引入，即以一定的概率随机地选择不同的行动。</p>
<ul>
<li>以“石头剪刀布”为例，如果你只出“石头”（一个纯策略），对手会立刻发现并一直出“布”来打败你。</li>
<li>你唯一能保证自己不输的策略，就是完全随机地出招（石头、剪刀、布各1/3概率）。</li>
<li>当你的对手也采取这种最优的混合策略时，你期望的平均收益就是0。因此，这个博弈的价值 <code>V=0</code>。</li>
</ul>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250625000231647.png" alt="image-20250625000231647"></p>
<p><strong>计算纳什均衡：两人零和博弈 (Computing Nash Equilibria: 2-person, Zero-Sum Games)</strong></p>
<ul>
<li>这个博弈没有纯策略纳什均衡。</li>
<li>根据纳什定理，它必然拥有一个<strong>混合策略</strong>纳什均衡。</li>
<li>我们该如何找到它呢？</li>
</ul>
<p><em>(注：这是一个两人零和博弈)</em></p>
<hr>
<p>这张幻灯片提出了一个核心问题：对于一个没有纯策略均衡的博弈，我们如何具体计算出它的混合策略纳什均衡？</p>
<p>下面是详细的计算步骤：</p>
<h4 id="第一步：验证不存在纯策略纳什均衡"><a href="#第一步：验证不存在纯策略纳什均衡" class="headerlink" title="第一步：验证不存在纯策略纳什均衡"></a>第一步：验证不存在纯策略纳什均衡</h4><p>我们可以通过“划线法”或“最优反应法”来快速验证。</p>
<ol>
<li>如果参与者2选择“列1”，参与者1的最优选择是“行2”（因为收益 +3 &gt; -2）。</li>
<li>如果参与者2选择“列2”，参与者1的最优选择是“行1”（因为收益 +3 &gt; -4）。</li>
<li>如果参与者1选择“行1”，参与者2的最优选择是“列1”（因为收益 +2 &gt; -3）。</li>
<li>如果参与者1选择“行2”，参与者2的最优选择是“列2”（因为收益 +4 &gt; -3）。</li>
</ol>
<p>我们发现，没有任何一个单元格是双方共同的最优选择，因此该博弈确实没有纯策略纳什均衡。</p>
<h4 id="第二步：设定混合策略"><a href="#第二步：设定混合策略" class="headerlink" title="第二步：设定混合策略"></a>第二步：设定混合策略</h4><p>在混合策略均衡中，核心思想是<strong>“无差异原则” (Indifference Principle)</strong>：每个参与者选择自己的混合策略（即概率），目的是让<strong>对方</strong>在自己的几个纯策略选择之间感到<strong>无所谓/无差异</strong>（即期望收益完全相等）。</p>
<ul>
<li>我们假设<strong>参与者1</strong>以概率 <strong>p</strong> 选择“行1”，以概率 <strong>(1-p)</strong> 选择“行2”。</li>
<li>我们假设<strong>参与者2</strong>以概率 <strong>q</strong> 选择“列1”，以概率 <strong>(1-q)</strong> 选择“列2”。</li>
</ul>
<h4 id="第三步：计算参与者1的混合策略-p"><a href="#第三步：计算参与者1的混合策略-p" class="headerlink" title="第三步：计算参与者1的混合策略 p"></a>第三步：计算参与者1的混合策略 p</h4><p>为了让<strong>参与者2</strong>感到无差异，参与者2选择“列1”的期望收益必须等于他选择“列2”的期望收益。</p>
<ul>
<li>参与者2选择“列1”的期望收益 E(列1) = p<em>(+2)+(1−p)</em>(−3)</li>
<li>参与者2选择“列2”的期望收益 E(列2) = p<em>(−3)+(1−p)</em>(+4)</li>
</ul>
<p>令 E(列1) = E(列2):</p>
<p>可得p=7/12</p>
<p>所以，参与者1的最优策略是：以 7/12 的概率选择“行1”，以 5/12 的概率选择“行2”。</p>
<h4 id="第四步：计算参与者2的混合策略-q"><a href="#第四步：计算参与者2的混合策略-q" class="headerlink" title="第四步：计算参与者2的混合策略 q"></a>第四步：计算参与者2的混合策略 q</h4><p>同样，为了让<strong>参与者1</strong>感到无差异，参与者1选择“行1”的期望收益必须等于他选择“行2”的期望收益。</p>
<ul>
<li>参与者1选择“行1”的期望收益 E(行1) = q<em>(−2)+(1−q)</em>(+3)</li>
<li>参与者1选择“行2”的期望收益 E(行2) = q<em>(+3)+(1−q)</em>(−4)</li>
</ul>
<p>令 E(行1) = E(行2):</p>
<p>q=7/12</p>
<p>所以，参与者2的最优策略是：以 7/12 的概率选择“列1”，以 5/12 的概率选择“列2”。</p>
<h4 id="第五步：结论与博弈的价值"><a href="#第五步：结论与博弈的价值" class="headerlink" title="第五步：结论与博弈的价值"></a>第五步：结论与博弈的价值</h4><ol>
<li><p><strong>混合策略纳什均衡</strong>：该博弈的唯一纳什均衡是：参与者1采取混合策略 ( 7/12 ,5/12)，参与者2采取混合策略 (7/12 ,5/12)。</p>
</li>
<li><p>博弈的价值 (Value of the Game)：在均衡状态下，参与者1的期望收益是多少？我们可以把 q = 7/12 代入 E(行1) 的公式中计算：</p>
<p>E(P1) = 1/12</p>
<p>因此，这个博弈对参与者1的<strong>价值是 +1/12</strong>，对参与者2的价值是 -1/12。这意味着，如果双方都采取最优的随机策略，长期来看，参与者1平均每次能赢1/12。</p>
</li>
</ol>
<p><strong>为什么目的是让对方在自己的几个纯策略选择之间感到无所谓/无差异？</strong></p>
<p>简单来说，<strong>让对方“无差异”并不是我们的最终目的，而是我们为了实现自身利益最大化，所必须达到的一个“结果”或“条件”。</strong></p>
<p>这是一种非常高明的策略思想，我们可以从三个层面来理解它：</p>
<hr>
<h3 id="1-核心思想：消除对方的确定性最优解"><a href="#1-核心思想：消除对方的确定性最优解" class="headerlink" title="1. 核心思想：消除对方的确定性最优解"></a>1. 核心思想：消除对方的确定性最优解</h3><p>在一个博弈中，如果你采取的策略让你的对手有一个明确的、唯一的“最优选择”，那你就输了一半。因为：</p>
<ol>
<li>一个理性的对手，一定会采取那个对他来说最优的选择。</li>
<li>这样一来，对手的行动就变得<strong>完全可以预测</strong>了。</li>
<li>一旦对手的行动是可预测的，你就可以反过来调整自己的策略，去专门“克制”他那个可预测的行动，从而让自己获利更多。</li>
<li>但这就产生了一个矛盾：如果你能调整策略获利更多，说明你最初的策略就不是最优的。</li>
</ol>
<p>这个矛盾循环说明，一个稳定的均衡状态，不应该让任何一方有“唯一的、确定的”最优解。而要做到这一点，你唯一的方法就是调整自己的策略组合（即概率 <code>p</code>），直到你的对手觉得“选A或选B，反正期望收益都一样，我无所谓了”。</p>
<p><strong>当你让对手“无所谓”时，你就消除了他行动的确定性，他才不得不也用一种随机的方式来对抗你。这才是对自己最有利的局面。</strong></p>
<hr>
<h3 id="2-反向思考：如果不让对方无差异会怎样？"><a href="#2-反向思考：如果不让对方无差异会怎样？" class="headerlink" title="2. 反向思考：如果不让对方无差异会怎样？"></a>2. 反向思考：如果不让对方无差异会怎样？</h3><p>我们用上一张幻灯片的例子来思考：</p>
<ul>
<li>你的策略是：以概率 <code>p</code> 出“行1”，概率 <code>(1-p)</code> 出“行2”。</li>
<li>假设你选择的 <code>p</code> 没有让对手无差异，而是让对手觉得<strong>“出‘列1’比出‘列2’的期望收益更高”</strong>。</li>
</ul>
<p>接下来会发生什么？</p>
<ol>
<li><strong>对手的反应</strong>：理性的对手会想：“既然出‘列1’更好，我为什么还要费事去随机出‘列2’呢？” 于是，他会100%地出“列1”。他的策略就不再是混合策略了。</li>
<li><strong>你的反应</strong>：你看到对手只会出“列1”，那你还会坚持你原来的概率 <code>p</code> 吗？当然不会！你会看支付矩阵的“列1”那一栏，发现你出“行1”收益是-2，出“行2”收益是+3。你显然会100%地出“行2”来应对他。</li>
</ol>
<p><strong>结论</strong>：你最初那个让对手“有差异”的策略 <code>p</code>，最终导致了你自己也想改变策略。这就说明，那个初始状态<strong>根本不稳定</strong>，因此<strong>不是纳什均衡</strong>。</p>
<p>唯一的稳定状态，就是你选择的概率 <code>p</code>，正好让对手觉得“出‘列1’和出‘列2’没差”，他没有理由偏向任何一方，所以他才愿意继续以一定概率 <code>q</code> 来混合他的策略。</p>
<hr>
<h3 id="3-一个直观的例子：点球大战"><a href="#3-一个直观的例子：点球大战" class="headerlink" title="3. 一个直观的例子：点球大战"></a>3. 一个直观的例子：点球大战</h3><p>想象一下足球比赛中的点球大战：</p>
<ul>
<li><strong>你的角色</strong>：踢球手。你可以选择踢左边或右边。</li>
<li><strong>对手的角色</strong>：守门员。他可以选择扑左边或右边。</li>
</ul>
<p>你的目标是什么？是让守门员<strong>对于“扑左还是扑右”感到无差异</strong>。</p>
<ul>
<li><strong>为什么？</strong> 如果你总喜欢踢左边（比如70%的概率），守门员就会发现这个规律，然后更倾向于扑向左边，这样你的进球率就会下降。你的行为变得“可预测”了。</li>
<li><strong>你的最优策略</strong>：你必须调整自己踢左边和右边的概率（比如各50%），使得守门员觉得“反正我扑左扑右，猜对的概率都一样，期望丢球数也一样，我无所谓了，只能瞎猜”。</li>
<li><strong>最终结果</strong>：只有当你成功地让守门员陷入“无所谓”的境地时，他才无法预测你的行动，你才能最大化你的进球率。</li>
</ul>
<p><strong>总结：</strong></p>
<p>在混合策略中，让对方“无所谓”，并不是一种善意的妥协，而是一种<strong>最顶级的进攻策略</strong>。它本质上是：</p>
<ul>
<li><strong>为了防止自己被预测和被针对。</strong></li>
<li><strong>为了迫使对方也必须采取不确定的、随机的策略来应对。</strong></li>
<li><strong>为了最终能在一个充满不确定性的稳定均衡中，保障自己获得最大化的期望收益。</strong></li>
</ul>
<p>所以，“让对方无差异”是<strong>你</strong>实现最优策略的<strong>必要条件</strong>，而不是你的目的本身。</p>
<h2 id="3-2-极大极小博弈"><a href="#3-2-极大极小博弈" class="headerlink" title="3.2 极大极小博弈"></a>3.2 极大极小博弈</h2><p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250625213116525.png" alt="image-20250625213116525"></p>
<p>这张幻灯片用一个政治竞选的例子，构建了一个经典的两人零和博弈。它给出了问题设定，但没有给出解。核心任务就是根据这些信息，计算出这场博弈的均衡解以及博弈的价值。</p>
<h4 id="第一步：检查是否存在纯策略均衡"><a href="#第一步：检查是否存在纯策略均衡" class="headerlink" title="第一步：检查是否存在纯策略均衡"></a>第一步：检查是否存在纯策略均衡</h4><p>我们首先检查是否存在一个稳定的单元格，使得任何一方都不想单方面改变策略。</p>
<ol>
<li>如果列玩家（候选人2）选择“道德”，行玩家（候选人1）会选择“经济”（因为收益3 &gt; -2）。</li>
<li>如果列玩家选择“减税”，行玩家会选择“社会”（因为收益1 &gt; -1）。</li>
<li>如果行玩家选择“经济”，列玩家会选择“减税”（因为收益1 &gt; -3）。</li>
<li>如果行玩家选择“社会”，列玩家会选择“道德”（因为收益2 &gt; -1）。</li>
</ol>
<p>我们发现，不存在任何一个稳定的策略组合。例如，在（经济, 道德）这个组合，行玩家很满意，但列玩家会想换到“减税”策略以获得更好的收益。因此，<strong>该博弈没有纯策略纳什均衡</strong>。我们必须寻找混合策略均衡。</p>
<h4 id="第二步：计算混合策略纳什均衡"><a href="#第二步：计算混合策略纳什均衡" class="headerlink" title="第二步：计算混合策略纳什均衡"></a>第二步：计算混合策略纳什均衡</h4><p>我们将使用<strong>无差异原则</strong>来求解。</p>
<ul>
<li>设行玩家（候选人1）以概率 <strong>x</strong> 选择“经济”，以概率 <strong>(1-x)</strong> 选择“社会”。</li>
<li>设列玩家（候选人2）以概率 <strong>y</strong> 选择“道德”，以概率 <strong>(1-y)</strong> 选择“减税”。</li>
</ul>
<p><strong>A. 计算行玩家的策略 x</strong></p>
<p>行玩家需要选择一个概率 <code>x</code>，使得列玩家对于选择“道德”还是“减税”感到<strong>无差异</strong>（期望收益相等）。</p>
<ul>
<li>列玩家选择“道德”的期望收益 = x⋅(−3)+(1−x)⋅(2)</li>
<li>列玩家选择“减税”的期望收益 = x⋅(1)+(1−x)⋅(−1)</li>
</ul>
<p>令二者相等：</p>
<script type="math/tex; mode=display">-3x + 2 - 2x =  - 1 + x$$$$2 - 5x = 2x - 1$$$$3 = 7x$$x=73

所以，**候选人1的最优策略**是：以 **3/7** 的概率主打“经济”议题，以 **4/7** 的概率主打“社会”议题。

**B. 计算列玩家的策略 y**

同样，列玩家需要选择一个概率 `y`，使得行玩家对于选择“经济”还是“社会”感到**无差异**。

- 行玩家选择“经济”的期望收益 = y⋅(3)+(1−y)⋅(−1)
- 行玩家选择“社会”的期望收益 = y⋅(−2)+(1−y)⋅(1)

令二者相等：

$$3y - 1 + y = -2y + 1 - y$$$$4y - 1 = 1 - 3y$$$$7y = 2$$y=72

所以，**候选人2的最优策略**是：以 **2/7** 的概率主打“道德”议题，以 **5/7** 的概率主打“减税”议题。

#### 第三步：计算博弈的价值

博弈的价值（Value）就是指在双方都采取最优策略时，行玩家的期望收益。我们可以将 y = 2/7 代入行玩家的任一期望收益公式：

V=3y−1+y=4y−1=4⋅(72)−1=78−1=71

#### 结论

- **纳什均衡**：该博弈的唯一纳什均衡是，候选人1采取混合策略 (73,74)，候选人2采取混合策略 (72,75)。这意味着两位候选人都应该以特定的概率随机选择自己的竞选焦点，让对手无法预测。
- **博弈价值**：该博弈对行玩家（候选人1）的价值是 **+1/7**。在竞选这个议题上，如果双方都采取最理性的策略，**候选人1平均可以期望净赚 1/7 百万（约14万）的选民**。这表明在这个特定的战略矩阵中，候选人1拥有微弱的优势。

![image-20250625214139041](C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250625214139041.png)

备注：下面这段话有提到“这代表了**列玩家的理性反应**。由于这是一个零和博弈，列玩家的目标是让行玩家的收益尽可能小。”，结合上面一张PPT的最后一个恒等式，可知，由于这是零和博弈，行玩家收益最少等价于列玩家收益最大。

------



这张幻灯片从一个更形式化、更根本的角度，展示了作为“领导者”（被迫先宣布策略的一方）应该如何思考，并将这个问题转化为了一个标准的数学优化问题。最终，它揭示了一个关于零和博弈的深刻结论。

#### 1. Maximin 公式解读：“在最坏的情况里做到最好”

幻灯片给出的第一个公式 

maxmin(3x1−2x2,−x1+x2)

 是“最大化最小值 (Maximin)”思想的完美数学体现。我们来拆解它：

- **内部的两个表达式**：
  - 3x1−2x2：这是当列玩家选择“道德”时，行玩家的期望收益。
  - −x1+x2：这是当列玩家选择“减税”时，行玩家的期望收益。
- **min(...) 部分**：这代表了**列玩家的理性反应**。由于这是一个零和博弈，列玩家的目标是让行玩家的收益尽可能小。所以，当行玩家宣布了一个策略 (x1,x2) 后，列玩家会审视这两个可能的收益，并选择那个能让行玩家收益**更小 (min)** 的策略来应对。这代表了行玩家在宣布策略 (x1,x2) 后，所能得到的**最坏结果保证**。
- **max(...) 部分**：这代表了**行玩家的决策**。行玩家知道对手会这样针对他。所以，他在选择自己的策略 (x1,x2) 时，目标就是让这个“最坏的结果保证”变得尽可能好。也就是要**最大化 (max)** 那个最小的收益。

这整个公式的逻辑就是：“我（行玩家）要选择一个策略 (x1,x2)，来最大化‘在我宣布这个策略后，对手尽最大努力打压我，我能得到的那个保底收益’”。

#### 2. 线性规划的转换：从博弈论到标准数学优化

Max-min 问题在数学上直接求解不方便，但可以非常巧妙地转化为一个标准的**线性规划 (Linear Programming, LP)** 问题。

- 我们引入一个新变量 `z`，让它代表那个“保底收益”。
- 我们的目标是 `最大化 z`。
- 约束条件是什么？`z` 必须小于等于所有可能的结果。所以：
  - z≤3x1−2x2  (保底收益不能超过对手选“道德”时我的收益)
  - z≤−x1+x2  (保底收益不能超过对手选“减税”时我的收益)
- 再加上概率本身的基本约束 x1+x2=1 和 x1,x2≥0。

这样，我们就把一个博弈问题，变成了一个可以用标准算法（如单纯形法）求解的数学问题。

#### 3. 最终结论：结果与之前完全相同！

幻灯片最后指出，解这个线性规划得到的结果是 x1=3/7，x2=4/7，并且最优的保底收益 z=1/7。

这正是我们之前用“无差异原则”为**同时博弈**计算出的**纳什均衡解**和**博弈价值 V**！

为什么会这样？

这并非巧合，而是冯·诺伊曼极小化极大定理 (Minimax Theorem) 的直接体现。该定理的核心是：

Maximin=Minimax

- 我们这张幻灯片求解的“领导者-跟随者”问题，正是行玩家的 **Maximin（最大化最小值）** 问题。
- 我们之前求解的“同时博弈”问题，得到的是**Minimax（极小化极大值）** 均衡解。

这个定理保证了，在任何两人零和博弈中，这两个值是相等的。这意味着，**在两人零和博弈中，不存在先手优势或后手优势**。你提前公布策略，虽然给了对方信息，但你也可以利用这一点来选择一个最稳妥的策略；对方虽然能看到你的策略，但也只能在你设定的框架内做出反应。最终双方的力量会完美抵消，达到同一个均衡结果。

## 3.3 纳什均衡的线性规划解法

![image-20250625215726625](C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250625215726625.png)

这张幻灯片介绍了一个在线性规划乃至整个优化理论中，都极具美感和威力的核心概念——**对偶性**。它揭示了每个优化问题都存在一个“影子问题”或“镜像问题”，而理解这个镜像能为我们提供关于原问题全新的、深刻的洞察。

#### 1. 什么是对偶性？一个直观的例子

要理解对偶，与其陷入复杂的数学转换，不如看一个经济学例子：

- **原问题 (Primal Problem)**：
  - 想象你是一家工厂的老板，你要决定生产多少桌子和椅子，来**最大化你的总利润**。
  - 你面临一些**约束**：你拥有的木材、劳动力、设备时间都是有限的。
  - 这就是一个典型的线性规划问题：最大化一个目标（利润），同时满足一系列约束（资源）。
- **对偶问题 (Dual Problem)**：
  - 现在，想象一个商人想来收购你所有的资源（木材、劳动力、设备）。他想**最小化他的收购总成本**。
  - 但他面临一个**约束**：他为每种资源开出的“影子价格”组合，必须能让你觉得“卖掉资源比我自己生产产品更划算”。例如，生产一张桌子需要消耗的资源，他打包收购的价格，必须不能低于你自己生产这张桌子能获得的利润。
  - 这个商人的问题——在一定约束下最小化成本——就是你那个最大化利润问题的“对偶问题”。

#### 2. 对偶性的“魔力”：原问题与对偶问题的关系

对偶理论中有两个核心定理，它们揭示了原问题和对偶问题之间的奇妙关系：

1. **弱对偶定理 (Weak Duality)**：对偶问题的最优解，永远是原问题最优解的一个“界限”。在上面的例子里，就是说：商人收购资源的**最小成本**，必然**大于等于**工厂老板自己生产的**最大利润**。这很直观，因为如果收购成本低于你的利润，你肯定不会卖。

2. 强对偶定理 (Strong Duality)：在绝大多数情况下，这个“大于等于”实际上是**“完全等于”**！也就是说：

   工厂能实现的最大利润=商人收购资源的最小成本

   这是一个非常深刻的结论。它意味着，你资源的内在价值，恰好等于你能用它们创造的最大利润。对偶问题中的变量（资源的“影子价格”），精确地量化了每一种稀缺资源的边际价值。

#### 3. 对偶性与“零和博弈”的惊人联系

现在，我们可以把这个概念带回我们之前讨论的博弈论了。这正是引入对偶概念的关键所在。

- 在前几张幻灯片中，我们把**行玩家**的问题构建成了一个线性规划：**最大化**他的保底收益 `V`。这可以看作是我们的**原问题 (Primal)**。
- 那么，**列玩家**的问题是什么？列玩家的目标是**最小化**行玩家能获得的最大收益 `W`。我们同样可以把列玩家的这个问题也构建成一个线性规划。
- **最关键的结论是**：**列玩家的“最小化极大值”线性规划问题，恰好就是行玩家“最大化最小值”线性规划问题的对偶问题！**

因此，线性规划的“强对偶定理”（最大值 = 最小值），在零和博弈的语境下，就直接变成了冯·诺伊曼的“极小化极大定理”！

max(行玩家的保底收益)=min(列玩家的最大损失)Maximin=Minimax

**总结**：对偶性不仅是线性规划的强大工具，它还为博弈论的基石——极小化极大定理——提供了最坚实的数学证明。它优美地揭示了，一个参与者的最大化问题和其对手的最小化问题，实际上是同一个数学结构的两个不同侧面，如同一枚硬币的两面，其价值必然相等。

![image-20250625220404379](C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250625220404379.png)

线性规划对偶性实例。

![image-20250625220536283](C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250625220536283.png)

弱对偶性和强对偶性

## 3.4 线性互补问题

![image-20250625222636285](C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250625222636285.png)

**标题：计算两人一般和博弈的纳什均衡 (Computing Nash equilibria of two-player, general-sum games)**

- 不幸的是，寻找一个两人**一般和博弈 (general-sum game)** 的纳什均衡，**无法**被构建成一个线性规划问题。
  - — 两名参与者的利益不再是***完全\*对立的 (completely opposed)**。
  - — 然而，我们仍然可以将我们的问题表述为某种优化问题。

------

这张幻灯片标志着一个重要的转折点。从“零和博弈”进入了更普遍、也更复杂的“一般和博弈”（或称“非零和博弈”）的世界。幻灯片的核心信息是：之前强大而高效的线性规划（LP）工具，在这里失效了。

#### 1. 什么是一般和博弈？

一般和博弈指的是，在任何一个结果下，所有参与者的收益之和**不一定为零**。这意味着博弈的结果可以是双赢、双输，或者一方赢多、一方输少。参与者的关系不再是“你死我活”的纯粹冲突，而是**冲突与合作并存**。

- 经典例子1：囚徒困境

  如果两个囚犯都背叛对方，他们可能各判5年（总收益-10）。如果他们都保持沉默，可能各判1年（总收益-2）。这是一个双输的“负和”博弈。

- 经典例子2：性别大战 (Battle of the Sexes)

  一对情侣都想待在一起（合作），但一人想看歌剧，另一人想看球赛（冲突）。如果他们去同一个地方，双方都能获得高收益（比如（5,2）或（2,5）），总收益为7。如果去不同地方，则两人都很不开心（0,0），总收益为0。这是一个“正和”博弈。

#### 2. 为什么线性规划会失效？—— 对偶性的崩塌

这是理解这张幻灯片最关键的地方。线性规划之所以能在零和博弈中大显神威，是因为其背后优美的“对偶性”，而这种对偶性源于双方利益的**完全对立**。

- **在零和博弈中**：
  - 行玩家的目标是：最大化自己的收益 `u₁`。
  - 列玩家的目标是：最大化自己的收益 `u₂`。
  - 由于 `u₂ = -u₁`，所以列玩家“最大化`u₂`”就**等价于**“最小化`u₁`”。
  - 因此，行玩家的“最大化最小值 (Maximin)”问题和列玩家的“最小化极大值 (Minimax)”问题，形成了一对完美的数学**对偶**。它们就像一枚硬币的两面，可以用同一个线性规划框架来解决。
- **在一般和博弈中**：
  - `u₂` 不再等于 `-u₁`。
  - 列玩家的目标——最大化他自己的 `u₂`——与行玩家的收益 `u₁` **没有直接的、负相关的关系**。
  - 列玩家不再是处心积虑地要让行玩家的收益最小化，他只关心自己的收益。
  - 这样一来，那种“我之所得即你之所失”的完美对偶关系就**彻底崩塌**了。我们无法再构建出一个单一的线性规划问题来同时描述双方的决策并找到那个共同的解。

#### 3. 那问题变成了什么？

虽然不能用线性规划，但寻找纳什均衡依然是一个数学优化问题，只是变得更复杂了。

- 寻找两人一般和博弈的纳什均衡，在数学上等价于求解一个**线性互补问题 (Linear Complementarity Problem, LCP)**。这是一个比线性规划更复杂的数学结构。
- 从计算复杂性的角度看，求解两人零和博弈是“容易”的（属于 **P** 问题），而求解两人一般和博弈则被证明是**PPAD-完全 (PPAD-complete)** 问题。这通常被认为是一个“更难”的计算等级，意味着找到解需要更复杂的算法，计算效率也更低。

**总结**：从“零和”到“一般和”的转变，是博弈论中一次巨大的复杂性飞跃。它让我们失去了线性规划这个简洁高效的工具，迫使我们进入一个更困难的计算领域。这也反过来凸显了两人零和博弈及其“极小化极大定理”在理论上的简洁与优美。

![image-20250625223646101](C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250625223646101.png)

**标题：计算两人一般和博弈的纳什均衡**

- 我们首先考虑一个**内部的 (inner)** 或**完全混合的 (totally mixed)** 纳什均衡 (X∗,Y∗)，即对于所有的 i 和 j，都有 xi∗>0 以及 yj∗>0（所有的纯策略都以正概率被使用）。

- 让 ai 表示行玩家的支付矩阵A的各行，让 bj 表示列玩家的支付矩阵B的各列。

- 利用“**在一个纳什均衡策略的支持集(support)中，所有纯策略都产生相同的支付**”这一事实，并且该支付大于或等于支持集之外的策略的支付，我们得到：

  - aiy∗=aky∗,i,k=1,2,…,m.

  - (x∗)Tbj=(x∗)Tbk,j,k=1,2,…,n.

    (注：此处公式经过订正以符合标准表达)

    (绿色文字) 假设每个纯策略都以正概率被使用。

- 上述构成了一个可以被高效求解的线性方程组。

#### **下图内容**

- 然而，“每个策略都以正概率被使用”的假设是有限制性的。大多数博弈并不拥有完全混合的纳什均衡；对于它们而言：

- 我们计算一个有限两人博弈的所有纳什均衡：

  一个混合策略组合 (x∗,y∗) 是一个具有支持集 S1,S2 的纳什均衡，当且仅当：

  - u=aiy∗,∀i∈S1 （对于支持集内的策略i，收益都等于均衡收益u）
  - u≥aiy∗,∀i∈/S1 （对于支持集外的策略i，收益不高于u）
  - v=(x∗)Tbj,∀j∈S2 （对于支持集内的策略j，收益都等于均衡收益v）
  - v≥(x∗)Tbj,∀j∈/S2 （对于支持集外的策略j，收益不高于v）
  - xi∗=0,∀i∈/S1, yj∗=0,∀j∈/S2  （u, v 是NE中的收益值）

- 要让上述过程可行，我们需要找到正确的**支持集 (supports)**。我们需要遍历所有可能的支持集组合。由于存在 2n+2m 种不同的支持集，这会导致算法具有指数级的复杂度。

- **备注 (Remark)**：计算有限博弈的纳什均衡的**计算复杂度**，就在于**找到正确的支持集**。

------

这两张幻灯片讲述了一个关于“求解一般和博弈”的完整故事：从一个理想化的、简单的特例，到一个普遍的、困难的现实。

#### 1. 理想情况：完全混合均衡（上图）

幻灯片的上半部分描绘了一种“完美”的均衡状态，即**完全混合均衡**。在这种均衡里，每一个参与者都认为对手的所有可选策略都值得提防，因此自己的最优策略是给自己的每一个选项都分配一个**大于零**的概率。

- 为什么这种情况简单？

  因为它使得“无差异原则”可以应用到所有策略上。为了让对手混合他的所有策略，你必须让你的对手在选择他的任何一个策略时，期望收益都完全相等。

- 如何求解？

  这就产生了一个完整的线性方程组（m-1个关于行玩家收益的等式，n-1个关于列玩家收益的等式，再加上两个概率和为1的等式）。这是一个标准的、可以用我们熟悉的方法高效求解的数学问题。

然而，这种所有策略都被用上的“雨露均沾”式的均衡，在现实中非常罕见。

#### 2. 现实情况：寻找“支持集”（下图）

幻灯片的下半部分指出了残酷的现实：在绝大多数博弈中，通常都会有一些策略是“劣势策略”或“糟糕的选项”，一个理性的玩家是永远不会使用它们的（即使用概率为0）。

- **支持集 (Support)**：在一个混合策略中，那些**真正以正概率被使用的纯策略的集合**，被称为这个混合策略的“支持集”。
- **核心困难**：求解的关键困难在于，我们**事先并不知道**最终的均衡解中，到底哪些策略会是“优势策略”（在支持集里），哪些是“劣势策略”（在支持集外）。

这就引出了一个计算上的巨大难题，我喜欢称之为**“寻找嫌疑人”的困境**：

- **无差异原则**就像一个完美的“审讯工具”，只要你把正确的“嫌疑人”（支持集里的策略）找来，它就能告诉你每个人的详细“作案手法”（混合策略的精确概率）。
- **但问题是**，你不知道谁是真正的“嫌疑人”。你面对着所有可能的策略，不知道该把哪些策略纳入“无差异”的审讯中。

#### 3. “暴力搜索”算法及其指数级复杂度

理论上，我们可以用一种“暴力”的方法来找到所有均衡：

1. **猜测**：我们先猜一个可能的支持集组合。例如，“我猜行玩家只会用策略1和3，而列玩家只会用策略2和4”。
2. **求解**：基于这个猜测，我们建立一个只包含这些策略的线性方程组（即只让这些策略满足无差异原则）并求解。
3. **验证**：检查解出的结果是否是一个合法的纳什均衡。这包括两部分：
   - 解出的概率值是否都在0和1之间？
   - 对于那些我们**没猜**的“局外”策略，它们的期望收益是否真的**不优于**我们算出的均衡收益？（这是最关键的验证，确保没人想把局外策略拉进局内）。
4. **重复**：如果验证失败，就回到第一步，换一种支持集的猜测，然后重复整个过程，直到遍历完**所有可能的支持集组合**。

这个方法的致命缺陷在于，支持集的组合数量是**指数级增长**的。如果一个博弈双方各有20个策略，那么可能的支持集组合数量会是一个天文数字。这就是幻灯片所说的“指数级复杂度”。

**总结**：寻找一般和博弈的纳什均衡，其计算上的困难**不在于解方程**，而在于**找到应该用哪些策略来列方程**。这个“寻找正确支持集”的组合搜索过程，是该问题计算复杂度（PPAD-complete）的根源，也使其与可以通过线性规划轻松求解的两人零和博弈产生了本质的区别。

## 7.1 多智能体强化学习介绍及基本概念

![image-20250630205121221](C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250630205121221.png)

这张幻灯片指出了从单智能体学习（比如我们熟知的AlphaGo下围棋的早期版本）迈向多智能体学习（比如王者荣耀或星际争霸中的AI）时，所遇到的一个根本性的、质的困难。

#### 1. 核心困难：移动的靶心 (The Moving-Target Problem)

- **在单智能体学习中**：环境是**静止的**或有固定规律的。一个智能体（比如一个机器人）学习走路，它只需要掌握如何应对地板、墙壁等物理规律即可。这个“游戏规则”是不会变的。
- **在多智能体学习中**：情况发生了根本变化。对于任何一个智能体（我们称之为“我”）来说，**其他的智能体也是环境的一部分**。但问题是，这些“其他的智能体”本身也在学习、在进化、在改变他们的策略。
  - **一个生动的例子**：你学习如何开车上班最快。如果只有你一个人在学习，而其他人的路线每天都固定不变，这就是一个简单的单智能体问题。但现实是，成千上万的司机（其他智能体）每天也都在尝试新的路线来优化他们自己的通勤时间。你今天发现的“最优路线”，明天可能因为有几百个和你一样的人也发现了它而变得极度拥堵。
  - 你试图瞄准的那个“最优策略”的靶心，因为他人的学习而**不断地移动**。这就是多智能体学习的核心困难，学术上称为**“环境的非平稳性” (Non-stationarity)**。

#### 2. 为什么简单的Q学习会失效？

标准的单智能体强化学习算法（如Q学习）之所以能成功，是因为它们依赖于一个基本假设：**马尔可夫决策过程 (MDP)**，即环境是平稳的。这意味着，在同一个状态（State）下，采取同一个动作（Action），得到的奖励（Reward）和状态转移的概率应该是基本一致的。

但在多智能体环境中，这个假设被彻底打破了：

- **今天**：在路口（状态S），你选择直行（动作A），因为其他人都选择了右转，所以一路畅通，你获得了很高的奖励。你的Q表格会更新，认为`(S, A)`是个好选择。
- **明天**：在同一个路口（状态S），你根据昨天的经验再次选择直行（动作A）。但昨天和你一样选择直行的其他智能体也获得了高奖励，所以今天他们也选择直行。结果造成了交通堵塞，你得到了一个很低的奖励。你的Q表格又必须更新，认为`(S, A)`是个坏选择。

你的Q值会这样剧烈地来回震荡，可能永远无法收敛到一个稳定的策略，因为一个动作的“好”与“坏”不再是固定的，而是完全取决于其他智能体当前正在执行的策略。

#### 3. 与博弈论的联系

这个学习过程中的“不稳定”问题，正是我们在前面博弈论部分看到的“均衡”问题的动态体现。

- 多智能体学习的目标，往往就是让这群智能体通过学习和试错，最终能够收敛到整个博弈的**纳什均衡**。
- 幻灯片中的螺旋图可以这样理解：中心点是博弈的纳什均衡点。这条螺旋线代表了所有智能体的联合策略随着时间演变的轨迹。如果学习算法设计得好（例如，在**势博弈**中），这条轨迹就会像图中一样稳定地**收敛**到中心。
- 如果算法设计不当或者博弈本身就很“恶劣”（比如“石头剪刀布”），那么学习过程可能永远无法收敛，只会在策略空间中不停地“绕圈子”。

**总结**：多智能体学习的困难在于，每个智能体的学习过程都会改变其他智能体的学习环境，形成一个复杂且动态的“移动靶心”问题。简单地将单智能体算法直接套用，会因环境的“非平稳性”而失效。因此，现代多智能体学习研究的核心，就是设计出能够在这种动态博弈中稳定地学习、并最终收敛到纳什均衡等合理状态的算法。

![image-20250630205734872](C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250630205734872.png)

我们可以通过两个关键问题来区分这些模型：

1. **有几个决策者（智能体）？** 一个还是多个？
2. **有多少种情况（状态）？** 一种还是多种？

#### 1. 马尔可夫决策过程 (MDPs): 单人探索世界

- **特征**：**一个**智能体，**多个**状态。
- **核心问题**：一个独立的决策者，在一个可以变化的环境中，如何学习一系列的动作以最大化其长期回报。
- **通俗例子**：一个机器人学习走迷宫。机器人是**唯一的智能体**，迷宫中不同的位置就是**不同的状态**。机器人需要学习在每个位置（状态）下，应该朝哪个方向走（动作），才能最快找到出口（最大化回报）。
- **地位**：这是现代**单智能体强化学习 (Single-Agent Reinforcement Learning)** 的数学基石。我们熟知的AlphaGo下棋，本质上也可以看作是在一个极度复杂的MDP中寻找最优策略。

#### 2. 重复博弈 (Repeated Games): 多人重复同一场游戏

- **特征**：**多个**智能体，**一个**状态。
- **核心问题**：多个决策者，反复地玩**同一个**博弈。
- **通俗例子**：两家寡头公司，每个月都要决定自己的产品定价。每个月的定价博弈，其本身的收益矩阵都是一样的，所以可以看作是**单一状态**。但因为博弈是**重复**的，今天的决策会影响声誉，从而影响对手明天的决策。这就引入了如“以牙还牙 (Tit-for-Tat)”这样的动态策略。我们之前讨论的各种矩阵博弈（如囚徒困境、零和博弈），如果将它们连续玩很多次，就构成了重复博弈。

#### 3. 随机博弈 / 马尔可夫博弈 (Stochastic/Markov Games): 多人探索动态世界

- **特征**：**多个**智能体，**多个**状态。
- **核心问题**：多个决策者在一个动态变化的环境中共同决策，他们的联合行动会共同决定环境如何进入下一个状态。
- **通俗例子**：一场足球比赛。场上有**多个智能体**（双方队员）。球和所有队员在场上的位置，共同构成了一个**状态**。当大家做出动作（跑动、传球、射门）后，场上的局面会变成一个**新的状态**。在每个不同的状态下，球员们面临的“局部博弈”也是不同的。
- **地位**：这是最普适、最复杂的模型，它构成了**多智能体强化学习 (Multi-Agent Reinforcement Learning, MARL)** 的理论基础。

总结与联系：

这张图清晰地告诉我们，随机博弈是“集大成者”，它统一了另外两个模型：

- 当随机博弈的智能体数量减少到1个时，它就退化成了**MDP**。
- 当随机博弈的状态数量减少到1个时，它就退化成了**重复博弈**。

这与我们上一张幻灯片讨论的**“多智能体学习的困难”**完美地衔接了起来。我们所说的“环境的非平稳性”，正是因为我们身处**随机博弈**的框架中：对于“我”这个智能体而言，环境之所以看起来在不停变化，是因为环境的下一个状态不仅取决于我的行动，还取决于其他所有智能体的行动，而他们本身也在学习和改变。

![image-20250630214131295](C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250630214131295.png)

这张幻灯片提出了一个非常经典的任务：为任意一个 2x2 零和博弈找到通用的解法，即用矩阵中的参数 a,b,c,d 来表达博弈的价值和双方的最优策略。

我们将遵循幻灯片给出的两步计划来完成这个推导。

#### 步骤一：检验纯策略均衡（鞍点）

一个纯策略纳什均衡（在零和博弈中也称为“鞍点”）存在的条件是：**某一个收益值，既是其所在行的最小值，又是其所在列的最大值**。

我们可以通过比较行玩家的“最大化最小值（Maximin）”和列玩家的“最小化极大值（Minimax）”来判断：

- 行玩家的保底收益（最大最小值）: max(min(a,b),min(d,c))
- 列玩家的保底收益（最小极大值）: min(max(a,d),max(b,c))

如果这两个值相等，则存在纯策略均衡，该值就是博弈的价值。例如，如果 `a <= b` 并且 `a >= d`，那么 `a` 就是一个鞍点，（行1，列1）就是纯策略纳什均衡。

#### 步骤二：求解混合策略均衡（假设不存在鞍点）

如果不存在鞍点，那么双方的最优策略必然是混合策略。我们将使用**无差异原则**来求解。

- **策略设定**:

  - 行玩家以概率 **p** 选择“行1”，以概率 **(1-p)** 选择“行2”。
  - 列玩家以概率 **q** 选择“列1”，以概率 **(1-q)** 选择“列2”。

- 推导行玩家的策略 p:

  行玩家选择 p 的目的是让列玩家在“列1”和“列2”之间感到无差异。

  - 列玩家选择“列1”的期望收益（注意，要用列玩家的收益矩阵`-A`）：p(−a)+(1−p)(−d)

  - 列玩家选择“列2”的期望收益：p(−b)+(1−p)(−c)

    令二者相等：

    $$-ap - d(1-p) = -bp - c(1-p)$$$$-ap - d + dp = -bp - c + cp$$$$d - c = p(a - d - b + c) = p((a+c) - (b+d))</script><pre><code>解得行玩家的最优概率 p：

p∗=(a+c)−(b+d)d−c
</code></pre><ul>
<li><p>推导列玩家的策略 q:</p>
<p>列玩家选择 q 的目的是让行玩家在“行1”和“行2”之间感到无差异。</p>
<ul>
<li><p>行玩家选择“行1”的期望收益：q(a)+(1−q)(b)</p>
</li>
<li><p>行玩家选择“行2”的期望收益：q(d)+(1−q)(c)</p>
<p>令二者相等：</p>
<script type="math/tex; mode=display">aq + b(1-q) = dq + c(1-q)$$$$aq + b - bq = dq + c - cq$$$$b - c = q(d - a - c + b) = q((b+d) - (a+c))</script><p>解得列玩家的最优概率 q：</p>
<p>q∗=(b+d)−(a+c)b−c=(a+c)−(b+d)c−b</p>
</li>
</ul>
</li>
</ul>
<h4 id="推导博弈的价值-V"><a href="#推导博弈的价值-V" class="headerlink" title="推导博弈的价值 V"></a>推导博弈的价值 V</h4><p>博弈的价值 V 是在均衡状态下，行玩家的期望收益。我们可以将求得的 q* 代入行玩家的期望收益公式：</p>
<p>V=aq∗+b(1−q∗)=q∗(a−b)+bV=(a+c)−(b+d)c−b(a−b)+b</p>
<p>对上式进行通分并化简：</p>
<p>V=(a+c)−(b+d)(c−b)(a−b)+b((a+c)−(b+d))</p>
<script type="math/tex; mode=display">V = \frac{ac - bc - ab + b^2 + ab + bc - b^2 - bd}{(a+c) - (b+d)}$$分子中的 -bc, +bc, -ab, +ab, +b², -b² 项都相互抵消了，最终得到一个非常优美的公式：

V=(a+c)−(b+d)ac−bd

------

### 最终通解

对于一个不存在纯策略均衡的 2x2 零和博弈矩阵 A=(adbc)，其通解为：

1. 行玩家的最优混合策略 (p∗,1−p∗)：

   p∗=(a+c)−(b+d)d−c

2. 列玩家的最优混合策略 (q∗,1−q∗)：

   q∗=(a+c)−(b+d)c−b

3. 该博弈对行玩家的价值 V：

   V=(a+c)−(b+d)ac−bd

### 为什么行玩家是最大化最小值，而不是最小化极大值？

问得特别好！这正是理解零和博弈中双方战略思想的精髓所在。

简单来说，答案是：**因为他们的根本目标不同。**

- **行玩家的目标**：最大化自己的收益。
- **列玩家的目标**：最大化自己的收益，这在零和博弈中就等价于**最小化行玩家的收益**。

让我们一步一步地、站在**行玩家（Player 1）**的角度来思考他为什么必须是“最大化最小值 (Maximin)”。

### 1. 行玩家的思考过程：“我如何为最坏的情况做准备？”

行玩家是一个理性的决策者，他想让自己赚得尽可能多。但他知道两件事：

- 他的收益不只取决于自己，还取决于对手的选择。
- 他的对手（列玩家）的目标和他完全相反。他每多赚一块钱，就意味着对手要多亏一块钱。所以，对手会想尽一切办法让他赚得最少。

基于这个“对手会尽力坑我”的假设，行玩家必须采取一种非常谨慎和保守的策略。他的思考逻辑如下：

1. 审视自己的第一个选项（比如“行1”）：

   “如果我选择‘行1’，我的对手会怎么做？他会看遍‘行1’的所有结果，然后选择那个能让我收益最低的选项。所以，如果我出‘行1’，我能得到的保底收益就是这一行里的最小值。”

2. 审视自己的第二个选项（比如“行2”）：

   “同理，如果我选择‘行2’，理性的对手也会选择那一列来让我获得‘行2’里的最低收益。这个最小值就是我出‘行2’的保底收益。”

3. 做出最终决策：

   现在，行玩家手上有一份“保底收益清单”，清单上的每一项都对应着他选择某一行后，在最坏情况下能得到的最低收益。作为一个想最大化自己收益的人，他会看着这份“保底清单”，然后选择那个能提供**最高（Maximum）**保底收益的选项。

整个过程串起来就是：他先找出每一行的**最小值 (Minimum)**，然后在这些最小值中，选择一个**最大值 (Maximum)**。这就是**最大化最小值 (Maximin)**。

这是一种“在最坏的情况里，为自己争取最好的结果”的策略。

------

### 2. 为什么“最小化极大值 (Minimax)”是对手的策略？

现在我们换位思考，站到**列玩家（Player 2）**的角度。

1. **他的目标**：他的目标是最小化自己的损失，也就是最小化行玩家的收益。
2. **他的思考过程**：
   - **审视他的第一个选项（比如“列1”）**：“如果我出‘列1’，我的对手会怎么做？他会看遍‘列1’的所有结果，然后选择那个能让他自己收益**最高**的选项。所以，如果我出‘列1’，我可能遭受的**最大损失**（也就是行玩家的最大收益）就是这一列里的**最大值**。”
   - **审视他的第二个选项（比如“列2”）**：“同理，如果我出‘列2’，我可能遭受的最大损失就是‘列2’里的**最大值**。”
   - **做出最终决策**：现在，列玩家手上也有一份“最大损失清单”。作为一个想最小化自己损失的人，他会看着这份清单，然后选择那个能让他的**最大损失变得最小 (Minimum)** 的选项。

整个过程串起来就是：他先找出每一列的**最大值 (Maximum)**，然后在这些最大值中，选择一个**最小值 (Minimum)**。这就是**最小化极大值 (Minimax)**。

### 总结

|          | **行玩家 (Player 1)**            | **列玩家 (Player 2)**                    |
| -------- | -------------------------------- | ---------------------------------------- |
| **目标** | 最大化自己的收益                 | 最小化自己的损失（即最小化行玩家的收益） |
| **思想** | 在最坏的情况里为自己争取最好结果 | 让对手在最好的情况下也占不到太大便宜     |
| **策略** | **最大化最小值 (Maximin)**       | **最小化极大值 (Minimax)**               |

所以，行玩家之所以是“最大化最小值”，是因为他的出发点是**为自己求利**，同时假设对手会让他**利无可利**；而“最小化极大值”则是从对手的视角出发，目标是**让行玩家的利益最小化**。冯·诺依曼的惊人发现在于，在两人零和博弈中，这两种看似不同的思考路径，最终会指向同一个均衡解。

![image-20250630215303870](C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250630215303870.png)

这张幻灯片介绍了“序贯决策”框架中那个最普适、最强大的模型——**随机博弈（或称马尔可夫博弈）**。它完美地将我们之前讨论过的“矩阵博弈”和“马尔可夫决策过程(MDP)”融合在了一起，是理解现代**多智能体强化学习 (MARL)** 的理论基石。

#### 1. 集大成者：融合了博弈论与强化学习

我们可以这样理解随机博弈的构成：

- 它借鉴了**矩阵博弈 (Matrix Games)** 的核心思想：在任何一个时刻，都有**多个智能体**在进行策略互动。
- 它又借鉴了**马尔可夫决策过程 (MDPs)** 的核心思想：整个系统存在**多个状态**，环境会根据参与者的行动从一个状态转移到另一个状态。

**随机博弈 = 多智能体的MDP = 随状态变化的重复博弈**

#### 2. 图示解读：一场动态演变的博弈

这张图非常直观地展示了随机博弈是如何运作的。我们可以把它想象成一场动态的冒险游戏：

1. **身处状态**：假设你和你的对手（参与者1和2）当前身处“**状态1 (State 1)**”。
2. **进行博弈**：在这个状态下，你们必须玩“状态1”对应的那个2x2矩阵博弈。比如，你（P1）选择了“下”，对手（P2）选择了“右”。
3. **获得即时回报**：根据矩阵，这个`(下, 右)`的联合行动会给你们带来 `(1, 1)` 的即时回报。
4. **世界发生改变（状态转移）**：这是最关键的一步。你们的联合行动 `(下, 右)` 触发了状态转移。紫色的箭头告诉我们，接下来会发生什么：
   - 有 **40%** 的概率，你们会进入“**状态2**”。
   - 有 **60%** 的概率，你们会进入“**状态3**”。
5. **进入新博弈**：假设你们进入了“状态2”。现在，你们面对的是一个**全新的2x2矩阵博弈**，有着完全不同的收益规则。你们需要在这个新规则下再次决策，然后获得新的回报，并再次触发新的状态转移。

这个过程会一直持续下去。

#### 3. 参与者的目标：深谋远虑

在一个随机博弈中，一个理性的参与者不会只盯着当前这一轮的得失。他的决策必须是**深谋远虑**的。

- 例如，在“状态1”中，`(上, 左)`这个选择能立刻带来`(2, 2)`的高回报。但如果这个选择有90%的概率会让你转移到一个对你极其不利的“惩罚状态”，那你可能就不会选它。
- 反之，你可能会选择一个即时回报较低的行动，如果它有很大概率把你带到一个未来回报极高的“天堂状态”。
- 这就是**折扣回报 (discounted rewards)** 的作用。玩家的目标是最大化未来所有回报的“总现值”，即找到一个在**所有状态下**都最优的策略（Policy），而不仅仅是当前状态。

**总结**：随机博弈为我们描绘了一幅最接近真实世界复杂性的图景——多个决策者在不断变化的环境中持续互动。我们之前讨论的**多智能体学习（MAL）的困难**，例如“环境的非平稳性”，正是源于这个框架。对于任何一个智能体来说，环境之所以“不稳定”，就是因为状态的转移和回报不仅取决于自己的行动，还取决于其他所有同样在学习和适应的智能体的行动。

![image-20250630220528288](C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250630220528288.png)

**标题：随机博弈 vs. MDP (Stochastic Games vs. MDP)**

- 在一个随机博弈中，如果除了一个参与者之外的所有其他参与者都采取**固定的 (fixed)** 策略，那么对于剩下的那个智能体来说，这个问题就**退化 (reverts back)** 回了一个MDP。
  - — 这是因为，固定其他智能体的策略（即使这些策略是随机的），会使得状态转移变得**马尔可夫化 (Markovian)**，即只取决于剩下的那个参与者的行动。

------

这张幻灯片通过一个“思想实验”，精准地指出了**多智能体学习（MARL）与单智能体学习（RL）的根本区别到底在哪里**。它告诉我们，多智能体问题的核心困难，并不在于“有多个会动的个体”，而在于“有多个会**学习和适应**的个体”。

#### 1. 问题复杂性的根源：变化的“游戏规则”

我们之前讨论，多智能体学习之所以困难，是因为环境的“非平稳性”（Non-stationarity）。对于任何一个智能体“我”来说，其他的智能体都是环境的一部分。当其他智能体也在学习、也在改变他们的策略时，就相当于“我”所面对的游戏规则本身在不断变化，这让学习变得极为困难。

#### 2. “固定策略”意味着什么？—— 从“对手”到“自然规律”

这张幻灯片提出的“固定其他所有人的策略”这个条件，是问题的关键。这意味着什么呢？让我们用一个生动的例子来说明：

- 情景A：随机博弈 (Stochastic Game)

  你是一个新手出租车司机（剩下的那个智能体），在一个大城市里学习如何最快地接送客人。城里还有成千上万的老司机（其他智能体），他们经验丰富，每天都在根据实时路况、新闻、个人习惯等调整自己的路线。这是一个极度复杂的多智能体学习问题，因为你的“环境”（即其他司机的行为）每天都在变。

- 情景B：退化为MDP

  现在，假设奇迹发生，城里所有其他司机都被换成了简单的机器人。这些机器人的行为遵循一套永不改变的固定程序。例如：“在周一上午8点的A路口，这群机器人有70%会右转，30%会直行”。这个概率是固定的，机器人不会再学习或改变了。

在这个时刻，对于你（唯一的人类司机）来说，发生了什么？

其他司机不再是具有主观能动性的“对手”了，他们变成了城市交通中一个虽然是随机的、但却是稳定的、可预测的自然规律。

#### 3. “马尔可夫化”：游戏规则被重新稳定下来

一旦其他人的策略被固定，整个系统对于“我”来说，就重新满足了**马尔可夫性质**。

- 当“我”在某个路口（状态s），选择直行（动作a），下一分钟会到达哪里、会花多长时间（下一个状态s'），其概率 **p(s′∣s,a)** 现在只取决于“我”的动作`a`和那群机器人固定的行为模式。
- 这个转移概率**不再随时间变化**了，因为机器人的“想法”不会变。
- 于是，这个复杂的、不可预测的多智能体博弈，就**退化**成了一个经典的、可解的**单智能体马尔可夫决策过程（MDP）**。

总结：

这张幻灯片通过“固定他人策略”这一巧妙的设定，帮助我们隔离并识别了多智能体问题的“困难之源”。困难不在于环境中存在多个行动者，而在于这些行动者策略的动态演化。这个洞见是许多高级多智能体学习算法的基础，例如，有些算法就是通过“轮流学习”（一个学习，其他暂时固定）或者对其他智能体的策略进行建模和预测，来试图克服这种“非平稳性”带来的挑战。

![image-20250630222547667](C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250630222547667.png)

这张幻灯片将原本简单的单次博弈，升级成了一个更复杂、也更贴近现实的**双状态随机博弈 (Two-state Stochastic Game)**。这里的“纳什均衡”不再是单个的行动组合，而是一个**策略组合 (a profile of policies)**，即每个厂商在**每种状态下**应该如何行动的完整计划。

这个问题的解取决于一个幻灯片上未给出但至关重要的参数——**折扣因子 γ (discount factor)**，它代表了厂商对未来收益的重视程度。

#### 1. 分解两个“子博弈”

我们首先分析在每个状态下，只考虑当前一轮收益的“短视”均衡是什么。

- 在状态1（无税收）：

  正如我们上一题分析的，双方都有一个占优策略：“污染”。因此，该状态下的短视纳什均衡是 (污染, 污染)。但这个选择会带来一个后果：根据转移概率 (0,1)，游戏将100%转移到状态2（有税收）。

- 在状态2（有税收）：

  我们分析这个新的利润矩阵：

  - 对厂商1：如果厂商2选择“清洁”，厂商1会选“污染”(4>1)；如果厂商2选“污染”，厂商1还是会选“污染”(3>0)。**“污染”是厂商1的占优策略**。
  - 对厂商2：如果厂商1选择“清洁”，厂商2会选“污染”(5>2)；如果厂商1选“污染”，厂商2还是会选“污染”(4>1)。**“污染”也是厂商2的占优策略**。
  - 因此，该状态下的短视纳什均衡也是 **(污染, 污染)**。这个选择的后果是，游戏将**100%留在状态2**，继续被征税。

#### 2. 长期战略的困境：短视 vs. 远见

分析完子博弈后，真正的战略困境浮现了。我们以厂商1在**状态1**的决策为例：

- **短视的选择（选择“污染”）**：可以立刻获得很高的收益（如果对方也污染，能得6）。但代价是，从下一轮开始，将永远陷入低收益的“状态2”。
- **远见的选择（选择“清洁”）**：会牺牲掉一部分即时收益（如果对方也清洁，只能得4）。但好处是，游戏将**100%留在高收益的“状态1”**，未来每一轮都可以继续获得高收益。

厂商会如何选择，完全取决于他们有多“远视”，即折扣因子 `γ` 有多大。

#### 3. 可能存在的纳什均衡

这个随机博弈可能存在多个纳什均衡。

**均衡A：“悲观”的污染均衡**

- **策略**：无论在哪种状态，双方都选择“污染”。
- **分析**：如果对方的策略是“永远污染”，那么你最好的应对也是“永远污染”。因为如果你单方面选择“清洁”，在状态1你会获得更低的即时收益(3 vs 6)然后还是会进入状态2；在状态2你单方面“清洁”的收益(0 vs 3)也更低。因此，没有任何一方有单方面改变策略的动机。
- **结论**：**（策略1=污染, 策略2=污染）是一个纳什均衡**。在这个均衡下，厂商们第一轮在状态1获得(6,7)的收益，然后永久地陷入状态2，每轮获得(3,4)的收益。这是一个低效的、“双输”的均衡。

**均衡B：“合作”的清洁均衡**

- **策略**：双方约定，只要在状态1，就都选择“清洁”。

- **分析**：要让这个“君子协定”成为一个稳定的纳什均衡，就必须保证“背叛”是无利可图的。

  - **遵守协定**的收益流（以厂商1为例）：4+4γ+4γ2+⋯=1−γ4

  - **单方面背叛**的收益流：在状态1选择“污染”获得一次性的高收益7，但之后游戏进入状态2，双方陷入“永远污染”的均衡，后续每轮收益为3。其收益流为: 7+3γ+3γ2+⋯=7+1−γ3γ

  - 要让大家遵守协定，必须满足“遵守的收益 ≥ 背叛的收益”：

    1−γ4≥7+1−γ3γ

    4≥7(1−γ)+3γ⟹4≥7−4γ⟹4γ≥3⟹γ≥43

- **结论**：**如果厂商们对未来的重视程度足够高（即折扣因子 γ ≥ 3/4），那么双方在状态1都选择“清洁”也可以成为一个纳什均衡**。因为对未来的长期高收益的渴望，足以抑制住当前“背叛”以获取短期利益的诱惑。

**总结**：这个引入了税收和状态转移的随机博弈模型，比单次博弈要复杂和深刻得多。它的均衡不再是唯一的，而是可能存在多个——一个“坏”的均衡和一个“好”的均衡。系统最终会落入哪个均衡，取决于参与者对未来的耐心和期望（由`γ`体现）。这也为政府政策的设计提供了启示：一个好的制度，应该让“合作”的门槛（即所需的`γ`值）尽可能低，让参与者更容易达成对社会有利的结果。

![image-20250630223314800](C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250630223314800.png)

**标题：例子：“大冒险”游戏 (Example: the game of Dare)**

- 参与者1，**领导者 (the leader)**，和参与者2，**挑战者 (the challenger)**，同时选择“**放弃 (pass)**”或“**挑战 (dare)**”。

  - — 如果双方都选择“放弃”，支付为零（且游戏结束）。
  - — 如果参与者1“放弃”而参与者2“挑战”，参与者1赢得1。
  - — 如果参与者1“挑战”而参与者2“放弃”，参与者1赢得3。
  - — 如果双方都选择“挑战”，这个基础游戏将**角色互换**后重新进行。
    - (领导者变成挑战者，反之亦然)。
  - — 如果参与者们永远地持续“挑战”下去，则支付为零。

- 博弈矩阵 G:

  $$G = \bordermatrix{ & \text{放弃} & \text{挑战} \cr \text{放弃} & 0 & 1 \cr \text{挑战} & 3 & -G^T }</script><p>  其中 -Gᵀ 代表角色互换后的游戏。（它的矩阵是G矩阵的转置的负数。）-Gᵀ的价值是G的价值的负数。</p>
<hr>
<p>这张幻灯片描述了一个非常有趣的<strong>递归博弈 (recursive game)</strong>，它是一种特殊的<strong>随机博弈 (stochastic game)</strong>。这个博弈只有两个状态：“P1是领导者”和“P2是领导者”。当出现 (挑战, 挑战) 的结果时，游戏就在这两个状态之间切换。</p>
<h4 id="1-问题的核心：递归的价值"><a href="#1-问题的核心：递归的价值" class="headerlink" title="1. 问题的核心：递归的价值"></a>1. 问题的核心：递归的价值</h4><p>这个问题的精髓在于右下角的那个支付 <code>-Gᵀ</code>。</p>
<ul>
<li><p>让我们设这个博弈对于<strong>当前的领导者</strong>来说，其<strong>价值 (Value)</strong> 为 <strong>V</strong>。</p>
</li>
<li><p>那么，领导者的支付矩阵就可以写成：</p>
<p>G=(031Value(subgame))</p>
</li>
<li><p>当双方都选择“挑战”时，游戏进入子博弈。在这个子博弈中，原先的挑战者（P2）变成了新的领导者。由于游戏的对称性，这个子博弈对于<strong>新的领导者（P2）</strong>来说，价值也应该是 <strong>V</strong>。</p>
</li>
<li><p>既然对于新的领导者（P2）价值是 <code>V</code>，那么对于<strong>新的挑战者（也就是原来的P1）</strong>来说，价值就是 <strong>-V</strong>（因为是零和博弈）。</p>
</li>
<li><p>因此，原领导者（P1）的支付矩阵可以写成一个包含其自身价值 V 的形式：</p>
<p>G=(031−V)</p>
</li>
</ul>
<h4 id="2-求解博弈价值-V"><a href="#2-求解博弈价值-V" class="headerlink" title="2. 求解博弈价值 V"></a>2. 求解博弈价值 V</h4><p>现在，问题转化为了：求解这个特殊矩阵的价值 V，并且这个价值 V 必须等于它自身。</p>
<script type="math/tex; mode=display">V = \text{value} \begin{pmatrix} 0 & 1 \ 3 & -V \end{pmatrix}$$我们可以使用之前推导出的 2x2 零和博弈的通用价值公式：$$V = \frac{ac - bd}{(a+c) - (b+d)}$$其中，a=0, b=1, d=3, c=-V。代入公式：$$V = \frac{(0)(-V) - (1)(3)}{(0)+(-V) - (1)-(3)} = \frac{-3}{-V-4}$$现在我们得到了一个关于 V 的方程，求解它：

$$V(-V - 4) = -3$$$$-V^2 - 4V = -3$$$$V^2 + 4V - 3 = 0</script><p>这是一个一元二次方程。使用求根公式 </p>
<p>我们得到了两个可能的解：V1=7−2≈0.646 和 V2=−7−2≈−4.646。</p>
<h4 id="3-选择正确的解"><a href="#3-选择正确的解" class="headerlink" title="3. 选择正确的解"></a>3. 选择正确的解</h4><p>哪个才是这个博弈真正的价值呢？</p>
<p>我们看领导者的支付矩阵，他有一个“放弃”的选项。如果他选择“放弃”，最坏的结果是对手选择“挑战”，此时他的收益是1。这意味着，领导者至少可以为自己保证一个非负的收益。因此，一个负数（比如-4.646）不可能是这个博弈的理性价值。</p>
<p>所以，这个博弈对于领导者的价值是：根号7减去2</p>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250630224225904.png" alt="image-20250630224225904"></p>
<p>求解博弈G1和G2的价值：</p>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250630224137356.png" alt="image-20250630224137356"></p>
<h2 id="7-2-值迭代与策略迭代"><a href="#7-2-值迭代与策略迭代" class="headerlink" title="7.2 值迭代与策略迭代"></a>7.2 值迭代与策略迭代</h2><p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250701210516613.png" alt="image-20250701210516613"></p>
<p>1、学习随机博弈中状态值函数贝尔曼方程推导</p>
<p>2、与单智能体MDP的关键区别</p>
<p>这个公式虽然形式上与单智能体MDP的贝尔曼方程很像，但幻灯片的最后一点指出了两个根本性的区别，这也是多智能体问题复杂性的根源：</p>
<ol>
<li><strong>价值是个人化的 (for each agent)</strong>：在MDP中，只有一个价值函数 <code>V(s)</code>。但在随机博弈中，<strong>每个参与者 <code>i</code> 都有自己的一套价值函数 Vi(s)</strong>。同一个状态 <code>s</code>，对我来说可能是天堂（Vi很高），对你来说可能却是地狱（Vj很低）。这体现了参与者之间合作与冲突并存的关系。</li>
<li><strong>价值依赖于联合策略 (on the joint policy)</strong>：这是最致命的区别。在MDP中，价值函数 Vπ(s) 只取决于我自己的策略 <code>π</code>。但在这里，Viπ(s) 的值不仅取决于我的策略 πi，还取决于<strong>其他所有人的策略</strong> π−i，因为是<strong>联合行动</strong>决定了回报和状态转移。<ul>
<li>这就导致了我们之前讨论的<strong>“非平稳性”</strong>或<strong>“移动靶心”</strong>问题。如果我的对手改变了他的策略，那么即使我的策略和当前状态都没变，我整个的价值函数 Vi(s) 也会跟着改变。我原以为很有价值的状态，可能因为对手策略的改变而突然变得一文不值。</li>
</ul>
</li>
</ol>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250701211212340.png" alt="image-20250701211212340"></p>
<p>这张幻灯片在重申了状态价值的定义之后，提出了一个关于它的非常重要的数学性质：<strong>有界性 (Boundedness)</strong>。这个性质是随机博弈能够被分析和求解的理论基础之一。</p>
<h4 id="1-核心思想：无限过程，有限价值"><a href="#1-核心思想：无限过程，有限价值" class="headerlink" title="1. 核心思想：无限过程，有限价值"></a>1. 核心思想：无限过程，有限价值</h4><p>这条结论的核心思想是：尽管一个随机博弈的过程可能永远持续下去，但从任何一个状态开始，任何一个参与者能够获得的<strong>总的“折扣”价值都不是无限的，而是一个有限的、有上限的数值</strong>。</p>
<p>幻灯片给出了这个上限的计算公式：1−γM。</p>
<ul>
<li><strong>M</strong>: 代表在整个游戏所有可能的情况下，任何参与者在<strong>单一一轮</strong>中所能获得的<strong>最大绝对收益</strong>。可以理解为这个游戏里“单次操作的最大奖励或最大惩罚（的绝对值）”。</li>
<li><strong>γ (gamma)</strong>: 是我们熟悉的折扣因子（0 &lt; γ &lt; 1），代表了我们对未来收益的耐心程度。</li>
</ul>
<h4 id="2-这个上限公式是怎么来的？（几何级数）"><a href="#2-这个上限公式是怎么来的？（几何级数）" class="headerlink" title="2. 这个上限公式是怎么来的？（几何级数）"></a>2. 这个上限公式是怎么来的？（几何级数）</h4><p>这个公式的推导非常直观，它基于我们熟知的等比数列（几何级数）求和。</p>
<ol>
<li><p>根据定义，状态价值是所有未来折扣回报的总和：</p>
<p>Viπ(s)=Eπ[r0+γr1+γ2r2+γ3r3+…]</p>
</li>
<li><p>在任何一步 <code>k</code>，我们能获得的即时回报 rk 的绝对值，都不可能超过定义好的最大单轮回报 <code>M</code>。即 ∣rk∣≤M。</p>
</li>
<li><p>因此，总价值 Viπ(s) 必然小于或等于一种最极端、最理想的情况：假设我们在未来的每一步，都能幸运地获得最大的正回报 M。</p>
<p>Viπ(s)≤M+γM+γ2M+γ3M+…</p>
</li>
<li><p>将 M 提取出来：</p>
<p>Viπ(s)≤M(1+γ+γ2+γ3+…)</p>
</li>
<li><p>括号里的部分是一个公比为 <code>γ</code> 的无穷等比数列。因为 <code>γ &lt; 1</code>，这个级数是收敛的，其和为 1−γ1。</p>
</li>
<li><p>因此，我们得到了最终的边界：</p>
<p>Viπ(s)≤1−γM</p>
</li>
</ol>
<h4 id="3-这个性质为什么重要？"><a href="#3-这个性质为什么重要？" class="headerlink" title="3. 这个性质为什么重要？"></a>3. 这个性质为什么重要？</h4><ol>
<li><strong>保证问题“有解”</strong>：这个有界性保证了我们要求解的状态价值函数是一个“行为良好”的函数，它的值不会发散到无穷大。这是所有后续分析和算法能够成立的数学前提。如果没有这个保证，我们可能都无法定义“最优策略”，因为所有策略的总回报都是无穷大，无法比较。</li>
<li><strong>为算法提供基础</strong>：在很多求解随机博弈的算法（例如价值迭代）中，这个边界可以用于初始化价值函数，或者作为算法收敛性的一个判断依据。它确保了算法的计算过程会在一个有限的数值空间内进行，最终能够稳定下来。</li>
</ol>
<p><strong>总结</strong>：这张幻灯片的核心是告诉我们，尽管随机博弈的博弈过程可能是无限的，但其价值是有限的。这个有界性不仅为问题的“可解性”提供了理论保障，也为实际的计算算法奠定了基础。</p>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250701215722881.png" alt="image-20250701215722881"></p>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250701215827727.png" alt="image-20250701215827727"></p>
<h4 id="结论与洞察"><a href="#结论与洞察" class="headerlink" title="结论与洞察"></a>结论与洞察</h4><p>这个例子深刻地揭示了<strong>短期利益与长期战略</strong>之间的权衡。</p>
<ul>
<li>从短期看，第1列对列玩家更有利（即时损失更小）。</li>
<li>但从长期看，第1列有更高的概率让游戏继续下去，这意味着他未来要持续地向行玩家支付价值为<code>v</code>的收益。而第2列能更快地结束游戏，从而“止损”。</li>
<li>最终的均衡策略显示，<strong>长期战略（避免未来损失）的重要性压倒了短期利益</strong>。因此，列玩家的最优策略是<strong>更频繁地选择那个能更快结束游戏的第2列</strong>，尽管它眼前的损失看起来更大。</li>
</ul>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250702003904470.png" alt="image-20250702003904470"></p>
<p><strong>标题：价值迭代 (Value Iteration)</strong></p>
<ul>
<li>夏普利证明了 vn(s) 会收敛到从s开始的随机博弈的<strong>真实价值 v(s)</strong>。<ul>
<li>— 首先，收敛是以<strong>指数速率 (exponential rate)</strong> 进行的：最大误差至少以 γn 的速度下降。</li>
<li>— 其次，在第 n+1 阶段的最大误差，至多是“<strong>从n到n+1阶段的最大变化量</strong>”乘以 γ/(1−γ)。</li>
</ul>
</li>
</ul>
<hr>
<p>这张幻灯片深入探讨了“价值迭代”算法的<strong>性能保证</strong>。它告诉我们，夏普利不仅证明了价值迭代这个方法是<strong>可行</strong>的（即最终能找到正确答案），更证明了它是<strong>高效</strong>和<strong>可靠</strong>的。这使得价值迭代从一个理论上的概念，变成了一个可以在实践中应用的强大工具。</p>
<h4 id="1-指数速率收敛：为什么说它“高效”？"><a href="#1-指数速率收敛：为什么说它“高效”？" class="headerlink" title="1. 指数速率收敛：为什么说它“高效”？"></a>1. 指数速率收敛：为什么说它“高效”？</h4><p>“指数速率收敛”听起来很抽象，但它的意思是，算法的精确度在每一步迭代后都会得到一个“质的飞跃”。</p>
<ul>
<li><strong>一个比喻</strong>：想象你在寻宝，宝藏在1公里外。有一个向导，你每走一步，他都会告诉你：“你现在离宝藏的距离，是你上一步距离的90%”。<ul>
<li>你的误差（与宝藏的距离）在每一步都会乘以一个固定的因子（0.9）。</li>
<li>第一次迭代后，误差是 1×0.9。</li>
<li>第二次迭代后，误差是 1×0.92。</li>
<li>第n次迭代后，误差是 1×0.9n。</li>
<li>误差以 0.9n 的速度急剧缩小，这就是指数级的衰减。</li>
</ul>
</li>
<li><strong>在价值迭代中</strong>：折扣因子 <code>γ</code> (一个小于1的数) 就扮演了这个“0.9”的角色。每迭代一次，我们估算的价值函数 vn(s) 与真实价值 v(s) 之间的最大误差，都会大致缩小一个 <code>γ</code> 倍。因为 <code>γ</code> 小于1，所以经过多次迭代后，误差会变得非常小，算法能很快地逼近真实解。这背后的数学原理是，夏普利证明了价值迭代的更新算子是一个<strong>压缩映射 (Contraction Mapping)</strong>。</li>
</ul>
<h4 id="2-误差边界：为什么说它“可靠”？"><a href="#2-误差边界：为什么说它“可靠”？" class="headerlink" title="2. 误差边界：为什么说它“可靠”？"></a>2. 误差边界：为什么说它“可靠”？</h4><p>第二点结论解决了一个非常实际的问题：“我怎么知道什么时候可以停止算法，并且保证我的答案足够精确了？”</p>
<ul>
<li><p><strong>面临的困境</strong>：我们希望我们的误差，即 <code>|我们的估算值 vₙ - 真实值 v*|</code>，小于一个我们能接受的阈值（比如0.001）。但问题是，我们并不知道那个神秘的“真实值 <code>v*</code>”到底是多少，所以无法直接计算这个误差。</p>
</li>
<li><p>夏普利提供的解决方案：他给出了一个可计算的误差上限。公式告诉我们：</p>
<p>真实的未知误差≤可计算的最大单步变化量×1−γγ</p>
<p>这里的“可计算的最大单-步变化量”指的是 ∣vn+1(s)−vn(s)∣max，也就是在你最近一次迭代中，所有状态的价值估算值变化最大的那一个。这个值我们在计算过程中是完全知道的。</p>
</li>
<li><p><strong>实际应用（停止条件）</strong>：</p>
<ol>
<li>我们设定一个目标精度 <code>ε</code>，比如我希望我的最终答案与真实值的误差不超过0.001。</li>
<li>根据公式，只要我们能让 <code>(可计算的最大单步变化量) * γ/(1-γ)</code> 这个上限小于 <code>ε</code>，那么真实的误差就一定小于 <code>ε</code>。</li>
<li>因此，我们的算法停止条件就变成了：<strong>持续迭代，直到我们观察到的最大单步变化量小于 ϵ⋅γ1−γ</strong>。</li>
<li>一旦满足这个条件，我们就可以放心地停止迭代，并宣布当前的估算值 vn+1 就是一个足够精确的解。</li>
</ol>
</li>
</ul>
<p><strong>总结</strong>：这张幻灯片从理论上为价值迭代算法的有效性提供了强有力的背书。<strong>指数速率收敛</strong>保证了它的计算速度，而<strong>实用的误差边界</strong>则为它在现实中的应用提供了可靠的停止准则，使其成为求解两人零和随机博弈的核心算法之一。</p>
<p><img src="C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250702005255997.png" alt="image-20250702005255997"></p>
<p><strong>为什么最小停止概率是0.5？</strong></p>
<h3 id="1-“最小停止概率是0-5”的来源分析"><a href="#1-“最小停止概率是0-5”的来源分析" class="headerlink" title="1. “最小停止概率是0.5”的来源分析"></a>1. “最小停止概率是0.5”的来源分析</h3><p>这个结论来自于对 G₁ 和 G₂ 两个矩阵中所有单元格的<strong>“持续概率”</strong>的分析。</p>
<p>在一个随机博弈的支付单元格中，形如 <code>“即时回报 + 概率 × 未来价值”</code> 的结构，那个<strong>概率</strong>就代表了游戏<strong>继续下去的可能性</strong>。而<strong>“停止概率”</strong>则等于 <strong>1 - 继续概率</strong>。</p>
<p>让我们来逐一检查两个矩阵中所有结果的“停止概率”：</p>
<p>对于博弈 G₁:</p>
<p>G(1)=(4+0.3G(1)1+0.4G(2)0+0.4G(2)3+0.5G(1))</p>
<ul>
<li>(行1, 列1): 继续概率是 0.3。 <strong>停止概率 = 1 - 0.3 = 0.7</strong></li>
<li>(行1, 列2): 继续概率是 0.4。 <strong>停止概率 = 1 - 0.4 = 0.6</strong></li>
<li>(行2, 列1): 继续概率是 0.4。 <strong>停止概率 = 1 - 0.4 = 0.6</strong></li>
<li>(行2, 列2): 继续概率是 0.5。 <strong>停止概率 = 1 - 0.5 = 0.5</strong></li>
</ul>
<p>对于博弈 G₂:</p>
<p>G(2)=(0+0.5G(1)−4−51+0.5G(2))</p>
<ul>
<li>(行1, 列1): 继续概率是 0.5。 <strong>停止概率 = 1 - 0.5 = 0.5</strong></li>
<li>(行1, 列2): 支付是-5（没有未来价值项）。继续概率是 0。<strong>停止概率 = 1 - 0 = 1</strong></li>
<li>(行2, 列1): 支付是-4。继续概率是 0。<strong>停止概率 = 1 - 0 = 1</strong></li>
<li>(行2, 列2): 继续概率是 0.5。<strong>停止概率 = 1 - 0.5 = 0.5</strong></li>
</ul>
<p>现在，我们把所有计算出的停止概率放在一起：{ 0.7, 0.6, 0.5, 1 }。</p>
<p>在所有这些可能性中，最小的那个值，就是 0.5。</p>
<p>这就是“最小停止概率是0.5”这句话的直接来源。</p>
<h3 id="2-这个数字为什么如此重要？"><a href="#2-这个数字为什么如此重要？" class="headerlink" title="2. 这个数字为什么如此重要？"></a>2. 这个数字为什么如此重要？</h3><p>这个“最小停止概率”反过来告诉了我们整个系统的<strong>“最大继续概率”</strong>。</p>
<ul>
<li>最小停止概率 = 0.5</li>
<li>最大继续概率 = 1 - 最小停止概率 = 1 - 0.5 = 0.5</li>
</ul>
<p>这个<strong>“最大继续概率”</strong>，可以被看作是整个随机博弈系统的<strong>有效折扣因子 γ (effective discount factor)</strong>。</p>
<p>为什么呢？</p>
<p>价值迭代算法的收敛速度，取决于其更新算子是不是一个“压缩映射”，而其“压缩程度”就由折扣因子γ决定。为了保证整个系统一定收敛，我们必须考虑最坏的情况。</p>
<ul>
<li><strong>收敛的最坏情况是什么？</strong> 就是收敛得最慢的情况。</li>
<li><strong>什么时候收敛得最慢？</strong> 就是“折扣”打得最少的时候，也就是游戏最不容易结束、<strong>继续下去的概率最大</strong>的时候。</li>
</ul>
<p>在这个博弈中，游戏继续下去的最大概率是0.5。因此，整个价值迭代算法的收敛速度就由这个0.5来决定。</p>
<ul>
<li><strong>收敛速率</strong>：因为有效折扣因子<code>γ</code>是0.5，所以算法的误差是以 (0.5)n 的指数速率下降的。</li>
<li><strong>误差边界</strong>：根据我们之前学过的误差边界公式 <code>真实误差 ≤ 最大单步变化量 × γ/(1-γ)</code>，代入<code>γ=0.5</code>，我们得到 <code>γ/(1-γ) = 0.5/0.5 = 1</code>。这意味着，真实的未知误差，不会超过我们在上一步迭代中能观测到的最大变化量。幻灯片中说v₆的误差至多是0.0002，就是基于这个原理计算出来的（从v₅到v₆的最大变化量约为0.0001，其上限0.0002是完全正确的）。</li>
</ul>
<p><strong>总结</strong>： “最小停止概率是0.5”这个结论，是通过分析所有可能结果得出的。它的真正意义在于，它为我们确定了整个动态系统的有效折扣因子 <code>γ=0.5</code>，从而为算法的<strong>收敛速度</strong>和<strong>误差分析</strong>提供了坚实的理论依据。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://zcl0219.github.io/2025/05/15/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRESENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="GGBond">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="Doing the tough things sets winners apart from losers">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/05/15/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRESENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/" class="post-title-link" itemprop="url">ATTENTION-GUIDED CONTRASTIVE ROLE REPRESENTATIONS FOR MULTI-AGENT REINFORCEMENT LEARNING</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-05-15 20:22:33" itemprop="dateCreated datePublished" datetime="2025-05-15T20:22:33+08:00">2025-05-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-05-16 00:17:28" itemprop="dateModified" datetime="2025-05-16T00:17:28+08:00">2025-05-16</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>摘要：<br>1、受角色与智能体行为模式之间相关性的启发，我们提出了一种名为 Attention-guided COntrastive Role representation learning for MARL（ACORM）的新型框架</p>
<blockquote id="fn_注释">
<sup>注释</sup>. 不同类型的角色具有不同类型的行为模式，比如有的偏向于拆塔，有的偏向于生产资源等等。作者应该是考虑利用不同角色与之对应的行为模式建立关系<a href="#reffn_注释" title="Jump back to footnote [注释] in the text."> &#8617;</a>
</blockquote>
<hr>
<p>2、引入互信息最大化来形式化角色表示学习，推导出一个对比学习目标，并简洁地近似负样本的分布</p>
<blockquote id="fn_注释">
<sup>注释</sup>. 1. <strong>Mutual Information Maximization（互信息最大化）</strong>：互信息是信息论中的一个概念，用于衡量两个随机变量之间的相关性。最大化互信息意味着让模型学习到的角色表示（role representations）与智能体的行为模式尽可能相关。这可能意味着通过优化互信息，使角色的表示能够更好地捕捉到不同角色之间的行为差异。2. <strong>Formalize Role Representation Learning（形式化角色表示学习）</strong>：这里“形式化”指的是将角色表示的学习过程转化为数学框架或优化问题。通过互信息最大化，将角色表示的学习转化为一个明确的优化目标，使得角色表示能够系统地学习到有用的特征。3. <strong>Derive a Contrastive Learning Objective（推导对比学习目标）</strong>：对比学习是一种自监督学习方法，通过比较正样本对和负样本对来学习表示。在这里，作者可能将互信息最大化转化为对比损失函数，通过对比不同角色或行为模式，使得相似的行为在表示空间中更接近，不同的更远。4. <strong>Concisely Approximate the Distribution of Negative Pairs（简洁地近似负样本对的分布）</strong>：对比学习中需要生成负样本对，但直接采样所有可能的负样本可能计算量大。这里“简洁地近似”可能指通过某种方法有效生成或选择负样本，减少计算复杂度，同时保持对比学习的有效性。<a href="#reffn_注释" title="Jump back to footnote [注释] in the text."> &#8617;</a>
</blockquote>
<hr>
<p>3、利用注意力机制提示全局状态关注价值分解中学习到的角色表示，隐式地引导智能体在技能型角色空间中进行协调，以产生更具表现力的信用分配。</p>
<blockquote id="fn_注释">
<sup>注释</sup>. 总的来说，还是解决信用分配问题？<a href="#reffn_注释" title="Jump back to footnote [注释] in the text."> &#8617;</a>
</blockquote>
<hr>
<p>引言</p>
<p>1、主要思想是学习一个紧凑的角色表征，以捕捉智能体的复杂行为模式，并使用该角色表征来促进智能体之间的行为异质性、知识迁移和熟练协调。首先，我们将学习目标形式化为角色与其表征之间的互信息最大化，以在给定智能体行为的情况下最大限度地减少角色不确定性，同时最大限度地保留与角色无关的信息。我们引入了一种对比学习方法来优化 infoNCE 损失，即互信息的下界。为了简洁地近似负样本的分布，我们通过将智能体的轨迹编码到潜在空间中来提取智能体行为，并定期根据其潜在嵌入将所有智能体划分为几个集群，来自不同集群的点被配对为负样本。其次，在集中式训练期间，我们采用注意力机制来提示全局状态关注价值分解中学习到的角色表征。 注意力机制在技能角色空间中隐式地指导智能体协调，从而在角色出现时产生更丰富的信用分配。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://zcl0219.github.io/2025/05/15/BYZANTINE-ROBUST-COOPERATIVE-MULTI-AGENT-REINFORCEMENT-LEARNING-AS-A-BAYESIAN-GAME/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="GGBond">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="Doing the tough things sets winners apart from losers">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/05/15/BYZANTINE-ROBUST-COOPERATIVE-MULTI-AGENT-REINFORCEMENT-LEARNING-AS-A-BAYESIAN-GAME/" class="post-title-link" itemprop="url">BYZANTINE ROBUST COOPERATIVE MULTI-AGENT REINFORCEMENT LEARNING AS A BAYESIAN GAME</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2025-05-15 20:12:57 / Modified: 20:22:45" itemprop="dateCreated datePublished" datetime="2025-05-15T20:12:57+08:00">2025-05-15</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>本片文章主要是依据历史经验来判断出干扰者，目标是合作方在最坏对抗干扰下，寻求最优策略<br>\begin{equation} \label{eq:ex_ante_equilibrium}<br>(\pi_{<em>}^{EA}(\cdot|H), \hat{\pi}_{</em>}^{EA}(\cdot|H, \theta)) \in \arg \max_{\pi(\cdot|H)} \mathbb{E}_{p(\theta)} \left[ \min_{\hat{\pi}(\cdot|H, \theta)} V_{\theta}(s) \right]<br>\end{equation}</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://zcl0219.github.io/2025/04/17/VDN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="GGBond">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="Doing the tough things sets winners apart from losers">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/04/17/VDN/" class="post-title-link" itemprop="url">VDN</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-04-17 01:21:22" itemprop="dateCreated datePublished" datetime="2025-04-17T01:21:22+08:00">2025-04-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-05-08 00:23:00" itemprop="dateModified" datetime="2025-05-08T00:23:00+08:00">2025-05-08</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="用于协作式多智能体学习的价值分解网络"><a href="#用于协作式多智能体学习的价值分解网络" class="headerlink" title="用于协作式多智能体学习的价值分解网络"></a>用于协作式多智能体学习的价值分解网络</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>我们研究了具有单个联合奖励信号的协作式多智能体强化学习问题。这类学习问题很困难，因为通常有很大的动作和观察组合空间。在完全集中式和去中心化的方法中，我们发现了虚假奖励的问题和一种我们称之为 “惰性代理” 问题的现象，这是由于部分可观察性而出现的。我们通过使用一种新的价值分解网络架构来训练单个代理来解决这些问题，该架构学习将团队价值函数分解为代理价值函数。我们在一系列部分可观察的多智能体领域进行了实验评估，并表明学习这种价值分解会带来卓越的结果，特别是当与权重共享、角色信息和信息通道相结合时。</p>
<hr>
<p><sup><a href="#fn_什么是单个联合奖励信号的协作式多智能体强化学习问题" id="reffn_什么是单个联合奖励信号的协作式多智能体强化学习问题">什么是单个联合奖励信号的协作式多智能体强化学习问题</a></sup>: </p>
<p>首先，协作式多智能体强化学习（Cooperative MARL）是指多个智能体共同合作完成一个任务，他们的目标一致，共享奖励或各自有奖励，但这里提到的是单个联合奖励信号，所以应该是所有智能体共享同一个奖励。那单个联合奖励信号是什么意思呢？可能是指所有智能体的行为共同影响一个全局奖励，比如团队胜利或任务完成。比如足球游戏里的多个球员，进球了大家都有同样的奖励，被进球了都扣分。这种情况下，每个智能体的行为都会影响整个团队的奖励，但每个智能体可能需要自己学习策略，或者有集中式的训练。接下来需要解释协作式MARL的基本概念，然后结合单个联合奖励来展开。可能需要比较一下其他情况，比如每个智能体有独立奖励的情况，或者竞争式MARL。然后说明这种单个联合奖励带来的挑战和常见解决方法。挑战可能包括信用分配问题，因为所有智能体共享同一个奖励，难以确定每个个体的贡献。比如在团队游戏中，某个关键动作可能由某个智能体做出，但奖励是大家共享的，其他智能体如何知道该动作的重要性？这时候可能需要某种方法来分配信用，比如COMA算法中的反事实基线，或者VDN、QMIX这类值分解的方法。另外，还需要提到环境的部分可观测性，每个智能体可能只能看到局部信息，这会影响决策。还有策略的协调问题，如何让多个智能体的策略在分散执行时保持一致，可能需要集中式训练、分散式执行的框架。常见的方法可能包括值分解网络（VDN, QMIX），这些方法通过分解联合Q值到个体Q值，从而在训练时利用全局信息，执行时每个智能体根据自己的Q值行动。还有Actor-Critic方法，如MADDPG，虽然每个智能体有自己的策略，但Critic可能会用到全局信息进行训练。另外，可能需要提到信用分配的具体技术，比如差异奖励（Difference Rewards），即每个智能体的奖励基于其贡献与全局奖励的关系，或者利用反事实基线来评估某个智能体是否存在时的奖励差异。总结的话，单个联合奖励的协作式MARL问题需要解决信用分配、环境复杂性、策略协调等问题，常用方法有值分解、集中训练分散执行、信用分配技术等。这样用户应该能理解这个问题的定义、挑战和解决思路了。</p>
<hr>
<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>我们考虑合作式多智能体强化学习（MARL）问题[，, , ]，其中多个学习智能体需要共同优化随时间累积的单一奖励信号——即<strong>团队奖励</strong>。每个智能体只能访问自身的“局部”观测信息，并负责从其自身的动作集中选择动作。协作式MARL问题出现在诸如协调自动驾驶车辆和/或交通信号系统、优化由多个交互组件组成的工厂生产效率等应用中。更广泛地说，随着AI智能体的普及，它们必须学会协作以实现共同目标。</p>
<p>尽管某些实际应用可能需要局部自治，但原则上合作式MARL问题可以通过<strong>集中式</strong>方法处理，即将问题简化为单智能体强化学习（RL），通过拼接观测空间和组合动作空间进行学习。然而，我们证明集中式方法在相对简单的协作式MARL问题上始终失败。我们设计了一个简单实验：集中式方法会学习到低效的策略，仅有一个智能体活跃，而另一个变得“懒惰”。这是因为当一个智能体学习到有用策略时，第二个智能体因探索行为可能妨碍前者并导致团队奖励下降，从而被抑制学习。</p>
<p><sup><a href="#fn_" id="reffn_"></a></sup>: </p>
<p>另一种方法是训练<strong>独立学习者</strong>以优化团队奖励。然而，由于每个智能体的环境动态会随着其他智能体行为的变化而改变，每个智能体实际上面临非稳态学习问题[]。此外，由于单个智能体的视角是部分可观测的，它们可能接收到来自队友（未被观测的）行为的虚假奖励信号。这种无法解释自身观测奖励的缺陷使得朴素独立RL通常无法成功：例如[]表明，独立Q学习器无法区分队友的探索与环境本身的随机性，甚至无法解决一个看似简单的2智能体、无状态、3×3动作的问题。而一般的Dec-POMDP问题已被证明是难解的[, ]。尽管本文聚焦于双智能体协作，但需注意独立学习者和集中式方法的问题会随智能体数量增加而恶化，因为此时大多数奖励与个体无关，而集中式方法的动作空间会指数级膨胀。</p>
<p>改进独立学习者的方法之一是设计更直接关联个体观测的局部奖励函数。然而，即使在单智能体场景中，奖励塑形也极为困难，仅有一小类塑形奖励函数能保证与真实目标的最优性一致[, ]。本文旨在探索更通用的自主解决方案，即通过学习分解团队价值函数。</p>
<p>我们提出一种新型的<strong>可学习加性价值分解</strong>方法，通过对个体智能体的价值函数进行线性组合。具体而言，价值分解网络（VDN）旨在通过反向传播团队Q值的梯度到表示个体价值函数的深度神经网络中，从而从团队奖励信号中学习最优的线性价值分解。这种加性分解的动机是避免独立学习者中出现的虚假奖励信号。每个智能体隐式学习到的价值函数仅依赖于局部观测，因此更易训练。该方法还缓解了[]中强调的独立学习的协调问题，因为它在训练时以集中方式学习，而部署时智能体可独立运行。</p>
<p>此外，我们结合权重共享、角色信息和信息通道等近期被证明可提升样本效率与降低内存需求的技术[, , ]，对所提出的智能体进行增强。但本文的核心比较集中在三种架构：基于个体的价值分解、独立学习者和集中式方法。我们在一系列新颖的双智能体协作场景中对这些技术的组合进行了实验与基准测试。结果表明，价值分解方法在性能上显著优于集中式或完全独立的学习者，且结合附加技术后，其表现更是大幅领先。</p>
<hr>
<p><sup><a href="#fn_备注" id="reffn_备注">备注</a></sup>: </p>
<p>集中式学习的缺点是随着智能体数量的增加，动作空间指数级增长；<br>独立式学习的缺点是每个智能体面临的是非稳态环境，学习不稳定还会影响其他智能体的学习；<br>因此，本文推出了一种可学习加性价值分解方法。换句话说就是如何将团队奖励（团队Q值）进行合理分解</p>
<hr>
<h4 id="2-3-多智能体强化学习"><a href="#2-3-多智能体强化学习" class="headerlink" title="2.3 多智能体强化学习"></a>2.3 多智能体强化学习</h4><p>如果我们将(\bar{h} := (h^1, h^2, …, h^d))定义为智能体历史记录的元组（其中每个(h^i)表示第(i)个智能体的历史），则联合策略通常是一个映射(\pi : \mathcal{H}^d \to \mathcal{P}(\mathcal{A}^d))。此映射基于所有智能体的联合历史(\bar{h})，输出联合动作空间(\mathcal{A}^d)上的概率分布。  </p>
<p>不过，我们特别关注一类特殊的策略：对于任意历史(\bar{h})，其输出的分布(\pi(\bar{h}))在(\mathcal{P}(\mathcal{A})^d)中具有<strong>独立分量</strong>。即，每个智能体的动作分布仅依赖于联合历史(\bar{h})，但各智能体的动作选择在概率上是相互独立的。因此，这类策略可表示为(\pi : \mathcal{H}^d \to \mathcal{P}(\mathcal{A})^d)。  </p>
<p>唯一的例外是使用最朴素的集中式智能体（即<strong>联合动作学习者</strong>）时，此时策略直接操作组合动作空间（(\mathcal{A}^d)），输出的分布属于(\mathcal{P}(\mathcal{A}^d))。  </p>
<hr>
<p><sup><a href="#fn_备注" id="reffn_备注">备注</a></sup>: </p>
<p>这段文字定义了多智能体强化学习（MARL）中的联合策略，并比较了两种不同的策略形式：</p>
<h3 id="核心概念解释"><a href="#核心概念解释" class="headerlink" title="核心概念解释"></a>核心概念解释</h3><ol>
<li><p><strong>智能体历史元组</strong>：<br>用(\bar{h} := (h^1, h^2, …, h^d))表示所有智能体的历史记录集合，其中(h^i)是第(i)个智能体的历史（包含其观测和动作序列）。</p>
</li>
<li><p><strong>联合策略的一般形式</strong>：<br>联合策略通常定义为映射(\pi : \mathcal{H}^d \to \mathcal{P}(\mathcal{A}^d))，即基于所有智能体的联合历史(\bar{h})，输出一个联合动作空间(\mathcal{A}^d)上的概率分布。  </p>
<ul>
<li>(\mathcal{H}^d)：所有智能体历史记录的联合空间。  </li>
<li>(\mathcal{P}(\mathcal{A}^d))：所有可能的联合动作的概率分布集合。  </li>
</ul>
</li>
<li><p><strong>独立分量的联合策略</strong>：<br>作者特别考虑一种特殊形式：对于任何联合历史(\bar{h})，策略输出的分布(\pi(\bar{h}))在(\mathcal{P}(\mathcal{A})^d)中具有<strong>独立分量</strong>。这意味着：  </p>
<ul>
<li>每个智能体的动作选择仅依赖于联合历史(\bar{h})，但动作之间的概率分布是独立的。  </li>
<li>数学上可分解为(\pi(\bar{h}) = (\pi^1(\bar{h}), \pi^2(\bar{h}), …, \pi^d(\bar{h})))，其中每个(\pi^i(\bar{h}) \in \mathcal{P}(\mathcal{A}))表示第(i)个智能体的局部策略。  </li>
<li><strong>优势</strong>：避免了组合动作空间的复杂性，同时仍能利用全局历史信息进行决策。</li>
</ul>
</li>
<li><p><strong>例外：集中式联合动作学习者</strong>：<br>当使用“最朴素的集中式智能体”时，策略直接操作组合动作空间（combinatorial action space），即联合动作学习者（joint action learners）。  </p>
<ul>
<li>此时，策略输出的分布(\pi(\bar{h}))属于(\mathcal{P}(\mathcal{A}^d))，需考虑所有智能体动作的联合分布，而非独立分量。  </li>
<li><strong>问题</strong>：动作空间维度随智能体数量(d)指数增长（(\mathcal{A}^d)），导致计算和训练复杂度极高。</li>
</ul>
</li>
</ol>
<h3 id="关键区别"><a href="#关键区别" class="headerlink" title="关键区别"></a>关键区别</h3><div class="table-container">
<table>
<thead>
<tr>
<th><strong>策略类型</strong></th>
<th><strong>输入</strong></th>
<th><strong>输出分布</strong></th>
<th><strong>特点</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>独立分量联合策略</td>
<td>联合历史(\bar{h})</td>
<td>(\mathcal{P}(\mathcal{A})^d)</td>
<td>动作分布独立，利用全局信息但保持动作解耦，复杂度线性增长（(d \times \</td>
<td>\mathcal{A}\</td>
<td>)）。</td>
</tr>
<tr>
<td>集中式联合动作学习者</td>
<td>联合历史(\bar{h})</td>
<td>(\mathcal{P}(\mathcal{A}^d))</td>
<td>动作分布耦合，需建模联合动作，复杂度指数增长（(\</td>
<td>\mathcal{A}\</td>
<td>^d)），难以扩展。</td>
</tr>
</tbody>
</table>
</div>
<h3 id="实际意义"><a href="#实际意义" class="headerlink" title="实际意义"></a>实际意义</h3><ul>
<li><strong>独立分量策略</strong>：适用于需要平衡全局协调与计算效率的场景，例如价值分解网络（VDN）等算法，通过共享观测历史但独立选择动作，缓解组合爆炸问题。  </li>
<li><strong>集中式联合动作学习者</strong>：尽管理论上能直接优化全局最优策略，但因动作空间过大，仅适用于智能体数量极少或动作空间极小的任务，实际应用受限。</li>
</ul>
<p>这一区分为后续讨论集中式方法在复杂任务中的失败（如动作空间爆炸、探索效率低）以及独立学习者或价值分解方法的优势奠定了基础。</p>
<hr>
<h3 id="三、深度强化学习（Deep-RL）架构用于合作型多智能体强化学习（Coop-MARL）"><a href="#三、深度强化学习（Deep-RL）架构用于合作型多智能体强化学习（Coop-MARL）" class="headerlink" title="三、深度强化学习（Deep-RL）架构用于合作型多智能体强化学习（Coop-MARL）"></a>三、深度强化学习（Deep-RL）架构用于合作型多智能体强化学习（Coop-MARL）</h3><p>减少可学习参数数量的一个方法是在代理之间共享某些网络权重。权重共享还引发了代理不变性的概念，这对于避免懒惰代理问题是有用的。</p>
<hr>
<p><sup><a href="#fn_备注" id="reffn_备注">备注</a></sup>: </p>
<p>首先，提到“减少可学习参数数量的方法之一是共享智能体之间的某些网络权重”。这里的“可学习参数”指的是神经网络中的权重，共享权重意味着不同的智能体使用相同的权重参数，而不是每个智能体都有自己独立的参数。这样可以减少整体的参数数量，从而降低模型的复杂度和训练所需的计算资源。</p>
<p>接下来，提到“权重共享还产生了智能体不变性的概念，这对避免懒惰智能体问题很有用”。“智能体不变性”应该是指通过共享权重，不同智能体在功能上变得相似或一致，从而避免某些智能体变得不活跃（即“懒惰智能体问题”）。</p>
<hr>
<h3 id="VDN代码实现"><a href="#VDN代码实现" class="headerlink" title="VDN代码实现"></a>VDN代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">VDN类由五个函数组成：init, learn, get_inputs, get_q_values, init_hidden, save_model;</span><br><span class="line">其中，初始化init包括：智能体数量，状态空间，观察空间，输入维度，动作空间，四种神经网络，是否使用GPU，是否加载已存在模型，初始化相同网络参数，初始化需要优化网络参数个数，初始化优化器，初始化隐藏元。</span><br><span class="line">输入维度不等于观察空间原因：当args.last_action为<span class="literal">True</span>时，输入中会加入上一个时间步的动作信息</span><br><span class="line">四种神经网络分别为：每个agent选择动作的网络，每个agent的目标网络，VDN累加Q值的网络，VDN目标网络</span><br><span class="line">网络参数优化器：注意网络参数优化器是针对哪些网络参数进行反向传递优化，这对于VDN算法的实现很重要</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://zcl0219.github.io/2025/04/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="GGBond">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="Doing the tough things sets winners apart from losers">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/04/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0/" class="post-title-link" itemprop="url">强化学习综述</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-04-16 23:50:40" itemprop="dateCreated datePublished" datetime="2025-04-16T23:50:40+08:00">2025-04-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-04-17 02:10:35" itemprop="dateModified" datetime="2025-04-17T02:10:35+08:00">2025-04-17</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="多智能体强化学习通信方法综述"><a href="#多智能体强化学习通信方法综述" class="headerlink" title="多智能体强化学习通信方法综述"></a>多智能体强化学习通信方法综述</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>通信是协调多个智能体行为、拓宽它们对环境的认识以及支持它们协作的有效机制。在多智能体深度强化学习（MADRL）领域，智能体可以通过通信来提高整体学习性能并实现其目标。智能体可以与所有智能体或特定智能体组通信，或者根据特定约束条件进行通信。随着 MADRL 通信（Comm-MADRL）研究工作的不断增多，目前缺乏一种系统性和结构化的方法来区分和分类现有的 Comm-MADRL 方法。在本文中，我们回顾了 Comm-MADRL 领域的最新研究成果，并考虑了在设计和开发多智能体强化学习系统中可能发挥作用的通信的各个方面。考虑到这些方面，我们提出了 9 个维度，沿着这些维度可以对 Comm-MADRL 方法进行分析、开发和比较。通过将现有工作投射到多维空间中，我们发现了一些有趣的趋势。 我们还通过探索可能性的组合，提出了一些设计未来 Comm-MADRL 系统的创新方向。</p>
<h3 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h3><p>许多现实场景，如自动驾驶<sup>[1]</sup>、传感器网络[2]、机器人[3]和游戏[4, 5]，都可以建模为多智能体系统。这些多智能体系统可以通过多智能体强化学习（MARL）技术进行设计和开发，以学习单个智能体的行为，这些行为可以是合作，竞争，或者它们的混合体。由于智能体通常分布在环境中，它们只能访问局部观察而不是环境的完整状态，因此在多智能体强化学习（MARL）中，部分可观测性成为一个基本假设[6-8]。此外，MARL 还面临着非平稳问题[9]，因为每个智能体都面临着受其他智能体变化和适应策略影响的动态环境。通信被视为解决 MARL 中部分可观测性和非平稳问题的关键手段。智能体可以交流个人信息，例如观察、意图、经验或派生特征，从而对环境有更全面的了解，这反过来又使他们能够做出明智的决策[9, 10]。</p>
<p>由于深度学习[11]及其在强化学习[12]中的应用最近的成功，多智能体深度强化学习（MADRL）近年来取得了巨大成就，其中智能体可以处理高维数据，并在大状态和动作空间中具有泛化能力[7, 8]。我们注意到，大量研究工作集中在具有通信的学习任务上，这些任务旨在通过通信和共享信息来学习解决特定领域的任务，如导航、交通和视频游戏。据我们所知，目前还没有涵盖多智能体深度强化学习（Comm-MADRL）中关于具有通信的学习任务最新工作的综述文献。早期的综述考虑了通信在多智能体强化学习（MARL）中的作用，但将其视为预定义的，而不是学习主题[13-15]。大多数 Comm-MADRL 综述仅涵盖少量研究工作，没有提出一个精细的分类系统来比较和分析它们。在合作场景中，Hernandez-Leal 等人 [16] 使用学习通信来表示学习通信协议的领域，以促进智能体的合作。我们发现的唯一一份将一些早期工作分类到 Comm-MADRL 的综述来自 Gronauer 和 Diepold [17]，它基于区分消息是否被所有智能体、一组智能体或智能体网络接收。然而，Comm-MADRL 的其他方面，如消息类型和训练范式，对于通信至关重要，可以帮助描述现有的通信协议，却被忽略了。因此，关于具有通信的学习任务的近期综述中，所审查的论文相当有限，提出的分类过于狭窄，无法区分 Comm-MADRL 中的现有工作。另一方面，还有一个密切相关的研究领域，即涌现语言/通信，它也通过各种强化学习技术考虑通过学习通信。[18] 与 Comm-MADRL 不同，涌现语言研究的主要目标是学习一种符号语言。然而，涌现语言研究的一部分工作追求一个额外的目标，即利用可学习的符号语言来提高任务级别的性能。值得注意的是，这些研究成果尚未被现有 Comm-MADRL 综述所涵盖，但被纳入我们的调查，称为具有涌现语言的认知任务。总之，我们的调查在范围上与涌现语言的调查重叠（即在具有涌现语言的认知任务中），但我们的调查关注不同的主要目标（即实现特定领域的任务，而不是学习符号语言）。我们进一步在第 2.2 节中阐明具有通信和学习涌现语言的学习任务之间的区别。</p>
<p>在我们的调查论文中，我们通过关注通信如何被利用来提高多智能体深度强化学习技术的性能，回顾了 Comm-MADRL 文献。具体来说，我们专注于可学习的通信协议，这些协议与强调通过深度强化学习技术开发动态和自适应通信的近期工作相一致，包括学习何时、如何以及与谁进行通信。通过对近期 Comm-MADRL 文献的全面回顾，我们提出了一种系统性和结构化的分类方法，旨在区分和分类各种 Comm-MADRL 方法。这种方法还将为新的 Comm-MADRL 系统的设计和进步提供指导。假设我们计划为当前领域任务开发一个 Comm-MADRL 系统。从何时、如何以及与谁进行通信的问题开始，系统可以从多个方面进行描述。智能体需要学习何时进行通信、与谁进行通信、传达什么信息、如何整合接收到的信息，最后，通过通信可以实现哪些学习目标。 我们提出了 9 个维度，对应于 Comm-MADRL 系统的独特方面：控制目标、通信约束、通信者类型、通信策略、传递消息、消息组合、内部整合、学习方法以及训练方案。这些维度构成了 Comm-MADRL 系统的骨架，可用于全面分析和深入了解设计的 Comm-MADRL 方法。通过将最近的 Comm-MADRL 方法映射到这个多维结构中，我们不仅提供了对该领域当前技术水平的洞察，还确定了设计未来 Comm-MADRL 系统的一些重要方向。</p>
<p>本文剩余部分的结构如下。第 2 节讨论了多智能体强化学习的预备知识，以及关于通信的现有扩展和近期调查的详细比较。第 3 节，我们提出了我们的建议维度，解释了如何将近期的工作按每个维度的类别进行分组。第 4 节，我们讨论了文献中发现的趋势，并受建议维度驱动，提出了该研究领域的可能研究方向。第 5 节，我们总结了本文的结论。</p>
<h3 id="2-背景"><a href="#2-背景" class="headerlink" title="2. 背景"></a>2. 背景</h3><p>在本节中，我们首先提供关于多智能体强化学习的必要背景知识。然后，我们展示如何将多智能体强化学习扩展到考虑智能体之间的通信。最后，我们介绍并比较了涉及通信的近期调查，从中我们可以直接看到我们填补现有调查中空白的原因。</p>
<h4 id="2-1-多智能体强化学习"><a href="#2-1-多智能体强化学习" class="headerlink" title="2.1 多智能体强化学习"></a>2.1 多智能体强化学习</h4><p>现实世界的应用通常包含多个在环境中运行的智能体。智能体通常被认为是自主的，并且需要学习策略以实现其目标。多智能体环境可以根据环境是否完全可观测、智能体的目标如何相关联等因素以多种方式形式化[6]。其中，部分可观测随机博弈（POSG）[19, 20]是最灵活的形式化之一。POSG 由一个元组定义，其中 I 是（有限）智能体集合，按{1, …, n}索引，S 是环境状态集合，𝜌是状态空间 S 上的初始状态分布，A 是智能体 i 可用的动作集合，O 是智能体 i 的观察集合。我们表示联合动作空间为 A = ×A，智能体联合观察空间为 O = ×O。因此，P ∶ S × A → Δ(S)表示从状态 s ∈ S 到新状态 s∈ S 的转移概率，给定智能体的联合动作⃗ a = ⟨a, …, a ⟩，其中⃗ a ∈ A。当环境过渡到新状态 s 时，给定联合动作⃗</p>
<p>a 根据观察概率函数 O ∶ S × A → Δ(O)确定。然后，每个智能体根据其自身的奖励函数 R∶ S × A × S → ℝ获得即时奖励。与联合动作和观察类似，我们可以将⃗ r = ⟨r, …, r⟩表示为联合奖励。如果智能体的奖励函数恰好相同，即它们具有相同的目标，那么对于每个时间步，r= r= … = r 都成立。在这种情况下，POSG 简化为 Dec-POMDP [6]。如果每个时间步的状态可以从智能体的当前观察集中唯一确定，即 s ≡⃗ o，则 Dec-POMDP 简化为 Dec-MDP。如果每个智能体都知道真实的环境状态，则 Dec-MDP 简化为多智能体 MDP。如果智能体集合中只有一个智能体，即 I = {1}，则多智能体 MDP 简化为 MDP，而 DecPOMDP 简化为 POMDP。由于部分可观测性，MARL 方法通常使用每个智能体的观察-动作历史𝜏= {o , a, o, …, o }直到时间步 t 来近似环境状态。请注意，为了简化，时间步 t 通常被省略。</p>
<p>在多智能体强化学习环境中，智能体可以以去中心化或中心化的方式学习其策略。在去中心化学习中（例如，去中心化 Q 学习[21, 22]），n 个智能体的多智能体强化学习问题被分解为 n 个去中心化的单智能体问题，其中每个智能体通过将所有其他智能体视为环境的一部分来学习自己的策略[23, 24]。在这样的去中心化设置中，每个智能体学习的策略取决于其局部观察和历史。去中心化学习的一个主要问题是环境的所谓非平稳性，即每个智能体在一个其他智能体同时探索和学习的环境中学习。中心化学习可以训练所有智能体的单个联合策略或中心化的价值函数，以促进 n 个去中心化策略的学习。虽然中心化（联合）学习消除了或减轻了部分可观察性和非平稳性问题，但它面临着联合行动（和观察）空间随着智能体数量及其行动的指数级扩展的挑战。 想要深入了解多智能体强化学习（MARL）中使用的各种训练方案，我们推荐阅读[17]的全面综述，该综述为策略的训练和执行提供了宝贵的见解。根据策略是从值函数导出还是直接学习，多智能体强化学习方法可以分为基于值的方法和基于策略的方法。这两种方法在通信多智能体深度强化学习（Comm-MADRL）中得到了广泛的应用。</p>
<h5 id="基于值"><a href="#基于值" class="headerlink" title="基于值"></a>基于值</h5><p>在多智能体情况下，基于值的方法借鉴了单智能体案例中的许多想法。作为最流行的基于值的方法之一，分布式 Q 学习为每个智能体学习一个局部 Q 函数。在智能体共享共同奖励的协作设置中，智能体 i 的更新规则如下：<br>其中 r 是共享奖励，ais 是下一个状态 s 中具有最高 Q 值的动作。在部分可观察环境中，环境状态不是完全可观察的，通常由每个代理的个体观察或历史记录来代替。每个状态-动作对的 Q 值根据 TD 误差逐步更新。这个误差，即 r + 𝛾 maxQ(s, a) − Q(s, a )，表示基于 Bellman 方程[25]的新估计（即 r + 𝛾 maxQ(s, a)）和当前估计（即 Q(s, a)）之间的差异。由于状态和动作空间可能太大，难以频繁遇到以进行准确估计，因此函数逼近方法，如深度神经网络，已经变得流行，为价值或策略模型赋予在离散和连续状态和动作上的泛化能力[12]。例如，深度 Q 网络（DQN）[12]最小化从采样奖励计算的新估计与参数化 Q 函数的当前估计之间的差异。在基于 DQN 的方法中，方程 1 中的 Q 函数表示为 Q(s, a ;𝜃)，它依赖于可学习的参数𝜃。 然而，基于价值的集中式学习方法学习一个联合 Q 函数 Q(s,⃗ a;𝜃)与参数𝜃。然而，这种方法在代理数量增加时难以扩展。值分解方法[26-29]是流行的多智能体强化学习（MARL）方法，可以将联合 Q 函数分解以实现高效的训练。这些方法也广泛应用于 Comm-MADRL 的研究工作中[30-32]。在部分可观察环境中，线性值分解方法将基于历史的联合 Q 函数分解如下：<br>其中，联合 Q 函数基于所有代理的联合历史，并基于个体历史分解为局部 Q 函数。权重 w 可以是固定值[26, 28]或受特定约束的可学习参数[29]。优势函数也可以替换上述方程中的 Q 函数以减少方差[33]。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://zcl0219.github.io/2024/10/30/DRL-Algorithms/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="GGBond">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="Doing the tough things sets winners apart from losers">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/10/30/DRL-Algorithms/" class="post-title-link" itemprop="url">DRL Algorithms</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2024-10-30 00:26:54 / Modified: 01:20:24" itemprop="dateCreated datePublished" datetime="2024-10-30T00:26:54+08:00">2024-10-30</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>在深度强化学习中，(\mathbb{E}_{S}[V(S, \theta)]) 表示对状态 (S) 的期望值。虽然 (S) 是状态，具体理解上可以这样考虑：</p>
<ol>
<li><p><strong>状态的分布</strong>：在强化学习中，状态 (S) 是随机变量，它遵循一个分布 (p(S))，该分布反映了状态在环境中的出现概率。这个分布可以是初始状态分布，也可以是策略执行时经过各个状态的访问分布。我们可以理解为期望值 (\mathbb{E}_{S}[V(S, \theta)]) 是对状态分布 (p(S)) 的加权平均。</p>
</li>
<li><p><strong>求期望的含义</strong>：当我们求 (\mathbb{E}_{S}[V(S, \theta)]) 时，实际上是在所有可能的状态上计算一个加权平均值，即把每个状态的价值函数 (V(S, \theta)) 按其出现的概率 (p(S)) 来进行加权。这种期望值通常用于估计模型在整个状态空间上的表现。</p>
</li>
<li><p><strong>样本估计</strong>：在实际中，这种期望值可以通过采样实现，比如通过从 (p(S)) 中采样得到一系列状态 (\{S_i\})，然后计算这些状态上 (V(S, \theta)) 的平均值来逼近 (\mathbb{E}_{S}[V(S, \theta)])。</p>
</li>
</ol>
<p>综上，(\mathbb{E}_{S}[V(S, \theta)]) 是对状态空间中的每个状态价值的加权平均，权重来自状态的分布 (p(S))。这种方法在强化学习中帮助我们通过全局状态分布来评估价值函数，从而更有效地训练模型。</p>
<p>随机梯度是对期望的蒙特卡洛近似</p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20241030002933171.png" alt="image-20241030002933171"></p>
<p>通常来说，J(\theta)是一个很复杂的函数，而L(\theta | \theta_{old})是一个相对简单的函数</p>
<p>构建L的方法多种多样，可以是J的二阶泰勒展开，也可以是J的蒙特卡洛近似</p>
<p>要maximization，即相当于求解一个带置信域约束的最优化问题，求解速度没有梯度上升和随机梯度上升算法快</p>
<p>但置信域算法强在比这两种算法表现稳定，哪怕最优化问题求解不是很准确。</p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20241030003652735.png" alt="image-20241030003652735"></p>
<p>J(\theta) = E_{S}[V_{\pi}(S)]中对S求期望，相当于消除了S，只剩下策略网络参数\theta</p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20241030004813870.png" alt="image-20241030004813870"></p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20241030005237667.png" alt="image-20241030005237667"></p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20241030005401769.png" alt="image-20241030005401769"></p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20241030010211366.png" alt="image-20241030010211366"></p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20241030010802155.png" alt="image-20241030010802155"></p>
<p>第四步需要内层循环求解最大化问题，第四步两个超参数，一个\delta，一个学习步长。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://zcl0219.github.io/2024/10/22/DiaDSP/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="GGBond">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="Doing the tough things sets winners apart from losers">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/10/22/DiaDSP/" class="post-title-link" itemprop="url">DiaDSP</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-10-22 21:38:57" itemprop="dateCreated datePublished" datetime="2024-10-22T21:38:57+08:00">2024-10-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-10-27 19:06:54" itemprop="dateModified" datetime="2024-10-27T19:06:54+08:00">2024-10-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Scholarly-Reads/" itemprop="url" rel="index"><span itemprop="name">Scholarly Reads</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="Differentially-Private-Distributed-Optimization-via-State-and-Direction-Perturbation-in-Multiagent-Systems"><a href="#Differentially-Private-Distributed-Optimization-via-State-and-Direction-Perturbation-in-Multiagent-Systems" class="headerlink" title="Differentially Private Distributed Optimization via State and Direction Perturbation in Multiagent Systems"></a>Differentially Private Distributed Optimization via State and Direction Perturbation in Multiagent Systems</h3>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2024/10/22/DiaDSP/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://zcl0219.github.io/2023/11/10/%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="GGBond">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="Doing the tough things sets winners apart from losers">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/11/10/%E7%AE%97%E6%B3%95/" class="post-title-link" itemprop="url">算法</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-11-10 18:44:01" itemprop="dateCreated datePublished" datetime="2023-11-10T18:44:01+08:00">2023-11-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-10-20 20:40:55" itemprop="dateModified" datetime="2024-10-20T20:40:55+08:00">2024-10-20</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>2023-11-10</p>
<p>并查集：<a target="_blank" rel="noopener" href="https://blog.csdn.net/YSJ367635984/article/details/113504723">https://blog.csdn.net/YSJ367635984/article/details/113504723</a></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2023/11/10/%E7%AE%97%E6%B3%95/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://zcl0219.github.io/2023/10/22/Zookeeper/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="GGBond">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="Doing the tough things sets winners apart from losers">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/22/Zookeeper/" class="post-title-link" itemprop="url">Zookeeper</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-10-22 18:27:16 / Modified: 20:52:38" itemprop="dateCreated datePublished" datetime="2023-10-22T18:27:16+08:00">2023-10-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>一、Zookeeper特点</p>
<ol>
<li>Zookeeper:一个领导者(Leader)，多个跟随者(Follower)组成的集群。<br>2)集群中只要有半数以上节点存活，Zookeeper集群就能正常服务。所以Zookeeper适合安装奇数台服务器。<br>3)全局数据一致:每个Server保存一份相同的数据副本， Client无论连接到哪个Server，数据都是一致的。<br>4)更新请求顺序执行，来自同一个Client的更新请求按其发送顺序依次执行。<br>5)数据更新原子性，一次数据更新要么成功，要么失败。<br>6)实时性，在一定时间范围内，Client能读到最新数据。</li>
</ol>
<p>二、数据结构<br>ZooKeeper数据模型的结构与Unix文件系统很类似，整体上可以看作是一棵树，每个节点称做一个ZNode。每一个ZNode默认能够存储1MB 的数据，每个ZNode都可以通过其路径唯一标识。</p>
<p>三、应用场景</p>
<p>提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等。</p>
<p>四、配置参数</p>
<p>tickTime &#x3D; 2000：通信心跳时间，Zookeeper服务器与客户端心跳时间，单位毫秒</p>
<p>initLimit &#x3D; 10：LF初始通信时限，10次心跳，也就是20s，Leader和Follower初始链接时限</p>
<p>syncLimit &#x3D; 5：LF同步通信时限，两者之间的通信时间（5s）</p>
<p>dataDir：保存Zookeeper中的数据，可以修改</p>
<p>clientPort &#x3D; 2181：客户端连接端口，通常不做修改</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://zcl0219.github.io/2023/10/22/%E5%AF%B9%E8%B1%A1%E7%9A%84%E4%BC%98%E5%8C%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="GGBond">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="Doing the tough things sets winners apart from losers">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/22/%E5%AF%B9%E8%B1%A1%E7%9A%84%E4%BC%98%E5%8C%96/" class="post-title-link" itemprop="url">对象的优化</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-10-22 16:31:53 / Modified: 18:26:35" itemprop="dateCreated datePublished" datetime="2023-10-22T16:31:53+08:00">2023-10-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>一、拷贝构造、赋值运算符重载的相关知识</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Test</span>&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Test</span>(<span class="type">int</span> a = <span class="number">10</span>): <span class="built_in">ma</span>(a) &#123; cout &lt;&lt; <span class="string">&quot;Test()&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">    <span class="built_in">Test</span>(<span class="type">const</span> Test &amp;t): <span class="built_in">ma</span>(t.ma) &#123; cout &lt;&lt; <span class="string">&quot;Test(const Test&amp;)&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">    Test&amp; <span class="keyword">operator</span>=(<span class="type">const</span> Test &amp;t)&#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;operator=&quot;</span> &lt;&lt; endl;</span><br><span class="line">        ma = t.ma;</span><br><span class="line">        <span class="keyword">return</span> *<span class="keyword">this</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    ~<span class="built_in">Test</span>() &#123; cout &lt;&lt; <span class="string">&quot;~Test()&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">int</span> ma;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    Test t1;</span><br><span class="line">    <span class="function">Test <span class="title">t2</span><span class="params">(t1)</span></span>;</span><br><span class="line">    Test t3 = t1;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Test(20)显式生成临时对象 生存周期：所在的语句</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    c++编译器对于对象构造的优化：</span></span><br><span class="line"><span class="comment">    用临时对象生成新对象的时候，临时对象就不产生了，直接构造新对象就可以了。</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    Test t4 = <span class="built_in">Test</span>(<span class="number">20</span>); <span class="comment">// Test t4(20)没有区别的!</span></span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;-----------------&quot;</span> &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    t4 = t2; <span class="comment">// t4已经存在，这是赋值</span></span><br><span class="line">    t4 = <span class="built_in">Test</span>(<span class="number">30</span>); <span class="comment">// t4.operator=(const Test &amp;t)</span></span><br><span class="line">    t4 = (Test)<span class="number">30</span>; <span class="comment">// int -&gt; Test,强制转换编译器会看有没有合适的构造函数</span></span><br><span class="line">    t4 = <span class="number">30</span>; <span class="comment">// 隐式生成临时对象 隐式类型转换，注意如果30换成一个char类型变量，那就不行了，因为没有对应的构造	  函数。</span></span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;-----------------&quot;</span> &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    Test *p = &amp;<span class="built_in">Test</span>(<span class="number">40</span>); <span class="comment">// p指向的是一个已经析构的临时对象</span></span><br><span class="line">    <span class="type">const</span> Test &amp;ref = <span class="built_in">Test</span>(<span class="number">50</span>);</span><br><span class="line">    <span class="comment">// 使用引用引用一个临时变量，临时变量的周期就变为了变量的生命周期</span></span><br><span class="line">    <span class="comment">// 自己思考为引用给变量起了一个别名，在引用的生命周期内，就可以继续访问这个临时变量了</span></span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;-----------------&quot;</span> &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Test</span>&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Test</span>(<span class="type">int</span> a = <span class="number">5</span>, <span class="type">int</span> b = <span class="number">5</span>): <span class="built_in">ma</span>(a), <span class="built_in">mb</span>(b)&#123; </span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;Test(int, int)&quot;</span> &lt;&lt; endl; </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">Test</span>(<span class="type">const</span> Test &amp;src): <span class="built_in">ma</span>(src.ma), <span class="built_in">mb</span>(src.mb)&#123; </span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;Test(const Test&amp;)&quot;</span> &lt;&lt; endl; </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">void</span> <span class="keyword">operator</span>=(<span class="type">const</span> Test &amp;src)&#123;</span><br><span class="line">        ma = src.ma;</span><br><span class="line">        mb = src.mb;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;operator=&quot;</span> &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    ~<span class="built_in">Test</span>() &#123; cout &lt;&lt; <span class="string">&quot;~Test()&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">int</span> ma;</span><br><span class="line">    <span class="type">int</span> mb;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">Test <span class="title">t1</span><span class="params">(<span class="number">10</span>, <span class="number">10</span>)</span></span>;  <span class="comment">//1. Test(int , int)</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="function">Test <span class="title">t2</span><span class="params">(<span class="number">20</span>, <span class="number">20</span>)</span></span>;  <span class="comment">//3. Test(int , int)</span></span><br><span class="line">    Test t3 = t2; <span class="comment">// 4.Test(const Test&amp;)</span></span><br><span class="line">    <span class="type">static</span> Test t4 = <span class="built_in">Test</span>(<span class="number">30</span>, <span class="number">30</span>); <span class="comment">//5. Test(int, int)</span></span><br><span class="line">    t2 = <span class="built_in">Test</span>(<span class="number">40</span>, <span class="number">40</span>); <span class="comment">//6.Test(int, int), operetor=, ~Test()</span></span><br><span class="line">    t2 = (Test)(<span class="number">50</span>, <span class="number">50</span>); <span class="comment">//7.Test(int, int), operetor=, ~Test()</span></span><br><span class="line">    t2 = <span class="number">60</span>; <span class="comment">//8.Test(int, int), operetor=, ~Test()</span></span><br><span class="line">    Test *p1 = <span class="keyword">new</span> <span class="built_in">Test</span>(<span class="number">70</span>, <span class="number">70</span>); <span class="comment">// 9.Test(int, int),堆上需要显式释放</span></span><br><span class="line">    Test *p2 = <span class="keyword">new</span> Test[<span class="number">2</span>]; <span class="comment">// 10.Test(int, int), Test(int, int)</span></span><br><span class="line">    Test *p3 = &amp;<span class="built_in">Test</span>(<span class="number">80</span>, <span class="number">80</span>); <span class="comment">//11.Test(int, int), ~Test()</span></span><br><span class="line">    <span class="type">const</span> Test &amp;p4 = <span class="built_in">Test</span>(<span class="number">90</span>, <span class="number">90</span>); <span class="comment">//12.Test(int, int)</span></span><br><span class="line">    <span class="keyword">delete</span> p1; <span class="comment">// 13.~Test()</span></span><br><span class="line">    <span class="keyword">delete</span> []p2; <span class="comment">// 14. ~Test(), ~Test()</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function">Test <span class="title">t5</span><span class="params">(<span class="number">100</span>, <span class="number">100</span>)</span></span>;  <span class="comment">//2. Test(int , int)</span></span><br><span class="line"><span class="comment">// 注意，t4的析构最晚，它在数据段，需要等程序结束。</span></span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">GGBond</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"cdn":"https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML","tags":"none","src":"custom_mathjax_source","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
