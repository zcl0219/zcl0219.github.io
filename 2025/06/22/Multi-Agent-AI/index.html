<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/avatar-32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/avatar-16.png">
  <link rel="mask-icon" href="/images/emoji-smile.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Code:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"zcl0219.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":null,"show_result":true},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="引言 q1：多智能体强化学习系统特点 智能体数量非单个 智能体之间存在一定联系：合作、对抗、合作+对抗（多智能体多动机） 智能体不仅需要考虑到环境，还需要考虑到其他智能体的行动 学习环境动态变化  博弈论 minmax nash equilibrium    上图说明了制定出合理的策略需要智能体之间communication  局部最优并不代表全局最优   对彼此的最佳对策被称为纳什均衡  纳什">
<meta property="og:type" content="article">
<meta property="og:title" content="Multi-Agent AI">
<meta property="og:url" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="引言 q1：多智能体强化学习系统特点 智能体数量非单个 智能体之间存在一定联系：合作、对抗、合作+对抗（多智能体多动机） 智能体不仅需要考虑到环境，还需要考虑到其他智能体的行动 学习环境动态变化  博弈论 minmax nash equilibrium    上图说明了制定出合理的策略需要智能体之间communication  局部最优并不代表全局最优   对彼此的最佳对策被称为纳什均衡  纳什">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250622214102079.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250622215001166.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250622215250842.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250622215526657.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250622221246041.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250622221913906.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250622222258083.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250622222540677.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250622223016230.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250622223222044.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250622225345099.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250623201854412.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250623202222337.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250623202435065.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250623202916496.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250623203105804.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250623203455003.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250623203658761.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250623203835329.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250623204324562.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250623204526705.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250623204705478.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250623204723342.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250623210315029.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250623211203839.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250623212809384.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250623213442013.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250623214724248.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250624002522709.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250624003714695.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250624004513150.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250624194410787.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250624200511139.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250624201207684.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250624201541524.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250624202953517.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250624203148105.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250624204837963.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250624205950398.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250624211206447.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250624211304691.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250624232921182.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250624233936853.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250625000231647.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250625213116525.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250630224225904.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250630224137356.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250701210516613.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250701211212340.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250701215722881.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250701215827727.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250702003904470.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250702005255997.png">
<meta property="article:published_time" content="2025-06-22T13:04:54.000Z">
<meta property="article:modified_time" content="2025-07-01T16:58:44.314Z">
<meta property="article:author" content="GGBond">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/Users/zcl/AppData/Roaming/Typora/typora-user-images/image-20250622214102079.png">


<link rel="canonical" href="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/","path":"2025/06/22/Multi-Agent-AI/","title":"Multi-Agent AI"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Multi-Agent AI | Hexo</title>
  







<link href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap" rel="stylesheet">

  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Hexo</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-%E9%87%8D%E5%A4%8D%E5%8D%9A%E5%BC%88"><span class="nav-number">1.</span> <span class="nav-text">2.1 重复博弈</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-%E6%89%A9%E5%B1%95%E5%BD%A2%E5%BC%8F%E5%8D%9A%E5%BC%88"><span class="nav-number">2.</span> <span class="nav-text">2.2 扩展形式博弈</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%B3%E4%BB%80%E5%9D%87%E8%A1%A1%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="nav-number">2.1.</span> <span class="nav-text">纳什均衡的定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%EF%BC%88L-RL%EF%BC%89%E4%B8%8D%E6%98%AF%E7%BA%B3%E4%BB%80%E5%9D%87%E8%A1%A1%EF%BC%9F"><span class="nav-number">2.2.</span> <span class="nav-text">为什么（L, RL）不是纳什均衡？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88PPT%E4%B8%AD%E5%88%97%E5%87%BA%E7%9A%84%EF%BC%88R-RL%EF%BC%89%E6%98%AF%E7%BA%B3%E4%BB%80%E5%9D%87%E8%A1%A1%EF%BC%8C%E8%80%8C%EF%BC%88L-RL%EF%BC%89%E4%B8%8D%E6%98%AF%EF%BC%9F"><span class="nav-number">2.3.</span> <span class="nav-text">为什么PPT中列出的（R, RL）是纳什均衡，而（L, RL）不是？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">2.4.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%9A%E5%BC%88%E7%BB%93%E6%9E%84%E4%B8%8E%E6%94%B6%E7%9B%8A"><span class="nav-number">2.5.</span> <span class="nav-text">博弈结构与收益</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%89%A9%E5%B1%95%E5%BC%8F%E8%A1%A8%E7%A4%BA%EF%BC%88%E5%8D%9A%E5%BC%88%E6%A0%91%EF%BC%89%EF%BC%9A"><span class="nav-number">2.5.1.</span> <span class="nav-text">扩展式表示（博弈树）：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%87%E5%87%86%E5%BC%8F%E8%A1%A8%E7%A4%BA%EF%BC%88%E6%94%B6%E7%9B%8A%E7%9F%A9%E9%98%B5%EF%BC%89%EF%BC%9A"><span class="nav-number">2.5.2.</span> <span class="nav-text">标准式表示（收益矩阵）：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%B3%E4%BB%80%E5%9D%87%E8%A1%A1%E5%88%86%E6%9E%90"><span class="nav-number">2.6.</span> <span class="nav-text">纳什均衡分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9D%87%E8%A1%A1%E5%90%88%E7%90%86%E6%80%A7%E6%A3%80%E9%AA%8C"><span class="nav-number">2.7.</span> <span class="nav-text">均衡合理性检验</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88-Out-F-%E4%B8%8D%E5%90%88%E7%90%86%EF%BC%9F"><span class="nav-number">2.7.1.</span> <span class="nav-text">为什么 (Out, F) 不合理？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%86%E5%90%91%E5%BD%92%E7%BA%B3%E9%AA%8C%E8%AF%81%EF%BC%9A"><span class="nav-number">2.7.2.</span> <span class="nav-text">逆向归纳验证：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%93%E8%AE%BA"><span class="nav-number">2.8.</span> <span class="nav-text">结论</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-%E5%8A%BF%E5%8D%9A%E5%BC%88"><span class="nav-number">3.</span> <span class="nav-text">2.3 势博弈</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E6%A0%B8%E5%BF%83%E7%BB%93%E8%AE%BA"><span class="nav-number">3.0.1.</span> <span class="nav-text">1. 核心结论</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E6%95%B0%E5%AD%A6%E8%AF%81%E6%98%8E%EF%BC%88%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%B8%AA%E7%BB%93%E8%AE%BA%E6%88%90%E7%AB%8B%EF%BC%9F%EF%BC%89"><span class="nav-number">3.0.2.</span> <span class="nav-text">2. 数学证明（为什么这个结论成立？）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E2%80%9C%E8%B7%AF%E5%BE%84%E2%80%9D%E4%B8%8E%E2%80%9C%E6%94%B9%E8%BF%9B%E8%B7%AF%E5%BE%84%E2%80%9D%E7%9A%84%E9%80%9A%E4%BF%97%E8%A7%A3%E9%87%8A"><span class="nav-number">3.0.3.</span> <span class="nav-text">1. “路径”与“改进路径”的通俗解释</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E2%80%9C%E7%9F%AD%E8%A7%86%E5%8F%82%E4%B8%8E%E8%80%85%E2%80%9D-Myopic-Players-%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="nav-number">3.0.4.</span> <span class="nav-text">2. “短视参与者” (Myopic Players) 的概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%9C%A8%E5%8A%BF%E5%8D%9A%E5%BC%88%E4%B8%AD%E8%AE%A8%E8%AE%BA%E8%BF%99%E4%B8%AA%EF%BC%9F%EF%BC%88%E6%A0%B8%E5%BF%83%E6%B4%9E%E8%A7%81%EF%BC%89"><span class="nav-number">3.0.5.</span> <span class="nav-text">3. 为什么要在势博弈中讨论这个？（核心洞见）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%8D%9A%E5%BC%88%E7%9A%84%E6%A0%B8%E5%BF%83%E8%A6%81%E7%B4%A0"><span class="nav-number">3.0.6.</span> <span class="nav-text">1. 博弈的核心要素</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%8D%9A%E5%BC%88%E7%9A%84%E5%86%85%E5%9C%A8%E5%86%B2%E7%AA%81"><span class="nav-number">3.0.7.</span> <span class="nav-text">2. 博弈的内在冲突</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E4%B8%8E%E2%80%9C%E5%8A%BF%E5%8D%9A%E5%BC%88%E2%80%9D%E7%9A%84%E6%B7%B1%E5%88%BB%E8%81%94%E7%B3%BB%EF%BC%88%E6%A0%B8%E5%BF%83%E6%B4%9E%E8%A7%81%EF%BC%89"><span class="nav-number">3.0.8.</span> <span class="nav-text">3. 与“势博弈”的深刻联系（核心洞见）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E9%87%8D%E8%A6%81%E6%8E%A8%E8%AE%BA"><span class="nav-number">3.0.9.</span> <span class="nav-text">4. 重要推论</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-%E9%9B%B6%E5%92%8C%E5%8D%9A%E5%BC%88%E5%8F%8A%E7%BA%B3%E4%BB%80%E5%9D%87%E8%A1%A1%E8%AE%A1%E7%AE%97"><span class="nav-number">4.</span> <span class="nav-text">3.1 零和博弈及纳什均衡计算</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E4%B8%8E%E5%8F%A4%E8%AF%BA%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A0%B9%E6%9C%AC%E5%8C%BA%E5%88%AB"><span class="nav-number">4.0.1.</span> <span class="nav-text">1. 与古诺模型的根本区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%A6%82%E4%BD%95%E6%B1%82%E8%A7%A3%EF%BC%9F%E2%80%94%E2%80%94-%E9%80%86%E5%90%91%E5%BD%92%E7%BA%B3%E6%B3%95-Backward-Induction"><span class="nav-number">4.0.2.</span> <span class="nav-text">2. 如何求解？—— 逆向归纳法 (Backward Induction)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E5%85%88%E6%89%8B%E4%BC%98%E5%8A%BF"><span class="nav-number">4.0.3.</span> <span class="nav-text">3. 先手优势</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E4%BB%80%E4%B9%88%E6%98%AF%E9%9B%B6%E5%92%8C%E5%8D%9A%E5%BC%88%EF%BC%9F"><span class="nav-number">4.0.4.</span> <span class="nav-text">1. 什么是零和博弈？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E2%80%9C%E6%9E%81%E5%B0%8F%E5%8C%96%E6%9E%81%E5%A4%A7%E2%80%9D%E5%AE%9A%E7%90%86%E7%9A%84%E9%80%9A%E4%BF%97%E8%A7%A3%E9%87%8A"><span class="nav-number">4.0.5.</span> <span class="nav-text">2. “极小化极大”定理的通俗解释</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E6%B7%B7%E5%90%88%E7%AD%96%E7%95%A5%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7"><span class="nav-number">4.0.6.</span> <span class="nav-text">3. 混合策略的重要性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A%E9%AA%8C%E8%AF%81%E4%B8%8D%E5%AD%98%E5%9C%A8%E7%BA%AF%E7%AD%96%E7%95%A5%E7%BA%B3%E4%BB%80%E5%9D%87%E8%A1%A1"><span class="nav-number">4.0.7.</span> <span class="nav-text">第一步：验证不存在纯策略纳什均衡</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A%E8%AE%BE%E5%AE%9A%E6%B7%B7%E5%90%88%E7%AD%96%E7%95%A5"><span class="nav-number">4.0.8.</span> <span class="nav-text">第二步：设定混合策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E6%AD%A5%EF%BC%9A%E8%AE%A1%E7%AE%97%E5%8F%82%E4%B8%8E%E8%80%851%E7%9A%84%E6%B7%B7%E5%90%88%E7%AD%96%E7%95%A5-p"><span class="nav-number">4.0.9.</span> <span class="nav-text">第三步：计算参与者1的混合策略 p</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AC%AC%E5%9B%9B%E6%AD%A5%EF%BC%9A%E8%AE%A1%E7%AE%97%E5%8F%82%E4%B8%8E%E8%80%852%E7%9A%84%E6%B7%B7%E5%90%88%E7%AD%96%E7%95%A5-q"><span class="nav-number">4.0.10.</span> <span class="nav-text">第四步：计算参与者2的混合策略 q</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AC%AC%E4%BA%94%E6%AD%A5%EF%BC%9A%E7%BB%93%E8%AE%BA%E4%B8%8E%E5%8D%9A%E5%BC%88%E7%9A%84%E4%BB%B7%E5%80%BC"><span class="nav-number">4.0.11.</span> <span class="nav-text">第五步：结论与博弈的价值</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3%EF%BC%9A%E6%B6%88%E9%99%A4%E5%AF%B9%E6%96%B9%E7%9A%84%E7%A1%AE%E5%AE%9A%E6%80%A7%E6%9C%80%E4%BC%98%E8%A7%A3"><span class="nav-number">4.1.</span> <span class="nav-text">1. 核心思想：消除对方的确定性最优解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%8F%8D%E5%90%91%E6%80%9D%E8%80%83%EF%BC%9A%E5%A6%82%E6%9E%9C%E4%B8%8D%E8%AE%A9%E5%AF%B9%E6%96%B9%E6%97%A0%E5%B7%AE%E5%BC%82%E4%BC%9A%E6%80%8E%E6%A0%B7%EF%BC%9F"><span class="nav-number">4.2.</span> <span class="nav-text">2. 反向思考：如果不让对方无差异会怎样？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E4%B8%80%E4%B8%AA%E7%9B%B4%E8%A7%82%E7%9A%84%E4%BE%8B%E5%AD%90%EF%BC%9A%E7%82%B9%E7%90%83%E5%A4%A7%E6%88%98"><span class="nav-number">4.3.</span> <span class="nav-text">3. 一个直观的例子：点球大战</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-%E6%9E%81%E5%A4%A7%E6%9E%81%E5%B0%8F%E5%8D%9A%E5%BC%88"><span class="nav-number">5.</span> <span class="nav-text">3.2 极大极小博弈</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A%E6%A3%80%E6%9F%A5%E6%98%AF%E5%90%A6%E5%AD%98%E5%9C%A8%E7%BA%AF%E7%AD%96%E7%95%A5%E5%9D%87%E8%A1%A1"><span class="nav-number">5.0.1.</span> <span class="nav-text">第一步：检查是否存在纯策略均衡</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A%E8%AE%A1%E7%AE%97%E6%B7%B7%E5%90%88%E7%AD%96%E7%95%A5%E7%BA%B3%E4%BB%80%E5%9D%87%E8%A1%A1"><span class="nav-number">5.0.2.</span> <span class="nav-text">第二步：计算混合策略纳什均衡</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8E%A8%E5%AF%BC%E5%8D%9A%E5%BC%88%E7%9A%84%E4%BB%B7%E5%80%BC-V"><span class="nav-number">5.0.3.</span> <span class="nav-text">推导博弈的价值 V</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E9%97%AE%E9%A2%98%E7%9A%84%E6%A0%B8%E5%BF%83%EF%BC%9A%E9%80%92%E5%BD%92%E7%9A%84%E4%BB%B7%E5%80%BC"><span class="nav-number">5.0.4.</span> <span class="nav-text">1. 问题的核心：递归的价值</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E6%B1%82%E8%A7%A3%E5%8D%9A%E5%BC%88%E4%BB%B7%E5%80%BC-V"><span class="nav-number">5.0.5.</span> <span class="nav-text">2. 求解博弈价值 V</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E9%80%89%E6%8B%A9%E6%AD%A3%E7%A1%AE%E7%9A%84%E8%A7%A3"><span class="nav-number">5.0.6.</span> <span class="nav-text">3. 选择正确的解</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-2-%E5%80%BC%E8%BF%AD%E4%BB%A3%E4%B8%8E%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3"><span class="nav-number">6.</span> <span class="nav-text">7.2 值迭代与策略迭代</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3%EF%BC%9A%E6%97%A0%E9%99%90%E8%BF%87%E7%A8%8B%EF%BC%8C%E6%9C%89%E9%99%90%E4%BB%B7%E5%80%BC"><span class="nav-number">6.0.1.</span> <span class="nav-text">1. 核心思想：无限过程，有限价值</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E8%BF%99%E4%B8%AA%E4%B8%8A%E9%99%90%E5%85%AC%E5%BC%8F%E6%98%AF%E6%80%8E%E4%B9%88%E6%9D%A5%E7%9A%84%EF%BC%9F%EF%BC%88%E5%87%A0%E4%BD%95%E7%BA%A7%E6%95%B0%EF%BC%89"><span class="nav-number">6.0.2.</span> <span class="nav-text">2. 这个上限公式是怎么来的？（几何级数）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E8%BF%99%E4%B8%AA%E6%80%A7%E8%B4%A8%E4%B8%BA%E4%BB%80%E4%B9%88%E9%87%8D%E8%A6%81%EF%BC%9F"><span class="nav-number">6.0.3.</span> <span class="nav-text">3. 这个性质为什么重要？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%B4%9E%E5%AF%9F"><span class="nav-number">6.0.4.</span> <span class="nav-text">结论与洞察</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E6%8C%87%E6%95%B0%E9%80%9F%E7%8E%87%E6%94%B6%E6%95%9B%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AF%B4%E5%AE%83%E2%80%9C%E9%AB%98%E6%95%88%E2%80%9D%EF%BC%9F"><span class="nav-number">6.0.5.</span> <span class="nav-text">1. 指数速率收敛：为什么说它“高效”？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E8%AF%AF%E5%B7%AE%E8%BE%B9%E7%95%8C%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AF%B4%E5%AE%83%E2%80%9C%E5%8F%AF%E9%9D%A0%E2%80%9D%EF%BC%9F"><span class="nav-number">6.0.6.</span> <span class="nav-text">2. 误差边界：为什么说它“可靠”？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E2%80%9C%E6%9C%80%E5%B0%8F%E5%81%9C%E6%AD%A2%E6%A6%82%E7%8E%87%E6%98%AF0-5%E2%80%9D%E7%9A%84%E6%9D%A5%E6%BA%90%E5%88%86%E6%9E%90"><span class="nav-number">6.1.</span> <span class="nav-text">1. “最小停止概率是0.5”的来源分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E8%BF%99%E4%B8%AA%E6%95%B0%E5%AD%97%E4%B8%BA%E4%BB%80%E4%B9%88%E5%A6%82%E6%AD%A4%E9%87%8D%E8%A6%81%EF%BC%9F"><span class="nav-number">6.2.</span> <span class="nav-text">2. 这个数字为什么如此重要？</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="GGBond"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">GGBond</p>
  <div class="site-description" itemprop="description">Doing the tough things sets winners apart from losers</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">33</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="GGBond">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="Doing the tough things sets winners apart from losers">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Multi-Agent AI | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Multi-Agent AI
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-06-22 21:04:54" itemprop="dateCreated datePublished" datetime="2025-06-22T21:04:54+08:00">2025-06-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-07-02 00:58:44" itemprop="dateModified" datetime="2025-07-02T00:58:44+08:00">2025-07-02</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><ol>
<li><p>引言</p>
<p>q1：多智能体强化学习系统特点</p>
<p>智能体数量非单个</p>
<p>智能体之间存在一定联系：合作、对抗、合作+对抗（多智能体多动机）</p>
<p>智能体不仅需要考虑到环境，还需要考虑到其他智能体的行动</p>
<p>学习环境动态变化</p>
</li>
<li><p>博弈论</p>
<p>minmax</p>
<p>nash equilibrium</p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250622214102079.png" alt="image-20250622214102079"></p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250622215001166.png" alt="image-20250622215001166"></p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250622215250842.png" alt="image-20250622215250842"></p>
<p>上图说明了制定出合理的策略需要智能体之间communication</p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250622215526657.png" alt="image-20250622215526657"></p>
<p>局部最优并不代表全局最优</p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250622221246041.png" alt="image-20250622221246041"></p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250622221913906.png" alt="image-20250622221913906"></p>
<p>对彼此的最佳对策被称为纳什均衡</p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250622222258083.png" alt="image-20250622222258083"></p>
<p>纳什均衡可以有多个</p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250622222540677.png" alt="image-20250622222540677"></p>
<p>博弈定义</p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250622223016230.png" alt="image-20250622223016230"></p>
<p>占优策略：对于一个玩家来说，拥有一个策略a，不论其他玩家选择任何策略，选择策略a都会使得他的效用u最大，这个策略a就被称为占优策略</p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250622223222044.png" alt="image-20250622223222044"></p>
<p>n-玩家纳什均衡定义</p>
<p>q2：是不是所有博弈情况都存在纳什均衡？</p>
</li>
<li><p>混合策略纳什均衡</p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250622225345099.png" alt="image-20250622225345099"></p>
</li>
</ol>
<h2 id="2-1-重复博弈"><a href="#2-1-重复博弈" class="headerlink" title="2.1 重复博弈"></a>2.1 重复博弈</h2><p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623201854412.png" alt="image-20250623201854412"></p>
<ul>
<li><p>为什么要研究重复博弈？</p>
<p>生活中我们参与的许多战略互动都是持续进行的，比如，我们会与相同的人重复互动等。重复博弈理论提供了一个框架用以研究这种重复行为</p>
</li>
</ul>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623202222337.png" alt="image-20250623202222337"></p>
<ul>
<li>重复博弈定义：同一个基础博弈（阶段博弈）被相同的参与者多次重复进行。换句话说，在重复博弈中，一个标准式博弈被同样的参与者反复进行。</li>
</ul>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623202435065.png" alt="image-20250623202435065"></p>
<ul>
<li><p>惩罚的威胁：</p>
<p>惩罚的威胁是理解重复博弈引入折扣因子的关键概念。基本思想是参与者可能会因为<strong>“惩罚”的“威胁”</strong>而被阻止利用其短期优势，这种威胁的直观效果就是会降低其长期收益。</p>
<p>通过未来惩罚的威胁来维持合作。参与者在做决策时不仅要考虑当前收益，还要考虑背叛行为可能招致的未来惩罚，从而在长期利益考量下选择合作而非短期的机会主义行为。</p>
</li>
</ul>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623202916496.png" alt="image-20250623202916496"></p>
<ul>
<li><p>重复博弈：囚徒困境</p>
<p>在单次博弈中，背叛是占优策略，但在重复博弈中，未来惩罚的威胁可能使合作成为可能。这为分析现实中的长期合作关系提供了理论基础。</p>
</li>
</ul>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623203105804.png" alt="image-20250623203105804"></p>
<ul>
<li><p>囚徒困境有一个唯一的纳什均衡：每个参与者都选D（背叛）</p>
</li>
<li><p>现在引入冷酷触发策略：</p>
<ul>
<li><p>只要对方参与者选择C，就选择C；</p>
</li>
<li><p>如果在任何时期对方参与者选择D，那么在<strong>此后的每个时期</strong>都选择D</p>
</li>
</ul>
</li>
<li><p>另一个参与者应该怎么做？</p>
<ul>
<li>只要她对未来收益的重视程度与当前收益相比不是太小，她最好在每个时期都选择C</li>
</ul>
</li>
<li><p>冷酷触发策略的工作原理：通过”一旦背叛，永远惩罚”的威胁来维持合作。关键在于参与者对未来收益的重视程度（贴现因子）必须足够高，使得长期合作的收益超过短期背叛的收益。</p>
</li>
</ul>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623203455003.png" alt="image-20250623203455003"></p>
<ul>
<li>每次都选择C的策略是应对冷酷触发策略最好的策略</li>
<li>为什么？<ul>
<li>如果她在每个时期都选择C，那么每个时期的结果都是(C,C)，她在每个时期获得收益2</li>
<li>如果她在某个时期转向D，那么她在该时期获得收益3，在此后的每个时期获得收益1</li>
<li>然而，只要她对未来收益的重视程度与当前收益相比不是太小，收益流(3,1,1,…)对她来说比收益流(2,2,2,…)更差 • 因此她最好在每个时期都选择C</li>
</ul>
</li>
<li>为什么冷酷触发策略能够维持合作：虽然背叛能带来一次性的更高收益（3 vs 2），但随后的永久惩罚（每期收益1）使得总体收益低于持续合作（每期收益2）。关键条件是参与者必须足够重视未来收益。</li>
</ul>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623203658761.png" alt="image-20250623203658761"></p>
<ul>
<li>这张幻灯片指出了重复博弈中的多重均衡问题。除了通过冷酷触发策略维持的合作均衡外，还存在”总是背叛”的均衡。在这种均衡中，由于对方无论如何都会背叛，自己也最好选择背叛，这与单次囚徒困境的结果相同。这说明重复博弈可能存在多个纳什均衡，既有合作的也有非合作的。</li>
</ul>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623203835329.png" alt="image-20250623203835329"></p>
<ul>
<li><p>两个关键问题：</p>
<ul>
<li><p><strong>耐心程度的量化</strong>：要维持合作均衡，参与者需要多重视未来收益？这涉及贴现因子的临界值计算。</p>
</li>
<li><p><strong>均衡结果的多样性</strong>：除了完全合作(C,C)和完全背叛(D,D)之外，还有哪些策略组合和结果可以构成纳什均衡？</p>
</li>
</ul>
</li>
</ul>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623204324562.png" alt="image-20250623204324562"></p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623204526705.png" alt="image-20250623204526705"></p>
<ul>
<li>通过冷酷触发策略结合折扣因子来解释折扣因子的边界取值。分别是一直选择合作（C）以及中途换选择（D），分别计算其策略收益，最后比较收益值，即可计算出折扣因子的边界值。</li>
<li><strong>当δ &lt; 1/2时，”一直背叛”是纳什均衡</strong>：<ul>
<li>无论δ值如何，”一直背叛”策略<strong>总是纳什均衡</strong>：<ul>
<li>如果玩家1总是背叛，玩家2的最佳响应是总是背叛（因为如果玩家2合作，支付为0；如果背叛，支付为1）。</li>
<li>同样，如果玩家2总是背叛，玩家1的最佳响应也是总是背叛。</li>
<li>支付为每期(1,1)，现值为11−δ1−<em>δ</em>1。</li>
<li>没有玩家能通过单方面改变策略（如尝试合作）获得更高支付，因为合作会被立即剥削（支付0），且未来收益折现后不足以补偿。</li>
</ul>
</li>
<li>这个均衡总是存在，但它导致<strong>帕累托低效的结果</strong>（支付(1,1)低于合作时的(2,2)）。</li>
</ul>
</li>
</ul>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623204705478.png" alt="image-20250623204705478"></p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623204723342.png" alt="image-20250623204723342"></p>
<ul>
<li>有限步惩罚策略与“以牙还牙”策略计算折扣因子边界值思想与冷酷触发策略计算类似，这里不再详细解释。</li>
</ul>
<h2 id="2-2-扩展形式博弈"><a href="#2-2-扩展形式博弈" class="headerlink" title="2.2 扩展形式博弈"></a>2.2 扩展形式博弈</h2><p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623210315029.png" alt="image-20250623210315029"></p>
<ul>
<li><p>上述内容对比了策略型博弈和扩展式博弈的核心区别，并定义了一种特定类型的扩展式博弈<strong>（多阶段可观测行动博弈）</strong>。以下是我对关键点的理解：</p>
<ol>
<li><p><strong>核心区别 (Sequentiality &amp; Information):</strong></p>
<ul>
<li><strong>策略型博弈 (战略式博弈):</strong> 强调<strong>同时决策</strong>。玩家在不知道对手选择的情况下做出一次性决策（如石头剪刀布、静态 Cournot 模型）。收益矩阵是其主要表示形式。</li>
<li><strong>扩展式博弈 (扩展式):</strong> 强调<strong>行动的先后顺序 (序列性)</strong> 和<strong>信息结构</strong>。玩家在不同时间点行动，并且后行动的玩家可能（但不一定）能观察到先行动玩家的选择（如象棋、动态 Stackelberg 模型、序贯议价）。博弈树是其核心表示工具。</li>
</ul>
</li>
<li><p><strong>关注类型 (Multi-stage with Observed Actions):</strong></p>
<ul>
<li><strong>多阶段 (Multi-stage):</strong> 博弈过程被划分为不同的阶段。</li>
<li><strong>可观测行动 (Observed Actions):</strong> 这是定义中<strong>最关键的信息假设</strong>。它意味着在每个阶段开始时，<strong>所有玩家都完全知道之前所有阶段中所有玩家选择的所有行动</strong>。这被称为<strong>完美信息 (Perfect Information)</strong>，但PPT的表述更一般化，因为它允许同一阶段内的玩家<strong>同时行动 (Simultaneous Moves)</strong>。</li>
<li><strong>完美信息 vs. 可观测行动：</strong> “完美信息”通常指在<em>每个决策点</em>，玩家确切知道之前发生的<em>所有</em>行动（即知道整个历史，知道当前处于哪个决策节点）。PPT定义的“具有可观测行动的多阶段博弈”在阶段之间是完美信息的（玩家知道之前所有阶段的所有行动），但在一个阶段内部，如果存在同时行动，则在该阶段内行动时，玩家可能不知道同阶段其他玩家的<em>即时</em>选择（但在下一阶段开始前，这些行动会被揭示）。这比严格的“完美信息”博弈（要求每个决策点都无同时行动且完全知晓历史）范围更广。</li>
</ul>
</li>
<li><p><strong>表示工具 (Game Trees):</strong></p>
<ul>
<li>博弈树是表示扩展式博弈最直观的方式。树根代表起点，树枝代表玩家可能的行动，树节点代表决策点（轮到哪个或哪些玩家行动），树叶代表终点（对应收益/结果）。</li>
<li>它天然地刻画了<strong>行动顺序</strong>和可能的<strong>路径 (历史)</strong>。</li>
</ul>
</li>
<li><p><strong>关键概念 (Histories):</strong></p>
<ul>
<li><strong>历史记录 (Histories)</strong> 是扩展式博弈模型中的<strong>基础构件</strong>。一个历史就是一个从博弈开始到某个时间点为止，所有玩家按顺序采取的行动的序列。</li>
<li>每个决策点（博弈树节点）都对应一个<strong>唯一的历史</strong>，该历史描述了到达该节点所经过的路径。</li>
<li>玩家的<strong>信息集 (Information Sets)</strong> 通常由其无法区分的一组历史（节点）来定义。但在PPT定义的“具有可观测行动的多阶段博弈”中，由于行动完全可观测，每个信息集通常只包含一个节点（即玩家总是确切知道自己处于哪个决策点），除非在同一个阶段内存在同时行动（此时玩家可能不知道同阶段对手的<em>即时</em>选择）。</li>
</ul>
</li>
</ol>
</li>
</ul>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623211203839.png" alt="image-20250623211203839"></p>
<ul>
<li><p><strong>玩家角色：</strong></p>
<ul>
<li><strong>玩家1 - 进入者 (Entrant):</strong> 考虑是否进入一个新市场（通常是一个已有在位企业的市场）。</li>
<li><strong>玩家2 - 在位者 (Incumbent):</strong> 是市场现有的主导企业，对进入者的行动做出反应。</li>
</ul>
</li>
<li><p><strong>行动顺序与信息：</strong></p>
<ul>
<li>这是一个<strong>序贯博弈</strong>：玩家1先行动，玩家2后行动。</li>
<li><strong>关键信息假设：</strong> 玩家2在做出决策（容纳还是斗争）之前，<strong>完全观察到了玩家1的选择</strong>（进入或不进入）。这意味着玩家2知道博弈进行到了哪个决策点（即玩家1选了哪个行动）。</li>
<li>这符合之前定义的“<strong>具有可观测行动</strong>”的多阶段博弈。在这里，只有一个阶段玩家1行动，紧接着一个阶段玩家2行动，且玩家2的行动是基于完全知晓玩家1行动的情况下做出的。</li>
</ul>
</li>
<li><p><strong>博弈树表示：</strong></p>
<ul>
<li>这个例子非常适合用<strong>博弈树</strong>来表示：<ul>
<li><strong>根节点 (Root):</strong> 玩家1的决策点（进入 / 不进入）。</li>
<li><strong>中间节点 (Decision Nodes):</strong> 玩家1选择“进入”后，会到达玩家2的决策点（容纳 / 斗争）。玩家1选择“不进入”后，博弈直接结束。</li>
<li><strong>叶节点 / 终点节点 (Terminal Nodes / Leaves):</strong> 代表博弈结束的点，标有收益向量 ((x, y))。每个叶节点对应一个<strong>完整的历史</strong>（行动序列）和最终的收益结果。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>收益：</strong></p>
<ul>
<li>收益 ((x, y)) 的具体数值决定了博弈的结果和均衡。虽然没有给出具体数字，但典型的设定可能是：<ul>
<li><code>(不进入)</code>: 进入者收益为0（无成本无收入），在位者收益较高（维持垄断利润）。</li>
<li><code>(进入, 容纳)</code>: 进入者获得正利润（但低于垄断利润），在位者利润下降（但仍为正，因为共享市场）。</li>
<li><code>(进入, 斗争)</code>: 进入者亏损（因在位者发起价格战等），在位者也亏损（价格战成本）。虽然进入者损失可能更大，但斗争通常对双方都不利，是两败俱伤的结果。</li>
</ul>
</li>
<li>玩家2（在位者）的决策取决于哪种行动（容纳或斗争）在给定玩家1已进入的前提下，能给他带来更高的收益 (y)。</li>
</ul>
</li>
<li><p>要分析这个博弈的均衡（例如，子博弈精炼纳什均衡），就需要具体设定收益值，并逆向归纳求解玩家2在观察到“进入”后的最优反应，以及玩家1预测到玩家2的最优反应后，最初是否选择“进入”。</p>
</li>
</ul>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623212809384.png" alt="image-20250623212809384"></p>
<ul>
<li>扩展型博弈的一些基本符号定义</li>
</ul>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623213442013.png" alt="image-20250623213442013"></p>
<ul>
<li><p><strong>纯策略 (Pure Strategy):</strong></p>
<ul>
<li>在扩展式博弈中，纯策略不仅仅是玩家在博弈开始时的一个单一选择。它是一个<strong>完整的行动计划</strong>。</li>
<li>策略必须规定玩家在<strong>博弈的每一个可能阶段 (k)</strong>，面对<strong>每一个可能到达该阶段的历史路径 (hᵏ ∈ Hᵏ)</strong> 时，他会选择哪个可用的行动 (sᵢᵏ(hᵏ) ∈ Sᵢ(hᵏ)。</li>
<li><strong>为什么需要这么复杂？</strong> <ul>
<li>因为博弈是序贯的，并且玩家在决策时可能面临不同的局面（由不同的历史 hᵏ 描述）。一个完整的策略必须说明玩家在 <em>所有可能遇到的情况</em> 下会怎么做，即使某些情况在博弈实际进行中可能不会发生（如果玩家遵循这个策略的话）。</li>
</ul>
</li>
<li>形式上，玩家 i 的策略 sᵢ 是一个<strong>函数集合</strong> {sᵢ⁰, sᵢ¹, …, sᵢᴷ}，其中每个函数 sᵢᵏ 将阶段 k 的<em>历史集合 Hᵏ</em> 映射到该玩家在该历史下<em>可用的行动集合 Sᵢ(Hᵏ)</em> 中的一个具体行动。</li>
</ul>
</li>
<li><p><strong>策略组合生成博弈路径:</strong></p>
<ul>
<li>当所有玩家都选定他们的纯策略 (s₁, s₂, …, sₗ) 后，博弈的实际进行路径就被唯一确定了。</li>
<li>路径是通过<strong>递归应用</strong>所有玩家的策略函数来生成的：<ul>
<li><strong>阶段 0:</strong> 从初始历史 h⁰ = ∅ 开始。所有玩家根据他们的策略 sᵢ⁰(∅) 选择行动，形成行动组合 a⁰ = (s₁⁰(∅), s₂⁰(∅), …, sₗ⁰(∅))。阶段 0 后的历史变为 h¹ = a⁰。</li>
<li><strong>阶段 1:</strong> 面对历史 h¹ = a⁰。所有玩家根据他们的策略 sᵢ¹(a⁰) 选择行动，形成行动组合 a¹ = (s₁¹(a⁰), s₂¹(a⁰), …, sₗ¹(a⁰))。阶段 1 后的历史变为 h² = (a⁰, a¹)。</li>
<li><strong>后续阶段:</strong> 以此类推，直到最终阶段 K。阶段 K 后的历史 hᴷ⁺¹ = (a⁰, a¹, …, aᴷ) 就是终端历史。</li>
</ul>
</li>
<li>策略组合 s <strong>完全决定了</strong>终端历史 hᴷ⁺¹。</li>
</ul>
</li>
<li><p><strong>收益 (Payoffs):</strong></p>
<ul>
<li>玩家的收益取决于博弈的最终<strong>结果 (outcome)</strong>，即终端历史 hᴷ⁺¹。</li>
<li>每个玩家 i 有一个<strong>效用函数 (utility function)</strong> uᵢ，该函数将每个可能的终端历史 hᴷ⁺¹ 映射到一个实数，表示玩家 i 在该结果下获得的收益（或效用）。</li>
<li>由于策略组合 s 决定了终端历史 hᴷ⁺¹，因此我们也可以说策略组合 s 决定了每个玩家的收益，记作 uᵢ(s)。uᵢ(s) 本质上是 uᵢ 在由 s 决定的那个特定终端历史 hᴷ⁺¹ 上的取值。</li>
</ul>
</li>
<li><p>扩展式博弈中的核心概念：</p>
<ul>
<li><strong>纯策略:</strong> 是玩家针对<strong>所有可能历史</strong>制定的完整应变计划，表现为一组映射函数 {sᵢᵏ}，每个函数为特定阶段 k 的每个可能历史 hᵏ 指定一个行动。</li>
<li><strong>策略组合决定路径:</strong> 当所有玩家选定策略后，博弈的路径（行动序列 a⁰, a¹, …, aᴷ）和最终的终端历史 hᴷ⁺¹ 就被策略函数递归地、确定性地生成。</li>
<li><strong>收益定义在结果上:</strong> 玩家的收益由终端历史（博弈的最终结果）决定。效用函数 uᵢ 量化了玩家对每个可能结果的偏好。策略组合 s 通过决定终端历史 hᴷ⁺¹ 来间接决定每个玩家的收益 uᵢ(s)。</li>
</ul>
</li>
<li><p>理解纯策略的这种“完备应变计划”性质对于分析扩展式博弈的均衡（如子博弈精炼纳什均衡）至关重要，因为它要求玩家即使在“偏离均衡路径”的历史下（即如果博弈意外地到达了那里），也要指定一个行动。</p>
</li>
</ul>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250623214724248.png" alt="image-20250623214724248"></p>
<ul>
<li><strong>关键总结 (PPT最后一句):</strong></li>
</ul>
<blockquote>
<p>一个玩家的策略规定了该玩家在其<strong>轮到行动的每一个历史</strong>（例如，玩家2在<code>&#123;C&#125;</code>之后或<code>&#123;D&#125;</code>之后）处所选择的行动。</p>
</blockquote>
<ul>
<li>玩家 1 的策略只需要规定在唯一的历史 <code>∅</code> 处选择 <code>C</code> 或 <code>D</code>。</li>
<li>玩家 2 的策略必须规定在 <em>两个</em> 可能的历史 <code>&#123;C&#125;</code> 和 <code>&#123;D&#125;</code> 处分别选择什么行动（<code>E/F</code> 和 <code>G/H</code> 的组合）。</li>
</ul>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624002522709.png" alt="image-20250624002522709"></p>
<ul>
<li>在给定的扩展式博弈及其转换后的标准式（策略型）博弈中，策略组合（L, RL）不是纳什均衡（Nash equilibrium）。以下基于提供的收益矩阵和纳什均衡的定义，逐步解释原因。</li>
</ul>
<ul>
<li><p><strong>博弈的收益矩阵</strong>：</p>
<p>| 玩家1 \ 玩家2 | LL   | LR   | RL   | RR   |<br>| ——————- | —— | —— | —— | —— |<br>| <strong>L</strong>         | 3,2  | 3,2  | 2,3  | 2,3  |<br>| <strong>R</strong>         | 4,1  | 0,1  | 4,1  | 0,1  |</p>
</li>
<li><p><strong>玩家2的策略含义</strong>（RL策略）：</p>
<ul>
<li>RL = 如果玩家1选择L，则玩家2选择R；如果玩家1选择R，则玩家2选择L。</li>
</ul>
</li>
<li><p><strong>策略组合（L, RL）的含义</strong>：</p>
<ul>
<li>玩家1选择L。</li>
<li>玩家2选择RL策略（因此，当玩家1选L时，玩家2选R）。</li>
<li>实际发生的行动路径：玩家1选L → 玩家2选R。</li>
<li>对应收益：从收益矩阵中，行L、列RL的单元格为（2,3），即玩家1收益为2，玩家2收益为3。</li>
</ul>
</li>
<li><h3 id="纳什均衡的定义"><a href="#纳什均衡的定义" class="headerlink" title="纳什均衡的定义"></a>纳什均衡的定义</h3><ul>
<li>纳什均衡要求：在给定其他玩家的策略下，没有任何一个玩家能通过单方面改变自己的策略而获得更高的收益。也就是说：</li>
<li>玩家1的策略必须是对玩家2策略的最优响应（best response）。</li>
<li>玩家2的策略必须是对玩家1策略的最优响应。<br>如果任何一个玩家有激励偏离当前策略，则该组合不是纳什均衡。</li>
</ul>
</li>
<li><h3 id="为什么（L-RL）不是纳什均衡？"><a href="#为什么（L-RL）不是纳什均衡？" class="headerlink" title="为什么（L, RL）不是纳什均衡？"></a>为什么（L, RL）不是纳什均衡？</h3><p>在策略组合（L, RL）下，玩家1有激励单方面改变策略。具体分析如下：</p>
</li>
</ul>
<ol>
<li><p><strong>给定玩家2的策略RL，玩家1的收益比较</strong>：</p>
<ul>
<li>如果玩家1保持选择L（当前策略）：<ul>
<li>玩家2的RL策略规定：当玩家1选L时，玩家2选R。</li>
<li>因此，收益为（2,3），玩家1获得收益<strong>2</strong>。</li>
</ul>
</li>
<li>如果玩家1单方面改为选择R：<ul>
<li>玩家2的RL策略规定：当玩家1选R时，玩家2选L（因为RL策略在玩家1选R时对应选L）。</li>
<li>因此，行动路径为（R, L），收益为（4,1）（从收益矩阵的行R、列RL单元格可得）。</li>
<li>玩家1获得收益<strong>4</strong>。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>玩家1的激励分析</strong>：</p>
<ul>
<li>玩家1的收益从<strong>2</strong>（选L）变为<strong>4</strong>（选R），收益增加（4 &gt; 2）。</li>
<li>因此，玩家1有严格激励（strict incentive）偏离策略L，改为选择R。因为收益更高，且这是单方面改变（玩家2的策略RL保持不变）。</li>
</ul>
</li>
<li><p><strong>玩家2的响应（虽不必要，但完整性分析）</strong>：</p>
<ul>
<li>在（L, RL）下，玩家2的收益为3（给定玩家1选L，玩家2选R）。</li>
<li>如果玩家1保持选L，玩家2改变策略（如改为LL、LR或RR）：<ul>
<li>例如，改为LL：当玩家1选L时，玩家2选L，收益为2（行L、列LL单元格为（3,2），玩家2收益2 &lt; 3）。</li>
<li>改为LR：当玩家1选L时，玩家2选L，收益为2（行L、列LR单元格为（3,2），玩家2收益2 &lt; 3）。</li>
<li>改为RR：当玩家1选L时，玩家2选R，收益为3（与当前相同）。</li>
<li>因此，玩家2无严格激励偏离RL（因为改变策略要么收益降低，要么不变），但这不是关键，因为玩家1的偏离已足够破坏均衡。</li>
</ul>
</li>
</ul>
</li>
</ol>
<ul>
<li><h3 id="为什么PPT中列出的（R-RL）是纳什均衡，而（L-RL）不是？"><a href="#为什么PPT中列出的（R-RL）是纳什均衡，而（L-RL）不是？" class="headerlink" title="为什么PPT中列出的（R, RL）是纳什均衡，而（L, RL）不是？"></a>为什么PPT中列出的（R, RL）是纳什均衡，而（L, RL）不是？</h3><ul>
<li><p><strong>（R, RL）是纳什均衡</strong>（如PPT所列）：</p>
<ul>
<li>玩家1选R，玩家2选RL（当玩家1选R时，玩家2选L）。</li>
<li>收益为（4,1）。</li>
<li>给定玩家2的RL策略，玩家1：选R收益4，选L收益2（因为如果玩家1选L，玩家2选R，收益2），4 &gt; 2，因此玩家1无激励偏离。</li>
<li>给定玩家1选R，玩家2：任何改变（如改为LL、LR或RR），在玩家1选R时收益均为1（无严格增加），因此玩家2无激励偏离。</li>
<li>所以（R, RL）满足纳什均衡条件（弱均衡）。</li>
</ul>
</li>
<li><p><strong>（L, RL）不是纳什均衡</strong>：</p>
<ul>
<li>如上所述，玩家1有严格激励偏离L到R，因此不满足“无玩家有激励偏离”的条件。</li>
</ul>
</li>
</ul>
</li>
<li><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li><strong>（L, RL）不是纳什均衡，因为玩家1可以通过单方面改变策略（从L到R）将收益从2提高到4。</strong> 给定玩家2的RL策略，玩家1选择L不是最优响应。</li>
<li>这体现了纳什均衡的核心要求：每个玩家的策略必须是对其他玩家策略的最优响应。在扩展式博弈中，这种分析也揭示了为什么有些策略组合在标准式中看似可行，但因动态不一致而被排除（如这里玩家1的偏离激励）。</li>
</ul>
</li>
</ul>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624003714695.png" alt="image-20250624003714695"></p>
<p>这张PPT探讨了进入博弈的均衡合理性，通过扩展式和标准式表示揭示了纳什均衡的局限性。以下是详细分析：</p>
<hr>
<h3 id="博弈结构与收益"><a href="#博弈结构与收益" class="headerlink" title="博弈结构与收益"></a>博弈结构与收益</h3><h4 id="扩展式表示（博弈树）："><a href="#扩展式表示（博弈树）：" class="headerlink" title="扩展式表示（博弈树）："></a>扩展式表示（博弈树）：</h4><ul>
<li><strong>玩家1（进入者）</strong>：先行动，选择：<ul>
<li><strong>In</strong>（进入市场）</li>
<li><strong>Out</strong>（不进入市场）→ 收益 (1,2)</li>
</ul>
</li>
<li><strong>玩家2（在位者）</strong>：观察到进入者行动后选择：<ul>
<li><strong>A</strong>（容纳）→ 收益 (2,1)</li>
<li><strong>F</strong>（斗争）→ 收益 (0,0)</li>
</ul>
</li>
</ul>
<h4 id="标准式表示（收益矩阵）："><a href="#标准式表示（收益矩阵）：" class="headerlink" title="标准式表示（收益矩阵）："></a>标准式表示（收益矩阵）：</h4><div class="table-container">
<table>
<thead>
<tr>
<th>进入者 \ 在位者</th>
<th>容纳 (A)</th>
<th>斗争 (F)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>进入 (In)</strong></td>
<td>(2,1)</td>
<td>(0,0)</td>
</tr>
<tr>
<td><strong>不进入 (Out)</strong></td>
<td>(1,2)</td>
<td>(1,2)</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="纳什均衡分析"><a href="#纳什均衡分析" class="headerlink" title="纳什均衡分析"></a>纳什均衡分析</h3><ol>
<li><p><strong>(In, A)</strong>：</p>
<ul>
<li>进入者收益：2（若改为Out，收益1&lt;2）→ <strong>无偏离激励</strong></li>
<li>在位者收益：1（若改为F，收益0&lt;1）→ <strong>无偏离激励</strong><br>✅ <strong>是纳什均衡</strong></li>
</ul>
</li>
<li><p><strong>(Out, F)</strong>：</p>
<ul>
<li>进入者收益：1（若改为In，在位者选F则收益0&lt;1）→ <strong>无偏离激励</strong></li>
<li>在位者收益：2（无论选A或F，收益均为2）→ <strong>无偏离激励</strong><br>✅ <strong>是纳什均衡（弱均衡）</strong></li>
</ul>
</li>
</ol>
<hr>
<h3 id="均衡合理性检验"><a href="#均衡合理性检验" class="headerlink" title="均衡合理性检验"></a>均衡合理性检验</h3><h4 id="为什么-Out-F-不合理？"><a href="#为什么-Out-F-不合理？" class="headerlink" title="为什么 (Out, F) 不合理？"></a>为什么 <strong>(Out, F)</strong> 不合理？</h4><p>尽管 <strong>(Out, F)</strong> 是纳什均衡，但它在动态博弈中<strong>不可信</strong>（缺乏子博弈精炼性）：</p>
<ol>
<li><p><strong>在位者的空威胁</strong>：</p>
<ul>
<li>在位者声称“若你进入，我会斗争（F）”，但若进入者真的选择 <strong>In</strong>：<ul>
<li>在位者选 <strong>A</strong> 收益为 <strong>1</strong></li>
<li>在位者选 <strong>F</strong> 收益为 <strong>0</strong>（更差）<br>→ 理性在位者实际会选择 <strong>A</strong>，而非 <strong>F</strong>。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>进入者的理性决策</strong>：</p>
<ul>
<li>进入者知道在位者的威胁不可信（一旦进入，在位者必选 <strong>A</strong>）。</li>
<li>因此进入者应选择 <strong>In</strong>（收益2 &gt; 选Out的收益1）。</li>
</ul>
</li>
</ol>
<h4 id="逆向归纳验证："><a href="#逆向归纳验证：" class="headerlink" title="逆向归纳验证："></a>逆向归纳验证：</h4><ol>
<li>若进入者选 <strong>In</strong>，在位者在子博弈中：<ul>
<li>选 <strong>A</strong> → 收益 <strong>1</strong></li>
<li>选 <strong>F</strong> → 收益 <strong>0</strong><br>→ 最优选择是 <strong>A</strong>。</li>
</ul>
</li>
<li>进入者预判：<ul>
<li>选 <strong>In</strong> → 收益 <strong>2</strong>（因在位者会选A）</li>
<li>选 <strong>Out</strong> → 收益 <strong>1</strong><br>→ 最优选择是 <strong>In</strong>。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><div class="table-container">
<table>
<thead>
<tr>
<th>均衡</th>
<th>合理性</th>
<th>原因</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>(In, A)</strong></td>
<td>✅ 合理</td>
<td>威胁可信，符合序贯理性</td>
</tr>
<tr>
<td><strong>(Out, F)</strong></td>
<td>❌ 不合理</td>
<td>依赖不可信的空威胁（动态不一致）</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>核心问题</strong>：纳什均衡在扩展式博弈中可能包含<strong>不可信威胁</strong>，需通过<strong>子博弈精炼纳什均衡</strong>（逆向归纳法）剔除不合理的均衡。</li>
<li><strong>本例唯一合理均衡</strong>：<strong>(In, A)</strong>，即进入者进入，在位者容纳。</li>
</ul>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624004513150.png" alt="image-20250624004513150"></p>
<ul>
<li>回顾匹配硬币博弈（matching pennies game）的两阶段扩展式版本。</li>
<li>在这个博弈中，存在两个<strong>真子博弈（proper subgames）</strong> 以及博弈本身（它也是一个子博弈），因此总共有<strong>三个子博弈</strong>。</li>
</ul>
<blockquote>
<p><strong>定义：</strong> 在一个（完美信息）博弈树中，<strong>每一个节点</strong>，连同从该节点到达之后所剩余的博弈部分，被称为一个<strong>子博弈（subgame）</strong>。<br>即，每一个<strong>非终端历史（non-terminal history）</strong> ( h ) 都对应一个子博弈。</p>
</blockquote>
<hr>
<p><strong>关键概念理解：</strong></p>
<ol>
<li><p><strong>子博弈 (Subgame):</strong></p>
<ul>
<li>子博弈是<strong>原博弈的一部分</strong>，它起始于博弈树中的<strong>某个单一决策节点</strong>（该节点代表一个特定的非终端历史 ( h^k )），并<strong>包含该节点之后的所有后续决策节点、行动分支和终端节点</strong>。</li>
<li>它本身必须构成一个<strong>完整的、独立的博弈</strong>，拥有明确的起点（该决策节点）、后续行动规则和最终收益。</li>
<li>子博弈继承了原博弈的所有规则（玩家、行动、信息、收益函数）。</li>
</ul>
</li>
<li><p><strong>非终端历史 (Non-terminal History):</strong></p>
<ul>
<li>指那些<strong>不是博弈最终结果</strong>的历史 ( h^k )（即 ( k &lt; K+1 )，其中 ( K+1 ) 是终端历史的索引）。</li>
<li>每个这样的历史 ( h^k ) 都标志着博弈进行到了一个<strong>尚未结束的决策点</strong>。</li>
<li><strong>关键联系：</strong> 每个非终端历史 ( h^k ) 都<strong>唯一确定了一个子博弈的起始点</strong>。这个子博弈就是从历史 ( h^k ) 所对应的那个决策节点开始的剩余游戏部分。</li>
</ul>
</li>
<li><p><strong>子博弈的数量:</strong></p>
<ul>
<li>在<strong>完美信息（perfect information）</strong> 博弈（即每个信息集只包含一个节点）中，子博弈的数量等于<strong>非终端决策节点的数量</strong>。</li>
<li>示例中提到匹配硬币两阶段博弈有<strong>三个子博弈</strong>：<ul>
<li><strong>子博弈 1：</strong> 起始于<strong>玩家1的第一个决策节点</strong>（对应初始历史 ( h^0 = \emptyset )）。<strong>这就是整个博弈本身。</strong></li>
<li><strong>子博弈 2：</strong> 起始于<strong>玩家2在玩家1选择“正面”之后的决策节点</strong>（对应历史 ( h^1 = (Heads) )）。</li>
<li><strong>子博弈 3：</strong> 起始于<strong>玩家2在玩家1选择“反面”之后的决策节点</strong>（对应历史 ( h^1 = (Tails) )）。</li>
</ul>
</li>
<li>后两个是<strong>真子博弈（proper subgames）</strong>，因为它们严格包含在原博弈之内且不等于原博弈。</li>
</ul>
</li>
<li><p><strong>为什么子博弈概念重要？</strong></p>
<ul>
<li><strong>均衡精炼：</strong> 子博弈概念是定义<strong>子博弈精炼纳什均衡（Subgame Perfect Nash Equilibrium, SPNE）</strong> 的核心。SPNE 要求均衡策略不仅在原博弈上构成纳什均衡，而且在<strong>每一个可能的子博弈</strong>上也构成纳什均衡。这旨在剔除那些依赖<strong>不可信威胁或承诺</strong>的纳什均衡（例如之前进入博弈中的 <code>(Out, F)</code> 均衡）。</li>
<li><strong>分析工具：</strong> 子博弈结构使得我们可以使用<strong>逆向归纳法（backward induction）</strong> 来求解完美信息有限博弈的 SPNE。我们从最小的子博弈（最接近终点的）开始求解，将其解（收益或最优行动）代入其父节点，再逐步倒推回根节点。</li>
<li><strong>模块化分析：</strong> 允许将大型复杂博弈分解为更小的、可管理的子部分进行分析。</li>
</ul>
</li>
<li><p><strong>子博弈求解方法解析</strong></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/199055190">https://zhuanlan.zhihu.com/p/199055190</a></p>
</li>
</ol>
<h2 id="2-3-势博弈"><a href="#2-3-势博弈" class="headerlink" title="2.3 势博弈"></a>2.3 势博弈</h2><p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624194410787.png" alt="image-20250624194410787"></p>
<ul>
<li><strong>请注意，对于所有厂商 i 和所有 q−i&gt;0 的情况：</strong><ul>
<li>ui(qi,q−i)−ui(qi′,q−i)&gt;0 <strong>当且仅当 (iff)</strong> Φ(qi,q−i)−Φ(qi′,q−i)&gt;0 ，此条件对于所有 qi,qi′&gt;0 均成立。</li>
<li>说明：<ul>
<li>这条关键的陈述指出，在其他厂商产量不变的情况下，厂商 i 从产量 qi′ 转换到 qi 能否增加自身利润，与这个转换能否增加函数 Φ 的值，两者是完全等价的。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>这张PPT的核心思想是为古诺竞争模型引入一个称为<strong>“势函数”(Potential Function)</strong> 的概念，并证明此模型是一个<strong>“势博弈”(Potential Game)</strong>。</p>
<ol>
<li><p>什么是古诺竞争？</p>
<p>这是一个经典的经济学模型，描述了在一个寡占市场中，几家厂商如何进行产量竞争。每家厂商都假设其竞争对手的产量是固定的，然后选择能让自己利润最大化的产量。当所有厂商都达到一个状态，即没有任何一家厂商可以单方面改变产量来增加自己的利润时，市场就达到了纳什均衡 (Nash Equilibrium)。</p>
</li>
<li><p>为什么要引入奇怪的函数 Φ？</p>
<p>支付函数 ui 非常直观，它就是厂商 i 的利润。然而，函数 Φ 看起来很抽象，它将“所有厂商产量的乘积”与“单位利润”相乘，并没有直接的经济学意义。</p>
<p>它的真正价值在于其数学性质。投影片的最后一条结论揭示了这个性质：任何单一厂商 i 调整策略 (产量 qi) 所带来的自身利润变化方向，都和函数 Φ 的变化方向完全一致。</p>
</li>
<li><p>这意味着什么？(核心理解)</p>
<p>这意味着，原本一个复杂的多人博弈问题（每个厂商都在最大化自己的 ui），可以被转化为一个相对简单的<strong>“寻找单一函数 Φ 最大值”</strong>的问题。</p>
<ul>
<li><strong>简化分析</strong>：我们不再需要同时追踪 I 个不同厂商的利润函数，只需要分析一个共同的“势函数”Φ 即可。</li>
<li><strong>保证均衡存在</strong>：在势博弈中，势函数 Φ 的局部最大值点对应着该博弈的纳什均衡。因为一个有界的连续函数必然存在最大值，这就为证明古诺均衡的存在性提供了一条优雅的路径。</li>
<li><strong>收敛性</strong>：势博弈还有一个重要特性，即如果玩家们轮流进行“最优反应”（每次都选择当下能让自己利润最大化的策略），这个过程最终一定会收敛到一个纳什均衡。函数 Φ 就像一个山坡，玩家们的每一步调整都像是在爬坡，最终必然会走到一个山顶（均衡点）。</li>
</ul>
</li>
</ol>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624200511139.png" alt="image-20250624200511139"></p>
<p>这两张PPT分别介绍了<strong>序数势博弈</strong>和<strong>精确势博弈</strong>。</p>
<ol>
<li><p>与“序数势”的核心区别</p>
<p>理解“精确势”的关键在于将其与上一张幻灯片中的“序数势”进行对比：</p>
<ul>
<li><strong>序数势 (Ordinal Potential)</strong>：只要求参与者收益变化的<strong>方向</strong>与势函数变化的<strong>方向</strong>一致。<ul>
<li>通俗地说：“只要我换策略能多赚钱，<code>Φ</code> 的值就一定会变大。” 它不关心你多赚了1块钱还是100块钱。</li>
</ul>
</li>
<li><strong>精确势 (Exact Potential)</strong>：要求参与者收益变化的<strong>确切数值</strong>与势函数变化的<strong>确切数值</strong>完全相等。<ul>
<li>通俗地说：“如果我换策略能多赚10块钱，<code>Φ</code> 的值就也必须不多不少，正好增加10。”</li>
</ul>
</li>
</ul>
</li>
<li><p>更严格的条件</p>
<p>显然，“精确势”是一个比“序数势”严格得多的条件。如果一个博弈是精确势博弈，那么它必然也是一个序数势博弈（因为如果两个改变量的数值相等，它们的正负号必然相同）。但反过来不成立，很多序数势博弈并不能满足精确势的苛刻条件。</p>
</li>
<li><p>指正幻灯片中的笔误</p>
<p>需要指出，幻灯片的最后一行存在一个明显的笔误。它写着 “G is called an exact potential game if it admits an ordinal potential.”（如果一个博弈拥有一个序数势，它就被称为一个精确势博弈）。这在逻辑上是错误的。</p>
<ul>
<li><strong>正确表述应为</strong>：“如果一个博弈 G 拥有一个<strong>精确势函数 (exact potential)</strong>，那么它就被称为一个<strong>精确势博弈</strong>。”</li>
</ul>
</li>
</ol>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624201207684.png" alt="image-20250624201207684"></p>
<p>这张幻灯片通过一个具体的2x2矩阵博弈（囚徒困境的一个变体）的例子，非常直观地展示了“势函数”是如何运作的。这张幻灯片的核心目的是让我们<strong>验证</strong>所给出的矩阵 <code>P</code> 是否真的是博弈 <code>G</code> 的一个势函数。</p>
<p>我们可以通过检验定义来验证。让我们看看当某个参与者单方面改变策略时，他个人收益的变化量是否与势函数 <code>P</code> 的变化量相匹配。</p>
<p><strong>1. 验证过程</strong></p>
<p>我们分别检查行参与者（玩家1）和列参与者（玩家2）的决策。</p>
<ul>
<li><p><strong>检验玩家1（行选择者）</strong></p>
<ul>
<li><p>当玩家2选择“左”时</p>
<p>：玩家1在“上”（收益1）和“下”（收益0）之间选择。</p>
<ul>
<li>收益变化: u1(上, 左)−u1(下, 左)=1−0=1</li>
<li>势函数变化: P(上, 左)−P(下, 左)=4−3=1</li>
<li>两者<strong>完全相等</strong>。</li>
</ul>
</li>
<li><p>当玩家2选择“右”时</p>
<p>：玩家1在“上”（收益9）和“下”（收益6）之间选择。</p>
<ul>
<li>收益变化: u1(上, 右)−u1(下, 右)=9−6=3</li>
<li>势函数变化: P(上, 右)−P(下, 右)=3−0=3</li>
<li>两者<strong>完全相等</strong>。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>检验玩家2（列选择者）</strong></p>
<ul>
<li><p>当玩家1选择“上”时</p>
<p>：玩家2在“左”（收益1）和“右”（收益0）之间选择。</p>
<ul>
<li>收益变化: u2(上, 左)−u2(上, 右)=1−0=1</li>
<li>势函数变化: P(上, 左)−P(上, 右)=4−3=1</li>
<li>两者<strong>完全相等</strong>。</li>
</ul>
</li>
<li><p>当玩家1选择“下”时</p>
<p>：玩家2在“左”（收益9）和“右”（收益6）之间选择。</p>
<ul>
<li>收益变化: u2(下, 左)−u2(下, 右)=9−6=3</li>
<li>势函数变化: P(下, 左)−P(下, 右)=3−0=3</li>
<li>两者<strong>完全相等</strong>。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>2. 结论</strong></p>
<p>由于<strong>任何</strong>参与者单方面改变策略所带来的收益变化，都<strong>精确地等于</strong>势函数 <code>P</code> 中对应数值的变化，因此，矩阵 <code>P</code> 是博弈 <code>G</code> 的一个<strong>精确势函数 (exact potential function)</strong>。这个博弈 <code>G</code> 是一个<strong>精确势博弈</strong>。</p>
<p><strong>3. 势函数的威力：寻找纳什均衡</strong></p>
<p>这个例子的美妙之处在于，它展示了势函数如何简化均衡的寻找过程。寻找博弈 <code>G</code> 的纯策略纳什均衡，现在等价于寻找势函数矩阵 <code>P</code> 的“稳定点”。</p>
<p>观察势函数矩阵 P：</p>
<p>P=(4330)</p>
<p>矩阵中的最大值是4，位于（上，左）位置。让我们看看这个点是否稳定：</p>
<ul>
<li>如果从 <code>P=4</code>（上，左）出发，玩家1单方面移到“下”，<code>P</code>的值会从4变成3，他不会移动。</li>
<li>如果从 <code>P=4</code>（上，左）出发，玩家2单方面移到“右”，<code>P</code>的值会从4变成3，他也不会移动。</li>
</ul>
<p>因为没有任何一方有动机从（上，左）这个位置离开，所以它是一个纳什均衡。这对应于原博弈 <code>G</code> 中的（1,1）这个结果。这个例子清晰地表明，<strong>势函数的局部最大值（在此例中也是全局最大值）对应着博弈的一个纳什均衡</strong>。这正是势函数在博弈分析中的核心价值。</p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624201541524.png" alt="image-20250624201541524"></p>
<p>有限序数势博弈的结果是有限的，势函数会为每一个博弈策略赋予一个值，因此必然存在一个最大值，这个最大值就是纳什均衡点。</p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624202953517.png" alt="image-20250624202953517"></p>
<h4 id="1-核心结论"><a href="#1-核心结论" class="headerlink" title="1. 核心结论"></a>1. 核心结论</h4><p>结论是最后一条：当厂商 <code>i</code> 单方面将自己的产量从 <code>q&#39;_i</code> 变为 <code>q_i</code> 时，他<strong>个人利润的变化量</strong>，与我们构造的那个看起来很复杂的函数 <code>Φ*</code> 的<strong>变化量是完全相等的</strong>。这正是“精确势函数”的定义。</p>
<h4 id="2-数学证明（为什么这个结论成立？）"><a href="#2-数学证明（为什么这个结论成立？）" class="headerlink" title="2. 数学证明（为什么这个结论成立？）"></a>2. 数学证明（为什么这个结论成立？）</h4><p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624203148105.png" alt="image-20250624203148105"></p>
<p>最后比较两个Δ即可</p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624204837963.png" alt="image-20250624204837963"></p>
<p><strong>标题：有限序数势博弈中的简单动态 (Simple Dynamics in Finite Ordinal Potential Games)</strong></p>
<p>定义 (Definition)</p>
<p>策略空间 S 中的一条路径 (path) 是一个策略向量序列 (s0,s1,⋯)，其中任意两个连续的策略只在一个坐标上不同（即，每一次恰好只有一名参与者改变了他的策略）。</p>
<p>一条<strong>改进路径 (improvement path)</strong> 是一条路径 (s0,s1,⋯)，它满足：</p>
<ul>
<li>uik(sk)&lt;uik(sk+1)，其中策略 sk 和 sk+1 在第 ik 个坐标上不同。换句话说，对于那个改变了策略的参与者来说，他的收益得到了改善。</li>
</ul>
<p>一条改进路径可以被认为是<strong>“短视参与者” (myopic players)</strong> 动态生成的结果。</p>
<hr>
<p>这张幻灯片将我们的视角从静态的“均衡分析”转向了动态的“演化过程分析”。它为我们描述“一个博弈是如何一步步演变的”提供了正式的语言。这对于理解系统如何达到稳定状态至关重要。</p>
<h4 id="1-“路径”与“改进路径”的通俗解释"><a href="#1-“路径”与“改进路径”的通俗解释" class="headerlink" title="1. “路径”与“改进路径”的通俗解释"></a>1. “路径”与“改进路径”的通俗解释</h4><ul>
<li><strong>路径 (Path)</strong>：你可以把它想象成一盘棋的“棋谱”或一个游戏的“回合记录”。它记录了博弈状态是如何一步步变化的，其核心规则是<strong>“一次只动一个”</strong>。在 sk 这一步，只有一名参与者会改变他的行动，从而进入 sk+1 状态。这是一种分析动态过程的合理简化。</li>
<li><strong>改进路径 (Improvement Path)</strong>：这是一种特殊的、由理性驱动的路径。它不仅记录了系统的演变，还说明了<strong>为什么</strong>会这样演变。每一步的发生，都是因为某个参与者发现“如果我单方面改变策略，我的收益会立刻增加”，于是他就这么做了。这完美地描述了那些只顾眼前利益的参与者的行为。</li>
</ul>
<h4 id="2-“短视参与者”-Myopic-Players-的概念"><a href="#2-“短视参与者”-Myopic-Players-的概念" class="headerlink" title="2. “短视参与者” (Myopic Players) 的概念"></a>2. “短视参与者” (Myopic Players) 的概念</h4><p>这是理解“改进路径”背后行为动机的关键。</p>
<ul>
<li><strong>“短视”</strong>意味着参与者们并不深谋远虑，他们不会去预测对手的对手的反应。</li>
<li>他们的决策逻辑非常简单：“在当前这个局面下，我有没有一个别的选择能让我马上赚得更多？”</li>
<li>如果答案是“有”，那么某个参与者就会采取行动，从而推动整个系统沿着“改进路径”向前走一步。这个过程可以被看作是一系列“更优反应 (better response)”的链式反应。</li>
</ul>
<h4 id="3-为什么要在势博弈中讨论这个？（核心洞见）"><a href="#3-为什么要在势博弈中讨论这个？（核心洞见）" class="headerlink" title="3. 为什么要在势博弈中讨论这个？（核心洞见）"></a>3. 为什么要在势博弈中讨论这个？（核心洞见）</h4><p>定义这些概念的最终目的，是为了引出势博弈最强大的性质之一：<strong>动态收敛性</strong>。</p>
<p>我们可以设想一下：</p>
<ol>
<li><strong>在普通博弈中</strong>：一条“改进路径”可能会没完没了地走下去，甚至可能陷入一个循环（比如A动、B动、C动，结果又回到了A动之前的局面），永远无法达到一个稳定的纳什均衡。</li>
<li><strong>但在势博弈中</strong>：奇迹发生了。我们知道，在势博弈中，只要某个参与者 <code>i</code> 的收益 <code>uᵢ</code> 增加了，全局的势函数 <code>Φ</code> 的值也<strong>必须增加</strong>。</li>
<li>因此，在势博弈里，<strong>每一条“改进路径”都必然是一条“势函数 <code>Φ</code> 值不断增加的路径”</strong>。</li>
<li>在一个<strong>有限</strong>博弈中，势函数 <code>Φ</code> 的可能取值是有限的，它必然有一个最大值。<code>Φ</code> 的值不可能无限地增加下去。</li>
</ol>
<p><strong>结论</strong>：在有限势博弈中，任何“改进路径”走不了几步就<strong>必然会停止</strong>。它会在哪里停下呢？它会停在一个任何人都无法再单方面改善自己收益的地方——而这个地方，根据定义，<strong>就是一个纯策略纳什均衡</strong>。</p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624205950398.png" alt="image-20250624205950398"></p>
<p><strong>标题：有限精确势博弈的特征描述 (Characterization of Finite Exact Potential Games)</strong></p>
<ul>
<li><p>对于一条有限路径 γ=(s0,s1,…,sN)，我们令：</p>
<p>I(γ)=i=1∑N(umi(si)−umi(si−1))</p>
<p>其中，mi 指代在路径的第 i 步改变其策略的参与者。</p>
</li>
<li><p>如果 s0=sN，则路径 γ=(s0,…,sN) 是<strong>闭合的 (closed)</strong>。如果在此外对于每一个 0≤l&lt;k≤N−1 都有 sl\\=sk，那么它就是一条<strong>简单闭合路径 (simple closed path)</strong>。</p>
</li>
</ul>
<p>定理 (Theorem)</p>
<p>一个博弈 G 是一个精确势博弈，当且仅当对于所有有限简单闭合路径 γ，都有 I(γ) = 0。此外，只检验长度为4的简单闭合路径就足够了。</p>
<hr>
<p>这个数学定理，它为我们提供了一个<strong>“试金石”</strong>，用来检验任何一个有限博弈到底是不是一个“精确势博弈”，而无需我们去猜测或构造那个势函数 <code>Φ</code>。</p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624211206447.png" alt="image-20250624211206447"></p>
<p><strong>标题：网络拥堵博弈 (Network Congestion Games)</strong></p>
<ul>
<li>一个包含 n 个用户的<strong>有向图 (directed graph)</strong> G=(V,E)，</li>
<li>图 G 中的每一条<strong>边 (edge)</strong> e 都有一个<strong>延迟函数 (delay function)</strong> fₑ，</li>
<li>用户 i 的<strong>策略 (Strategy)</strong> 是选择一条从<strong>起点 (source)</strong> sᵢ 到<strong>终点 (destination)</strong> tᵢ 的<strong>路径 (path)</strong> Aᵢ，</li>
<li>一条路径的延迟是该路径上所有边的延迟之和，</li>
<li>每个用户都想通过选择最佳路径来<strong>最小化 (minimize)</strong> 其自身的延迟。</li>
</ul>
<hr>
<p>这张幻灯片介绍了一类在现实世界中应用极其广泛的博弈模型——<strong>网络拥堵博弈</strong>。这是理解交通堵塞、互联网数据包路由、供应链物流等众多问题的核心理论框架。</p>
<h4 id="1-博弈的核心要素"><a href="#1-博弈的核心要素" class="headerlink" title="1. 博弈的核心要素"></a>1. 博弈的核心要素</h4><p>这个模型非常直观，它完美地捕捉了“拥堵”现象的本质：</p>
<ul>
<li><strong>参与者 (Players)</strong>：n 个用户（比如，n 位司机）。</li>
<li><strong>策略 (Strategies)</strong>：每个司机可以选择的路线（例如，从家 <code>sᵢ</code> 到公司 <code>tᵢ</code> 的不同路径）。</li>
<li><strong>成本 (Cost)</strong>：每个司机在路上花费的时间，即“延迟”。</li>
</ul>
<p>这个博弈最关键、最有趣的地方在于成本（延迟）的计算方式。一条路（边 <code>e</code>）的延迟<code>fₑ</code><strong>不是一个固定的数，而是一个函数</strong>。它的值取决于有多少人同时在使用这条路。</p>
<h4 id="2-博弈的内在冲突"><a href="#2-博弈的内在冲突" class="headerlink" title="2. 博弈的内在冲突"></a>2. 博弈的内在冲突</h4><p>每个司机都想自私地选择一条“最快”的路。但正是因为所有人都这么想，才导致了问题的产生：</p>
<ul>
<li>如果有一条近路看起来最快，所有司机可能都会涌向这条路。</li>
<li>结果，这条路变得极度拥堵，它的延迟函数 <code>fₑ(x)</code> 因为 <code>x</code>（用户数）变得很大而给出一个非常高的延迟值。</li>
<li>这条所谓的“最快”的路，实际上可能比其他更长但无人问津的路要慢得多。</li>
</ul>
<p><strong>每个人的最优选择都依赖于其他所有人的选择</strong>。我的决策影响你的成本，你的决策也影响我的成本。这就是博弈的核心所在。</p>
<h4 id="3-与“势博弈”的深刻联系（核心洞见）"><a href="#3-与“势博弈”的深刻联系（核心洞见）" class="headerlink" title="3. 与“势博弈”的深刻联系（核心洞见）"></a>3. 与“势博弈”的深刻联系（核心洞见）</h4><p>这个模型最惊人的特性是，<strong>网络拥堵博弈是“精确势博弈”的经典范例</strong>。这是由学者 Rosenthal 在1973年发现的里程碑式成果。</p>
<p>存在一个全局的势函数 Φ（通常称为罗森塔尔势函数），它能够完美地刻画整个系统的动态。这个势函数的定义方式非常巧妙：</p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624211304691.png" alt="image-20250624211304691"></p>
<p>其中，xe 是当前选择了边 e 的总用户数。这个公式的含义是，把网络中每一条边的“从第1个用户到第 xe 个用户的延迟依次加起来”，然后再把所有边的这个值汇总。</p>
<p>可以被严格证明：<strong>当任何一个用户 i 单方面改变自己的路径时，他个人延迟的变化量，与这个全局势函数 <code>Φ</code> 的变化量是完全相等的！</strong></p>
<h4 id="4-重要推论"><a href="#4-重要推论" class="headerlink" title="4. 重要推论"></a>4. 重要推论</h4><p>既然拥堵博弈是精确势博弈，那么我们之前讨论过的所有优美性质就都可以应用在这里：</p>
<ol>
<li><strong>均衡必然存在</strong>：任何一个网络拥堵博弈都<strong>至少存在一个纯策略纳什均衡</strong>。在交通模型里，这被称为“瓦德罗普均衡 (Wardrop equilibrium)”。这意味着，总会存在一种稳定的交通分配格局，在这种格局下，没有单个司机可以通过单方面改变路线来缩短自己的通勤时间。</li>
<li><strong>动态必然收敛</strong>：如果司机们是“短视的”（例如，每天根据昨天的路况尝试寻找更快的路），这个不断调整、学习的过程<strong>必然会收敛到一个稳定的均衡状态</strong>，而不会永久地混乱或振荡下去。</li>
</ol>
<h2 id="3-1-零和博弈及纳什均衡计算"><a href="#3-1-零和博弈及纳什均衡计算" class="headerlink" title="3.1 零和博弈及纳什均衡计算"></a>3.1 零和博弈及纳什均衡计算</h2><p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624232921182.png" alt="image-20250624232921182"></p>
<p><strong>练习：斯塔克尔伯格双寡头模型 (Exercise: The Stackelberg model of Duopoly)</strong></p>
<ul>
<li>斯塔克尔伯格双寡头模型 (1934):<ul>
<li>一个参与者，被称为主导者或<strong>领导者 (leader)</strong>，首先行动，并且该参与者的选择结果在另一位参与者（<strong>跟随者, follower</strong>）做出选择之前就已告知对方。</li>
<li>例如，通用汽车公司（General Motors）在美国历史上的某些时期，就曾在汽车行业中扮演了如此主导的角色。</li>
</ul>
</li>
</ul>
<p><em>（来源：Game Theory, Second Edition, 2014. Thomas S. Ferguson）</em></p>
<p><strong>标题：练习：斯塔克尔伯格双寡头模型</strong></p>
<ul>
<li>厂商1首先选择一个生产数量 q1，其单位成本为 c。</li>
<li>这个数量会被告知厂商2，然后厂商2再选择自己的生产数量 q2，其单位成本同样为 c。</li>
<li>之后，市场的单位价格 P 由以下公式决定： P(Q)={a−Q0if 0≤Q≤aif Q&gt;a=(a−Q)+ 其中 Q=q1+q2，并且 a 是一个常数。</li>
<li>参与者们获得如下支付（利润）： u1(q1,q2)=q1P(q1+q2)−cq1=q1(a−q1−q2)+−cq1 u2(q1,q2)=q2P(q1+q2)−cq2=q2(a−q1−q2)+−cq2 其中单位成本 c&lt;a。</li>
</ul>
<p>斯塔克尔伯格模型是产业组织理论中一个基石性的模型。它与我们之前讨论的古诺模型（Cournot model）最大的不同在于，它将“同时行动”改为了<strong>“序贯行动” (Sequential Moves)</strong>，从而引入了<strong>先手优势 (First-mover Advantage)</strong> 的概念。</p>
<h4 id="1-与古诺模型的根本区别"><a href="#1-与古诺模型的根本区别" class="headerlink" title="1. 与古诺模型的根本区别"></a>1. 与古诺模型的根本区别</h4><ul>
<li><strong>古诺模型</strong>：两家厂商<strong>同时</strong>决定产量，谁也不知道对方会生产多少，是一个静态的、猜对手心思的博弈。</li>
<li><strong>斯塔克尔伯格模型</strong>：两家厂商有明确的行动顺序。一家是“领导者”（先动），另一家是“跟随者”（后动）。领导者率先公布自己的产量，跟随者在<strong>观察到</strong>领导者的产量后再决定自己的最优产量。这是一个动态博弈。</li>
</ul>
<h4 id="2-如何求解？——-逆向归纳法-Backward-Induction"><a href="#2-如何求解？——-逆向归纳法-Backward-Induction" class="headerlink" title="2. 如何求解？—— 逆向归纳法 (Backward Induction)"></a>2. 如何求解？—— 逆向归纳法 (Backward Induction)</h4><p>对于这种有先后顺序的博弈，标准的解法是“逆向归纳”，即从后往前推。</p>
<p><strong>第一步：求解跟随者（厂商2）的问题</strong></p>
<p>我们先站在厂商2的角度。此时，厂商1的产量 q1 已经确定，是一个已知的数字。厂商2的目标是选择自己的产量 q2 来最大化自身利润 u2。</p>
<p>q2max u2(q1,q2)=q2(a−q1−q2)−cq2</p>
<p>为了求最大值，我们对 q2 求导并令其为0：</p>
<p>∂q2∂u2=a−q1−2q2−c=0</p>
<p>解出 q2，我们就得到了厂商2的反应函数 (Reaction Function)：</p>
<p>q2∗(q1)=2a−c−q1</p>
<p>这个函数告诉我们：不论领导者厂商1生产多少 (q1)，跟随者厂商2的最优应对策略是什么。</p>
<p><strong>第二步：求解领导者（厂商1）的问题</strong></p>
<p>厂商1非常“聪明”，它完全知道厂商2会如何根据它的 q1 来做出反应。因此，厂商1在做决策时，会把厂商2的反应函数直接代入自己的利润公式中，以此来预测自己选择不同 q1 的最终后果。</p>
<p>厂商1的利润函数变为：</p>
<p>u1(q1)=q1(a−q1−q2∗(q1))−cq1=q1(a−q1−2a−c−q1)−cq1</p>
<p>化简括号内的部分：</p>
<p>u1(q1)=q1(2a−c−q1)−cq1=2aq1−cq1−q12−cq1</p>
<p>厂商1的目标是选择 q1 来最大化这个新的利润函数。我们对 q1 求导并令其为0：</p>
<p>∂q1∂u1=2a−c−2q1=0⟹a−c−2q1=0</p>
<p>解出领导者厂商1的最优产量：</p>
<p>q1∗=2a−c</p>
<p><strong>第三步：得出最终均衡结果</strong></p>
<p>将厂商1的最优产量代入厂商2的反应函数，得到厂商2的产量：</p>
<p>q2∗=2a−c−q1∗=2a−c−(a−c)/2=2(a−c)/2=4a−c</p>
<p><strong>斯塔克尔伯格均衡解为：(领导者产量 q1∗=2a−c， 跟随者产量 q2∗=4a−c)</strong></p>
<h4 id="3-先手优势"><a href="#3-先手优势" class="headerlink" title="3. 先手优势"></a>3. 先手优势</h4><ul>
<li>在斯塔克尔伯格均衡中，领导者产量(2a−c)是跟随者(4a−c)的两倍。</li>
<li>我们可以对比一下古诺均衡的结果：在古诺模型中，两家厂商的产量相同，均为 3a−c。</li>
<li>比较可知：q1Stackelberg(2a−c)&gt;qCournot(3a−c)&gt;q2Stackelberg(4a−c)。</li>
<li>这意味着，通过率先行动并承诺一个较高的产量，领导者可以有效地“挤压”跟随者的市场空间，迫使跟随者选择一个较低的产量，从而为自己攫取更高的市场份额和利润。这就是<strong>先手优势</strong>的体现。</li>
</ul>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250624233936853.png" alt="image-20250624233936853"></p>
<p><strong>零和博弈 (Zero-Sum Games)</strong></p>
<p>极小化极大定理 (Minimax Theorem) (约翰·冯·诺伊曼, 1928):</p>
<p>对于每一个具有有限个纯策略的两人零和博弈，都存在一个适用于各方参与者的混合策略和一个价值 V，使得：</p>
<ul>
<li>给定参与者2的策略，参与者1可能获得的最佳支付为 V。</li>
<li>给定参与者1的策略，参与者2可能获得的最佳支付为 -V。</li>
</ul>
<p>策略存在的部分是<strong>纳什定理的一个特例</strong>，也是其先驱。</p>
<p>这基本上是说，参与者1可以保证自己获得<strong>至少 V</strong> 的支付，而参与者2可以保证自己获得<strong>至少 -V</strong> 的支付。如果双方都采取最优策略，这恰好就是他们将得到的结果。</p>
<p>它之所以被称为“极小化极大 (minimax)”，是因为参与者是通过一种试图<strong>最小化 (minimize) 对手可能获得的最大 (maximum) 支付</strong>的策略来获得这个价值的。我们稍后会再回到这一点。</p>
<p><strong>定义</strong>：价值 V 被称为该博弈的<strong>价值 (value)</strong>（或回报、支付）。</p>
<p><strong>例如</strong>：石头剪刀布的价值是0；假设参与者2采取最优策略（以1/3的概率出每一种手势），参与者1能期望获得的最好结果是0的支付。</p>
<hr>
<p>这张幻灯片介绍了博弈论的奠基性概念之一——<strong>两人零和博弈</strong>，以及该领域第一个里程碑式的定理——冯·诺伊曼的<strong>极小化极大定理</strong>。</p>
<h4 id="1-什么是零和博弈？"><a href="#1-什么是零和博弈？" class="headerlink" title="1. 什么是零和博弈？"></a>1. 什么是零和博弈？</h4><p>首先，零和博弈指的是在一个博弈中，所有参与者的收益（或亏损）加起来永远等于零。这意味着，<strong>一方的所得，必然是另一方的所失</strong>。这是一个纯粹冲突、完全竞争的模型，没有任何合作共赢的可能。经典的例子包括：</p>
<ul>
<li><strong>棋类游戏</strong>：如象棋、围棋，一方赢就是另一方输。</li>
<li><strong>石头剪刀布</strong>：一方赢一分，另一方就输一分。</li>
<li><strong>竞技体育</strong>：大多数只有两方对阵的比赛。</li>
</ul>
<h4 id="2-“极小化极大”定理的通俗解释"><a href="#2-“极小化极大”定理的通俗解释" class="headerlink" title="2. “极小化极大”定理的通俗解释"></a>2. “极小化极大”定理的通俗解释</h4><p>这个定理解决了一个核心问题：在这样你死我活的纯冲突中，“理性”的策略是什么？冯·诺伊曼给出了一个天才的答案，其思考逻辑如下：</p>
<p><strong>从参与者1（P1）的角度（最大化最小值, Maximin）</strong>：</p>
<ol>
<li>P1必须假设P2是完全理性的，并且会尽一切努力损害P1的利益。</li>
<li>P1会思考：“对于我可能采取的每一个策略，P2都会用对我最不利的方式来回应。我先列出每一种策略下，我最坏会得到什么结果（我的<strong>最小</strong>收益）。”</li>
<li>“然后，在所有这些‘最坏结果’中，我选择那个能让我得到最好结果的策略。”</li>
<li>这个过程，就是<strong>最大化自己的最小保证收益 (Maximize a minimum payoff)</strong>，简称 <strong>Maximin</strong>。</li>
</ol>
<p><strong>从参与者2（P2）的角度（极小化极大, Minimax）</strong>：</p>
<ol>
<li>P2同样假设P1会尽全力损害自己。</li>
<li>P2会思考：“对于P1的每一个策略，我最坏会损失多少（即P1能获得的最大收益）？”</li>
<li>“然后，我选择一个策略，能让P1可能获得的最大收益变得最小。”</li>
<li>这个过程，就是<strong>最小化自己的最大可能损失 (Minimize a maximum loss)</strong>，简称 <strong>Minimax</strong>。</li>
</ol>
<p><strong>定理的“奇迹”</strong>：冯·诺伊曼证明，对于任何两人零和博弈，P1通过“最大化最小值”策略能保证得到的收益 <code>V</code>，与P2通过“极小化极大”策略能保证让P1得到的收益 <code>V</code>，是<strong>完全同一个数值</strong>！这个 <code>V</code> 就是该博弈的“价值”。</p>
<p>这意味着，这类纯冲突博弈存在一个绝对理性的、稳定的解。双方的最佳策略将会在这个点上交汇。</p>
<h4 id="3-混合策略的重要性"><a href="#3-混合策略的重要性" class="headerlink" title="3. 混合策略的重要性"></a>3. 混合策略的重要性</h4><p>这个定理的成立，往往需要<strong>混合策略</strong>的引入，即以一定的概率随机地选择不同的行动。</p>
<ul>
<li>以“石头剪刀布”为例，如果你只出“石头”（一个纯策略），对手会立刻发现并一直出“布”来打败你。</li>
<li>你唯一能保证自己不输的策略，就是完全随机地出招（石头、剪刀、布各1/3概率）。</li>
<li>当你的对手也采取这种最优的混合策略时，你期望的平均收益就是0。因此，这个博弈的价值 <code>V=0</code>。</li>
</ul>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250625000231647.png" alt="image-20250625000231647"></p>
<p><strong>计算纳什均衡：两人零和博弈 (Computing Nash Equilibria: 2-person, Zero-Sum Games)</strong></p>
<ul>
<li>这个博弈没有纯策略纳什均衡。</li>
<li>根据纳什定理，它必然拥有一个<strong>混合策略</strong>纳什均衡。</li>
<li>我们该如何找到它呢？</li>
</ul>
<p><em>(注：这是一个两人零和博弈)</em></p>
<hr>
<p>这张幻灯片提出了一个核心问题：对于一个没有纯策略均衡的博弈，我们如何具体计算出它的混合策略纳什均衡？</p>
<p>下面是详细的计算步骤：</p>
<h4 id="第一步：验证不存在纯策略纳什均衡"><a href="#第一步：验证不存在纯策略纳什均衡" class="headerlink" title="第一步：验证不存在纯策略纳什均衡"></a>第一步：验证不存在纯策略纳什均衡</h4><p>我们可以通过“划线法”或“最优反应法”来快速验证。</p>
<ol>
<li>如果参与者2选择“列1”，参与者1的最优选择是“行2”（因为收益 +3 &gt; -2）。</li>
<li>如果参与者2选择“列2”，参与者1的最优选择是“行1”（因为收益 +3 &gt; -4）。</li>
<li>如果参与者1选择“行1”，参与者2的最优选择是“列1”（因为收益 +2 &gt; -3）。</li>
<li>如果参与者1选择“行2”，参与者2的最优选择是“列2”（因为收益 +4 &gt; -3）。</li>
</ol>
<p>我们发现，没有任何一个单元格是双方共同的最优选择，因此该博弈确实没有纯策略纳什均衡。</p>
<h4 id="第二步：设定混合策略"><a href="#第二步：设定混合策略" class="headerlink" title="第二步：设定混合策略"></a>第二步：设定混合策略</h4><p>在混合策略均衡中，核心思想是<strong>“无差异原则” (Indifference Principle)</strong>：每个参与者选择自己的混合策略（即概率），目的是让<strong>对方</strong>在自己的几个纯策略选择之间感到<strong>无所谓/无差异</strong>（即期望收益完全相等）。</p>
<ul>
<li>我们假设<strong>参与者1</strong>以概率 <strong>p</strong> 选择“行1”，以概率 <strong>(1-p)</strong> 选择“行2”。</li>
<li>我们假设<strong>参与者2</strong>以概率 <strong>q</strong> 选择“列1”，以概率 <strong>(1-q)</strong> 选择“列2”。</li>
</ul>
<h4 id="第三步：计算参与者1的混合策略-p"><a href="#第三步：计算参与者1的混合策略-p" class="headerlink" title="第三步：计算参与者1的混合策略 p"></a>第三步：计算参与者1的混合策略 p</h4><p>为了让<strong>参与者2</strong>感到无差异，参与者2选择“列1”的期望收益必须等于他选择“列2”的期望收益。</p>
<ul>
<li>参与者2选择“列1”的期望收益 E(列1) = p<em>(+2)+(1−p)</em>(−3)</li>
<li>参与者2选择“列2”的期望收益 E(列2) = p<em>(−3)+(1−p)</em>(+4)</li>
</ul>
<p>令 E(列1) = E(列2):</p>
<p>可得p=7/12</p>
<p>所以，参与者1的最优策略是：以 7/12 的概率选择“行1”，以 5/12 的概率选择“行2”。</p>
<h4 id="第四步：计算参与者2的混合策略-q"><a href="#第四步：计算参与者2的混合策略-q" class="headerlink" title="第四步：计算参与者2的混合策略 q"></a>第四步：计算参与者2的混合策略 q</h4><p>同样，为了让<strong>参与者1</strong>感到无差异，参与者1选择“行1”的期望收益必须等于他选择“行2”的期望收益。</p>
<ul>
<li>参与者1选择“行1”的期望收益 E(行1) = q<em>(−2)+(1−q)</em>(+3)</li>
<li>参与者1选择“行2”的期望收益 E(行2) = q<em>(+3)+(1−q)</em>(−4)</li>
</ul>
<p>令 E(行1) = E(行2):</p>
<p>q=7/12</p>
<p>所以，参与者2的最优策略是：以 7/12 的概率选择“列1”，以 5/12 的概率选择“列2”。</p>
<h4 id="第五步：结论与博弈的价值"><a href="#第五步：结论与博弈的价值" class="headerlink" title="第五步：结论与博弈的价值"></a>第五步：结论与博弈的价值</h4><ol>
<li><p><strong>混合策略纳什均衡</strong>：该博弈的唯一纳什均衡是：参与者1采取混合策略 ( 7/12 ,5/12)，参与者2采取混合策略 (7/12 ,5/12)。</p>
</li>
<li><p>博弈的价值 (Value of the Game)：在均衡状态下，参与者1的期望收益是多少？我们可以把 q = 7/12 代入 E(行1) 的公式中计算：</p>
<p>E(P1) = 1/12</p>
<p>因此，这个博弈对参与者1的<strong>价值是 +1/12</strong>，对参与者2的价值是 -1/12。这意味着，如果双方都采取最优的随机策略，长期来看，参与者1平均每次能赢1/12。</p>
</li>
</ol>
<p><strong>为什么目的是让对方在自己的几个纯策略选择之间感到无所谓/无差异？</strong></p>
<p>简单来说，<strong>让对方“无差异”并不是我们的最终目的，而是我们为了实现自身利益最大化，所必须达到的一个“结果”或“条件”。</strong></p>
<p>这是一种非常高明的策略思想，我们可以从三个层面来理解它：</p>
<hr>
<h3 id="1-核心思想：消除对方的确定性最优解"><a href="#1-核心思想：消除对方的确定性最优解" class="headerlink" title="1. 核心思想：消除对方的确定性最优解"></a>1. 核心思想：消除对方的确定性最优解</h3><p>在一个博弈中，如果你采取的策略让你的对手有一个明确的、唯一的“最优选择”，那你就输了一半。因为：</p>
<ol>
<li>一个理性的对手，一定会采取那个对他来说最优的选择。</li>
<li>这样一来，对手的行动就变得<strong>完全可以预测</strong>了。</li>
<li>一旦对手的行动是可预测的，你就可以反过来调整自己的策略，去专门“克制”他那个可预测的行动，从而让自己获利更多。</li>
<li>但这就产生了一个矛盾：如果你能调整策略获利更多，说明你最初的策略就不是最优的。</li>
</ol>
<p>这个矛盾循环说明，一个稳定的均衡状态，不应该让任何一方有“唯一的、确定的”最优解。而要做到这一点，你唯一的方法就是调整自己的策略组合（即概率 <code>p</code>），直到你的对手觉得“选A或选B，反正期望收益都一样，我无所谓了”。</p>
<p><strong>当你让对手“无所谓”时，你就消除了他行动的确定性，他才不得不也用一种随机的方式来对抗你。这才是对自己最有利的局面。</strong></p>
<hr>
<h3 id="2-反向思考：如果不让对方无差异会怎样？"><a href="#2-反向思考：如果不让对方无差异会怎样？" class="headerlink" title="2. 反向思考：如果不让对方无差异会怎样？"></a>2. 反向思考：如果不让对方无差异会怎样？</h3><p>我们用上一张幻灯片的例子来思考：</p>
<ul>
<li>你的策略是：以概率 <code>p</code> 出“行1”，概率 <code>(1-p)</code> 出“行2”。</li>
<li>假设你选择的 <code>p</code> 没有让对手无差异，而是让对手觉得<strong>“出‘列1’比出‘列2’的期望收益更高”</strong>。</li>
</ul>
<p>接下来会发生什么？</p>
<ol>
<li><strong>对手的反应</strong>：理性的对手会想：“既然出‘列1’更好，我为什么还要费事去随机出‘列2’呢？” 于是，他会100%地出“列1”。他的策略就不再是混合策略了。</li>
<li><strong>你的反应</strong>：你看到对手只会出“列1”，那你还会坚持你原来的概率 <code>p</code> 吗？当然不会！你会看支付矩阵的“列1”那一栏，发现你出“行1”收益是-2，出“行2”收益是+3。你显然会100%地出“行2”来应对他。</li>
</ol>
<p><strong>结论</strong>：你最初那个让对手“有差异”的策略 <code>p</code>，最终导致了你自己也想改变策略。这就说明，那个初始状态<strong>根本不稳定</strong>，因此<strong>不是纳什均衡</strong>。</p>
<p>唯一的稳定状态，就是你选择的概率 <code>p</code>，正好让对手觉得“出‘列1’和出‘列2’没差”，他没有理由偏向任何一方，所以他才愿意继续以一定概率 <code>q</code> 来混合他的策略。</p>
<hr>
<h3 id="3-一个直观的例子：点球大战"><a href="#3-一个直观的例子：点球大战" class="headerlink" title="3. 一个直观的例子：点球大战"></a>3. 一个直观的例子：点球大战</h3><p>想象一下足球比赛中的点球大战：</p>
<ul>
<li><strong>你的角色</strong>：踢球手。你可以选择踢左边或右边。</li>
<li><strong>对手的角色</strong>：守门员。他可以选择扑左边或右边。</li>
</ul>
<p>你的目标是什么？是让守门员<strong>对于“扑左还是扑右”感到无差异</strong>。</p>
<ul>
<li><strong>为什么？</strong> 如果你总喜欢踢左边（比如70%的概率），守门员就会发现这个规律，然后更倾向于扑向左边，这样你的进球率就会下降。你的行为变得“可预测”了。</li>
<li><strong>你的最优策略</strong>：你必须调整自己踢左边和右边的概率（比如各50%），使得守门员觉得“反正我扑左扑右，猜对的概率都一样，期望丢球数也一样，我无所谓了，只能瞎猜”。</li>
<li><strong>最终结果</strong>：只有当你成功地让守门员陷入“无所谓”的境地时，他才无法预测你的行动，你才能最大化你的进球率。</li>
</ul>
<p><strong>总结：</strong></p>
<p>在混合策略中，让对方“无所谓”，并不是一种善意的妥协，而是一种<strong>最顶级的进攻策略</strong>。它本质上是：</p>
<ul>
<li><strong>为了防止自己被预测和被针对。</strong></li>
<li><strong>为了迫使对方也必须采取不确定的、随机的策略来应对。</strong></li>
<li><strong>为了最终能在一个充满不确定性的稳定均衡中，保障自己获得最大化的期望收益。</strong></li>
</ul>
<p>所以，“让对方无差异”是<strong>你</strong>实现最优策略的<strong>必要条件</strong>，而不是你的目的本身。</p>
<h2 id="3-2-极大极小博弈"><a href="#3-2-极大极小博弈" class="headerlink" title="3.2 极大极小博弈"></a>3.2 极大极小博弈</h2><p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250625213116525.png" alt="image-20250625213116525"></p>
<p>这张幻灯片用一个政治竞选的例子，构建了一个经典的两人零和博弈。它给出了问题设定，但没有给出解。核心任务就是根据这些信息，计算出这场博弈的均衡解以及博弈的价值。</p>
<h4 id="第一步：检查是否存在纯策略均衡"><a href="#第一步：检查是否存在纯策略均衡" class="headerlink" title="第一步：检查是否存在纯策略均衡"></a>第一步：检查是否存在纯策略均衡</h4><p>我们首先检查是否存在一个稳定的单元格，使得任何一方都不想单方面改变策略。</p>
<ol>
<li>如果列玩家（候选人2）选择“道德”，行玩家（候选人1）会选择“经济”（因为收益3 &gt; -2）。</li>
<li>如果列玩家选择“减税”，行玩家会选择“社会”（因为收益1 &gt; -1）。</li>
<li>如果行玩家选择“经济”，列玩家会选择“减税”（因为收益1 &gt; -3）。</li>
<li>如果行玩家选择“社会”，列玩家会选择“道德”（因为收益2 &gt; -1）。</li>
</ol>
<p>我们发现，不存在任何一个稳定的策略组合。例如，在（经济, 道德）这个组合，行玩家很满意，但列玩家会想换到“减税”策略以获得更好的收益。因此，<strong>该博弈没有纯策略纳什均衡</strong>。我们必须寻找混合策略均衡。</p>
<h4 id="第二步：计算混合策略纳什均衡"><a href="#第二步：计算混合策略纳什均衡" class="headerlink" title="第二步：计算混合策略纳什均衡"></a>第二步：计算混合策略纳什均衡</h4><p>我们将使用<strong>无差异原则</strong>来求解。</p>
<ul>
<li>设行玩家（候选人1）以概率 <strong>x</strong> 选择“经济”，以概率 <strong>(1-x)</strong> 选择“社会”。</li>
<li>设列玩家（候选人2）以概率 <strong>y</strong> 选择“道德”，以概率 <strong>(1-y)</strong> 选择“减税”。</li>
</ul>
<p><strong>A. 计算行玩家的策略 x</strong></p>
<p>行玩家需要选择一个概率 <code>x</code>，使得列玩家对于选择“道德”还是“减税”感到<strong>无差异</strong>（期望收益相等）。</p>
<ul>
<li>列玩家选择“道德”的期望收益 = x⋅(−3)+(1−x)⋅(2)</li>
<li>列玩家选择“减税”的期望收益 = x⋅(1)+(1−x)⋅(−1)</li>
</ul>
<p>令二者相等：</p>
<script type="math/tex; mode=display">-3x + 2 - 2x =  - 1 + x$$$$2 - 5x = 2x - 1$$$$3 = 7x$$x=73

所以，**候选人1的最优策略**是：以 **3/7** 的概率主打“经济”议题，以 **4/7** 的概率主打“社会”议题。

**B. 计算列玩家的策略 y**

同样，列玩家需要选择一个概率 `y`，使得行玩家对于选择“经济”还是“社会”感到**无差异**。

- 行玩家选择“经济”的期望收益 = y⋅(3)+(1−y)⋅(−1)
- 行玩家选择“社会”的期望收益 = y⋅(−2)+(1−y)⋅(1)

令二者相等：

$$3y - 1 + y = -2y + 1 - y$$$$4y - 1 = 1 - 3y$$$$7y = 2$$y=72

所以，**候选人2的最优策略**是：以 **2/7** 的概率主打“道德”议题，以 **5/7** 的概率主打“减税”议题。

#### 第三步：计算博弈的价值

博弈的价值（Value）就是指在双方都采取最优策略时，行玩家的期望收益。我们可以将 y = 2/7 代入行玩家的任一期望收益公式：

V=3y−1+y=4y−1=4⋅(72)−1=78−1=71

#### 结论

- **纳什均衡**：该博弈的唯一纳什均衡是，候选人1采取混合策略 (73,74)，候选人2采取混合策略 (72,75)。这意味着两位候选人都应该以特定的概率随机选择自己的竞选焦点，让对手无法预测。
- **博弈价值**：该博弈对行玩家（候选人1）的价值是 **+1/7**。在竞选这个议题上，如果双方都采取最理性的策略，**候选人1平均可以期望净赚 1/7 百万（约14万）的选民**。这表明在这个特定的战略矩阵中，候选人1拥有微弱的优势。

![image-20250625214139041](C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250625214139041.png)

备注：下面这段话有提到“这代表了**列玩家的理性反应**。由于这是一个零和博弈，列玩家的目标是让行玩家的收益尽可能小。”，结合上面一张PPT的最后一个恒等式，可知，由于这是零和博弈，行玩家收益最少等价于列玩家收益最大。

------



这张幻灯片从一个更形式化、更根本的角度，展示了作为“领导者”（被迫先宣布策略的一方）应该如何思考，并将这个问题转化为了一个标准的数学优化问题。最终，它揭示了一个关于零和博弈的深刻结论。

#### 1. Maximin 公式解读：“在最坏的情况里做到最好”

幻灯片给出的第一个公式 

maxmin(3x1−2x2,−x1+x2)

 是“最大化最小值 (Maximin)”思想的完美数学体现。我们来拆解它：

- **内部的两个表达式**：
  - 3x1−2x2：这是当列玩家选择“道德”时，行玩家的期望收益。
  - −x1+x2：这是当列玩家选择“减税”时，行玩家的期望收益。
- **min(...) 部分**：这代表了**列玩家的理性反应**。由于这是一个零和博弈，列玩家的目标是让行玩家的收益尽可能小。所以，当行玩家宣布了一个策略 (x1,x2) 后，列玩家会审视这两个可能的收益，并选择那个能让行玩家收益**更小 (min)** 的策略来应对。这代表了行玩家在宣布策略 (x1,x2) 后，所能得到的**最坏结果保证**。
- **max(...) 部分**：这代表了**行玩家的决策**。行玩家知道对手会这样针对他。所以，他在选择自己的策略 (x1,x2) 时，目标就是让这个“最坏的结果保证”变得尽可能好。也就是要**最大化 (max)** 那个最小的收益。

这整个公式的逻辑就是：“我（行玩家）要选择一个策略 (x1,x2)，来最大化‘在我宣布这个策略后，对手尽最大努力打压我，我能得到的那个保底收益’”。

#### 2. 线性规划的转换：从博弈论到标准数学优化

Max-min 问题在数学上直接求解不方便，但可以非常巧妙地转化为一个标准的**线性规划 (Linear Programming, LP)** 问题。

- 我们引入一个新变量 `z`，让它代表那个“保底收益”。
- 我们的目标是 `最大化 z`。
- 约束条件是什么？`z` 必须小于等于所有可能的结果。所以：
  - z≤3x1−2x2  (保底收益不能超过对手选“道德”时我的收益)
  - z≤−x1+x2  (保底收益不能超过对手选“减税”时我的收益)
- 再加上概率本身的基本约束 x1+x2=1 和 x1,x2≥0。

这样，我们就把一个博弈问题，变成了一个可以用标准算法（如单纯形法）求解的数学问题。

#### 3. 最终结论：结果与之前完全相同！

幻灯片最后指出，解这个线性规划得到的结果是 x1=3/7，x2=4/7，并且最优的保底收益 z=1/7。

这正是我们之前用“无差异原则”为**同时博弈**计算出的**纳什均衡解**和**博弈价值 V**！

为什么会这样？

这并非巧合，而是冯·诺伊曼极小化极大定理 (Minimax Theorem) 的直接体现。该定理的核心是：

Maximin=Minimax

- 我们这张幻灯片求解的“领导者-跟随者”问题，正是行玩家的 **Maximin（最大化最小值）** 问题。
- 我们之前求解的“同时博弈”问题，得到的是**Minimax（极小化极大值）** 均衡解。

这个定理保证了，在任何两人零和博弈中，这两个值是相等的。这意味着，**在两人零和博弈中，不存在先手优势或后手优势**。你提前公布策略，虽然给了对方信息，但你也可以利用这一点来选择一个最稳妥的策略；对方虽然能看到你的策略，但也只能在你设定的框架内做出反应。最终双方的力量会完美抵消，达到同一个均衡结果。

## 3.3 纳什均衡的线性规划解法

![image-20250625215726625](C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250625215726625.png)

这张幻灯片介绍了一个在线性规划乃至整个优化理论中，都极具美感和威力的核心概念——**对偶性**。它揭示了每个优化问题都存在一个“影子问题”或“镜像问题”，而理解这个镜像能为我们提供关于原问题全新的、深刻的洞察。

#### 1. 什么是对偶性？一个直观的例子

要理解对偶，与其陷入复杂的数学转换，不如看一个经济学例子：

- **原问题 (Primal Problem)**：
  - 想象你是一家工厂的老板，你要决定生产多少桌子和椅子，来**最大化你的总利润**。
  - 你面临一些**约束**：你拥有的木材、劳动力、设备时间都是有限的。
  - 这就是一个典型的线性规划问题：最大化一个目标（利润），同时满足一系列约束（资源）。
- **对偶问题 (Dual Problem)**：
  - 现在，想象一个商人想来收购你所有的资源（木材、劳动力、设备）。他想**最小化他的收购总成本**。
  - 但他面临一个**约束**：他为每种资源开出的“影子价格”组合，必须能让你觉得“卖掉资源比我自己生产产品更划算”。例如，生产一张桌子需要消耗的资源，他打包收购的价格，必须不能低于你自己生产这张桌子能获得的利润。
  - 这个商人的问题——在一定约束下最小化成本——就是你那个最大化利润问题的“对偶问题”。

#### 2. 对偶性的“魔力”：原问题与对偶问题的关系

对偶理论中有两个核心定理，它们揭示了原问题和对偶问题之间的奇妙关系：

1. **弱对偶定理 (Weak Duality)**：对偶问题的最优解，永远是原问题最优解的一个“界限”。在上面的例子里，就是说：商人收购资源的**最小成本**，必然**大于等于**工厂老板自己生产的**最大利润**。这很直观，因为如果收购成本低于你的利润，你肯定不会卖。

2. 强对偶定理 (Strong Duality)：在绝大多数情况下，这个“大于等于”实际上是**“完全等于”**！也就是说：

   工厂能实现的最大利润=商人收购资源的最小成本

   这是一个非常深刻的结论。它意味着，你资源的内在价值，恰好等于你能用它们创造的最大利润。对偶问题中的变量（资源的“影子价格”），精确地量化了每一种稀缺资源的边际价值。

#### 3. 对偶性与“零和博弈”的惊人联系

现在，我们可以把这个概念带回我们之前讨论的博弈论了。这正是引入对偶概念的关键所在。

- 在前几张幻灯片中，我们把**行玩家**的问题构建成了一个线性规划：**最大化**他的保底收益 `V`。这可以看作是我们的**原问题 (Primal)**。
- 那么，**列玩家**的问题是什么？列玩家的目标是**最小化**行玩家能获得的最大收益 `W`。我们同样可以把列玩家的这个问题也构建成一个线性规划。
- **最关键的结论是**：**列玩家的“最小化极大值”线性规划问题，恰好就是行玩家“最大化最小值”线性规划问题的对偶问题！**

因此，线性规划的“强对偶定理”（最大值 = 最小值），在零和博弈的语境下，就直接变成了冯·诺伊曼的“极小化极大定理”！

max(行玩家的保底收益)=min(列玩家的最大损失)Maximin=Minimax

**总结**：对偶性不仅是线性规划的强大工具，它还为博弈论的基石——极小化极大定理——提供了最坚实的数学证明。它优美地揭示了，一个参与者的最大化问题和其对手的最小化问题，实际上是同一个数学结构的两个不同侧面，如同一枚硬币的两面，其价值必然相等。

![image-20250625220404379](C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250625220404379.png)

线性规划对偶性实例。

![image-20250625220536283](C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250625220536283.png)

弱对偶性和强对偶性

## 3.4 线性互补问题

![image-20250625222636285](C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250625222636285.png)

**标题：计算两人一般和博弈的纳什均衡 (Computing Nash equilibria of two-player, general-sum games)**

- 不幸的是，寻找一个两人**一般和博弈 (general-sum game)** 的纳什均衡，**无法**被构建成一个线性规划问题。
  - — 两名参与者的利益不再是***完全\*对立的 (completely opposed)**。
  - — 然而，我们仍然可以将我们的问题表述为某种优化问题。

------

这张幻灯片标志着一个重要的转折点。从“零和博弈”进入了更普遍、也更复杂的“一般和博弈”（或称“非零和博弈”）的世界。幻灯片的核心信息是：之前强大而高效的线性规划（LP）工具，在这里失效了。

#### 1. 什么是一般和博弈？

一般和博弈指的是，在任何一个结果下，所有参与者的收益之和**不一定为零**。这意味着博弈的结果可以是双赢、双输，或者一方赢多、一方输少。参与者的关系不再是“你死我活”的纯粹冲突，而是**冲突与合作并存**。

- 经典例子1：囚徒困境

  如果两个囚犯都背叛对方，他们可能各判5年（总收益-10）。如果他们都保持沉默，可能各判1年（总收益-2）。这是一个双输的“负和”博弈。

- 经典例子2：性别大战 (Battle of the Sexes)

  一对情侣都想待在一起（合作），但一人想看歌剧，另一人想看球赛（冲突）。如果他们去同一个地方，双方都能获得高收益（比如（5,2）或（2,5）），总收益为7。如果去不同地方，则两人都很不开心（0,0），总收益为0。这是一个“正和”博弈。

#### 2. 为什么线性规划会失效？—— 对偶性的崩塌

这是理解这张幻灯片最关键的地方。线性规划之所以能在零和博弈中大显神威，是因为其背后优美的“对偶性”，而这种对偶性源于双方利益的**完全对立**。

- **在零和博弈中**：
  - 行玩家的目标是：最大化自己的收益 `u₁`。
  - 列玩家的目标是：最大化自己的收益 `u₂`。
  - 由于 `u₂ = -u₁`，所以列玩家“最大化`u₂`”就**等价于**“最小化`u₁`”。
  - 因此，行玩家的“最大化最小值 (Maximin)”问题和列玩家的“最小化极大值 (Minimax)”问题，形成了一对完美的数学**对偶**。它们就像一枚硬币的两面，可以用同一个线性规划框架来解决。
- **在一般和博弈中**：
  - `u₂` 不再等于 `-u₁`。
  - 列玩家的目标——最大化他自己的 `u₂`——与行玩家的收益 `u₁` **没有直接的、负相关的关系**。
  - 列玩家不再是处心积虑地要让行玩家的收益最小化，他只关心自己的收益。
  - 这样一来，那种“我之所得即你之所失”的完美对偶关系就**彻底崩塌**了。我们无法再构建出一个单一的线性规划问题来同时描述双方的决策并找到那个共同的解。

#### 3. 那问题变成了什么？

虽然不能用线性规划，但寻找纳什均衡依然是一个数学优化问题，只是变得更复杂了。

- 寻找两人一般和博弈的纳什均衡，在数学上等价于求解一个**线性互补问题 (Linear Complementarity Problem, LCP)**。这是一个比线性规划更复杂的数学结构。
- 从计算复杂性的角度看，求解两人零和博弈是“容易”的（属于 **P** 问题），而求解两人一般和博弈则被证明是**PPAD-完全 (PPAD-complete)** 问题。这通常被认为是一个“更难”的计算等级，意味着找到解需要更复杂的算法，计算效率也更低。

**总结**：从“零和”到“一般和”的转变，是博弈论中一次巨大的复杂性飞跃。它让我们失去了线性规划这个简洁高效的工具，迫使我们进入一个更困难的计算领域。这也反过来凸显了两人零和博弈及其“极小化极大定理”在理论上的简洁与优美。

![image-20250625223646101](C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250625223646101.png)

**标题：计算两人一般和博弈的纳什均衡**

- 我们首先考虑一个**内部的 (inner)** 或**完全混合的 (totally mixed)** 纳什均衡 (X∗,Y∗)，即对于所有的 i 和 j，都有 xi∗>0 以及 yj∗>0（所有的纯策略都以正概率被使用）。

- 让 ai 表示行玩家的支付矩阵A的各行，让 bj 表示列玩家的支付矩阵B的各列。

- 利用“**在一个纳什均衡策略的支持集(support)中，所有纯策略都产生相同的支付**”这一事实，并且该支付大于或等于支持集之外的策略的支付，我们得到：

  - aiy∗=aky∗,i,k=1,2,…,m.

  - (x∗)Tbj=(x∗)Tbk,j,k=1,2,…,n.

    (注：此处公式经过订正以符合标准表达)

    (绿色文字) 假设每个纯策略都以正概率被使用。

- 上述构成了一个可以被高效求解的线性方程组。

#### **下图内容**

- 然而，“每个策略都以正概率被使用”的假设是有限制性的。大多数博弈并不拥有完全混合的纳什均衡；对于它们而言：

- 我们计算一个有限两人博弈的所有纳什均衡：

  一个混合策略组合 (x∗,y∗) 是一个具有支持集 S1,S2 的纳什均衡，当且仅当：

  - u=aiy∗,∀i∈S1 （对于支持集内的策略i，收益都等于均衡收益u）
  - u≥aiy∗,∀i∈/S1 （对于支持集外的策略i，收益不高于u）
  - v=(x∗)Tbj,∀j∈S2 （对于支持集内的策略j，收益都等于均衡收益v）
  - v≥(x∗)Tbj,∀j∈/S2 （对于支持集外的策略j，收益不高于v）
  - xi∗=0,∀i∈/S1, yj∗=0,∀j∈/S2  （u, v 是NE中的收益值）

- 要让上述过程可行，我们需要找到正确的**支持集 (supports)**。我们需要遍历所有可能的支持集组合。由于存在 2n+2m 种不同的支持集，这会导致算法具有指数级的复杂度。

- **备注 (Remark)**：计算有限博弈的纳什均衡的**计算复杂度**，就在于**找到正确的支持集**。

------

这两张幻灯片讲述了一个关于“求解一般和博弈”的完整故事：从一个理想化的、简单的特例，到一个普遍的、困难的现实。

#### 1. 理想情况：完全混合均衡（上图）

幻灯片的上半部分描绘了一种“完美”的均衡状态，即**完全混合均衡**。在这种均衡里，每一个参与者都认为对手的所有可选策略都值得提防，因此自己的最优策略是给自己的每一个选项都分配一个**大于零**的概率。

- 为什么这种情况简单？

  因为它使得“无差异原则”可以应用到所有策略上。为了让对手混合他的所有策略，你必须让你的对手在选择他的任何一个策略时，期望收益都完全相等。

- 如何求解？

  这就产生了一个完整的线性方程组（m-1个关于行玩家收益的等式，n-1个关于列玩家收益的等式，再加上两个概率和为1的等式）。这是一个标准的、可以用我们熟悉的方法高效求解的数学问题。

然而，这种所有策略都被用上的“雨露均沾”式的均衡，在现实中非常罕见。

#### 2. 现实情况：寻找“支持集”（下图）

幻灯片的下半部分指出了残酷的现实：在绝大多数博弈中，通常都会有一些策略是“劣势策略”或“糟糕的选项”，一个理性的玩家是永远不会使用它们的（即使用概率为0）。

- **支持集 (Support)**：在一个混合策略中，那些**真正以正概率被使用的纯策略的集合**，被称为这个混合策略的“支持集”。
- **核心困难**：求解的关键困难在于，我们**事先并不知道**最终的均衡解中，到底哪些策略会是“优势策略”（在支持集里），哪些是“劣势策略”（在支持集外）。

这就引出了一个计算上的巨大难题，我喜欢称之为**“寻找嫌疑人”的困境**：

- **无差异原则**就像一个完美的“审讯工具”，只要你把正确的“嫌疑人”（支持集里的策略）找来，它就能告诉你每个人的详细“作案手法”（混合策略的精确概率）。
- **但问题是**，你不知道谁是真正的“嫌疑人”。你面对着所有可能的策略，不知道该把哪些策略纳入“无差异”的审讯中。

#### 3. “暴力搜索”算法及其指数级复杂度

理论上，我们可以用一种“暴力”的方法来找到所有均衡：

1. **猜测**：我们先猜一个可能的支持集组合。例如，“我猜行玩家只会用策略1和3，而列玩家只会用策略2和4”。
2. **求解**：基于这个猜测，我们建立一个只包含这些策略的线性方程组（即只让这些策略满足无差异原则）并求解。
3. **验证**：检查解出的结果是否是一个合法的纳什均衡。这包括两部分：
   - 解出的概率值是否都在0和1之间？
   - 对于那些我们**没猜**的“局外”策略，它们的期望收益是否真的**不优于**我们算出的均衡收益？（这是最关键的验证，确保没人想把局外策略拉进局内）。
4. **重复**：如果验证失败，就回到第一步，换一种支持集的猜测，然后重复整个过程，直到遍历完**所有可能的支持集组合**。

这个方法的致命缺陷在于，支持集的组合数量是**指数级增长**的。如果一个博弈双方各有20个策略，那么可能的支持集组合数量会是一个天文数字。这就是幻灯片所说的“指数级复杂度”。

**总结**：寻找一般和博弈的纳什均衡，其计算上的困难**不在于解方程**，而在于**找到应该用哪些策略来列方程**。这个“寻找正确支持集”的组合搜索过程，是该问题计算复杂度（PPAD-complete）的根源，也使其与可以通过线性规划轻松求解的两人零和博弈产生了本质的区别。

## 7.1 多智能体强化学习介绍及基本概念

![image-20250630205121221](C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250630205121221.png)

这张幻灯片指出了从单智能体学习（比如我们熟知的AlphaGo下围棋的早期版本）迈向多智能体学习（比如王者荣耀或星际争霸中的AI）时，所遇到的一个根本性的、质的困难。

#### 1. 核心困难：移动的靶心 (The Moving-Target Problem)

- **在单智能体学习中**：环境是**静止的**或有固定规律的。一个智能体（比如一个机器人）学习走路，它只需要掌握如何应对地板、墙壁等物理规律即可。这个“游戏规则”是不会变的。
- **在多智能体学习中**：情况发生了根本变化。对于任何一个智能体（我们称之为“我”）来说，**其他的智能体也是环境的一部分**。但问题是，这些“其他的智能体”本身也在学习、在进化、在改变他们的策略。
  - **一个生动的例子**：你学习如何开车上班最快。如果只有你一个人在学习，而其他人的路线每天都固定不变，这就是一个简单的单智能体问题。但现实是，成千上万的司机（其他智能体）每天也都在尝试新的路线来优化他们自己的通勤时间。你今天发现的“最优路线”，明天可能因为有几百个和你一样的人也发现了它而变得极度拥堵。
  - 你试图瞄准的那个“最优策略”的靶心，因为他人的学习而**不断地移动**。这就是多智能体学习的核心困难，学术上称为**“环境的非平稳性” (Non-stationarity)**。

#### 2. 为什么简单的Q学习会失效？

标准的单智能体强化学习算法（如Q学习）之所以能成功，是因为它们依赖于一个基本假设：**马尔可夫决策过程 (MDP)**，即环境是平稳的。这意味着，在同一个状态（State）下，采取同一个动作（Action），得到的奖励（Reward）和状态转移的概率应该是基本一致的。

但在多智能体环境中，这个假设被彻底打破了：

- **今天**：在路口（状态S），你选择直行（动作A），因为其他人都选择了右转，所以一路畅通，你获得了很高的奖励。你的Q表格会更新，认为`(S, A)`是个好选择。
- **明天**：在同一个路口（状态S），你根据昨天的经验再次选择直行（动作A）。但昨天和你一样选择直行的其他智能体也获得了高奖励，所以今天他们也选择直行。结果造成了交通堵塞，你得到了一个很低的奖励。你的Q表格又必须更新，认为`(S, A)`是个坏选择。

你的Q值会这样剧烈地来回震荡，可能永远无法收敛到一个稳定的策略，因为一个动作的“好”与“坏”不再是固定的，而是完全取决于其他智能体当前正在执行的策略。

#### 3. 与博弈论的联系

这个学习过程中的“不稳定”问题，正是我们在前面博弈论部分看到的“均衡”问题的动态体现。

- 多智能体学习的目标，往往就是让这群智能体通过学习和试错，最终能够收敛到整个博弈的**纳什均衡**。
- 幻灯片中的螺旋图可以这样理解：中心点是博弈的纳什均衡点。这条螺旋线代表了所有智能体的联合策略随着时间演变的轨迹。如果学习算法设计得好（例如，在**势博弈**中），这条轨迹就会像图中一样稳定地**收敛**到中心。
- 如果算法设计不当或者博弈本身就很“恶劣”（比如“石头剪刀布”），那么学习过程可能永远无法收敛，只会在策略空间中不停地“绕圈子”。

**总结**：多智能体学习的困难在于，每个智能体的学习过程都会改变其他智能体的学习环境，形成一个复杂且动态的“移动靶心”问题。简单地将单智能体算法直接套用，会因环境的“非平稳性”而失效。因此，现代多智能体学习研究的核心，就是设计出能够在这种动态博弈中稳定地学习、并最终收敛到纳什均衡等合理状态的算法。

![image-20250630205734872](C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250630205734872.png)

我们可以通过两个关键问题来区分这些模型：

1. **有几个决策者（智能体）？** 一个还是多个？
2. **有多少种情况（状态）？** 一种还是多种？

#### 1. 马尔可夫决策过程 (MDPs): 单人探索世界

- **特征**：**一个**智能体，**多个**状态。
- **核心问题**：一个独立的决策者，在一个可以变化的环境中，如何学习一系列的动作以最大化其长期回报。
- **通俗例子**：一个机器人学习走迷宫。机器人是**唯一的智能体**，迷宫中不同的位置就是**不同的状态**。机器人需要学习在每个位置（状态）下，应该朝哪个方向走（动作），才能最快找到出口（最大化回报）。
- **地位**：这是现代**单智能体强化学习 (Single-Agent Reinforcement Learning)** 的数学基石。我们熟知的AlphaGo下棋，本质上也可以看作是在一个极度复杂的MDP中寻找最优策略。

#### 2. 重复博弈 (Repeated Games): 多人重复同一场游戏

- **特征**：**多个**智能体，**一个**状态。
- **核心问题**：多个决策者，反复地玩**同一个**博弈。
- **通俗例子**：两家寡头公司，每个月都要决定自己的产品定价。每个月的定价博弈，其本身的收益矩阵都是一样的，所以可以看作是**单一状态**。但因为博弈是**重复**的，今天的决策会影响声誉，从而影响对手明天的决策。这就引入了如“以牙还牙 (Tit-for-Tat)”这样的动态策略。我们之前讨论的各种矩阵博弈（如囚徒困境、零和博弈），如果将它们连续玩很多次，就构成了重复博弈。

#### 3. 随机博弈 / 马尔可夫博弈 (Stochastic/Markov Games): 多人探索动态世界

- **特征**：**多个**智能体，**多个**状态。
- **核心问题**：多个决策者在一个动态变化的环境中共同决策，他们的联合行动会共同决定环境如何进入下一个状态。
- **通俗例子**：一场足球比赛。场上有**多个智能体**（双方队员）。球和所有队员在场上的位置，共同构成了一个**状态**。当大家做出动作（跑动、传球、射门）后，场上的局面会变成一个**新的状态**。在每个不同的状态下，球员们面临的“局部博弈”也是不同的。
- **地位**：这是最普适、最复杂的模型，它构成了**多智能体强化学习 (Multi-Agent Reinforcement Learning, MARL)** 的理论基础。

总结与联系：

这张图清晰地告诉我们，随机博弈是“集大成者”，它统一了另外两个模型：

- 当随机博弈的智能体数量减少到1个时，它就退化成了**MDP**。
- 当随机博弈的状态数量减少到1个时，它就退化成了**重复博弈**。

这与我们上一张幻灯片讨论的**“多智能体学习的困难”**完美地衔接了起来。我们所说的“环境的非平稳性”，正是因为我们身处**随机博弈**的框架中：对于“我”这个智能体而言，环境之所以看起来在不停变化，是因为环境的下一个状态不仅取决于我的行动，还取决于其他所有智能体的行动，而他们本身也在学习和改变。

![image-20250630214131295](C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250630214131295.png)

这张幻灯片提出了一个非常经典的任务：为任意一个 2x2 零和博弈找到通用的解法，即用矩阵中的参数 a,b,c,d 来表达博弈的价值和双方的最优策略。

我们将遵循幻灯片给出的两步计划来完成这个推导。

#### 步骤一：检验纯策略均衡（鞍点）

一个纯策略纳什均衡（在零和博弈中也称为“鞍点”）存在的条件是：**某一个收益值，既是其所在行的最小值，又是其所在列的最大值**。

我们可以通过比较行玩家的“最大化最小值（Maximin）”和列玩家的“最小化极大值（Minimax）”来判断：

- 行玩家的保底收益（最大最小值）: max(min(a,b),min(d,c))
- 列玩家的保底收益（最小极大值）: min(max(a,d),max(b,c))

如果这两个值相等，则存在纯策略均衡，该值就是博弈的价值。例如，如果 `a <= b` 并且 `a >= d`，那么 `a` 就是一个鞍点，（行1，列1）就是纯策略纳什均衡。

#### 步骤二：求解混合策略均衡（假设不存在鞍点）

如果不存在鞍点，那么双方的最优策略必然是混合策略。我们将使用**无差异原则**来求解。

- **策略设定**:

  - 行玩家以概率 **p** 选择“行1”，以概率 **(1-p)** 选择“行2”。
  - 列玩家以概率 **q** 选择“列1”，以概率 **(1-q)** 选择“列2”。

- 推导行玩家的策略 p:

  行玩家选择 p 的目的是让列玩家在“列1”和“列2”之间感到无差异。

  - 列玩家选择“列1”的期望收益（注意，要用列玩家的收益矩阵`-A`）：p(−a)+(1−p)(−d)

  - 列玩家选择“列2”的期望收益：p(−b)+(1−p)(−c)

    令二者相等：

    $$-ap - d(1-p) = -bp - c(1-p)$$$$-ap - d + dp = -bp - c + cp$$$$d - c = p(a - d - b + c) = p((a+c) - (b+d))</script><pre><code>解得行玩家的最优概率 p：

p∗=(a+c)−(b+d)d−c
</code></pre><ul>
<li><p>推导列玩家的策略 q:</p>
<p>列玩家选择 q 的目的是让行玩家在“行1”和“行2”之间感到无差异。</p>
<ul>
<li><p>行玩家选择“行1”的期望收益：q(a)+(1−q)(b)</p>
</li>
<li><p>行玩家选择“行2”的期望收益：q(d)+(1−q)(c)</p>
<p>令二者相等：</p>
<script type="math/tex; mode=display">aq + b(1-q) = dq + c(1-q)$$$$aq + b - bq = dq + c - cq$$$$b - c = q(d - a - c + b) = q((b+d) - (a+c))</script><p>解得列玩家的最优概率 q：</p>
<p>q∗=(b+d)−(a+c)b−c=(a+c)−(b+d)c−b</p>
</li>
</ul>
</li>
</ul>
<h4 id="推导博弈的价值-V"><a href="#推导博弈的价值-V" class="headerlink" title="推导博弈的价值 V"></a>推导博弈的价值 V</h4><p>博弈的价值 V 是在均衡状态下，行玩家的期望收益。我们可以将求得的 q* 代入行玩家的期望收益公式：</p>
<p>V=aq∗+b(1−q∗)=q∗(a−b)+bV=(a+c)−(b+d)c−b(a−b)+b</p>
<p>对上式进行通分并化简：</p>
<p>V=(a+c)−(b+d)(c−b)(a−b)+b((a+c)−(b+d))</p>
<script type="math/tex; mode=display">V = \frac{ac - bc - ab + b^2 + ab + bc - b^2 - bd}{(a+c) - (b+d)}$$分子中的 -bc, +bc, -ab, +ab, +b², -b² 项都相互抵消了，最终得到一个非常优美的公式：

V=(a+c)−(b+d)ac−bd

------

### 最终通解

对于一个不存在纯策略均衡的 2x2 零和博弈矩阵 A=(adbc)，其通解为：

1. 行玩家的最优混合策略 (p∗,1−p∗)：

   p∗=(a+c)−(b+d)d−c

2. 列玩家的最优混合策略 (q∗,1−q∗)：

   q∗=(a+c)−(b+d)c−b

3. 该博弈对行玩家的价值 V：

   V=(a+c)−(b+d)ac−bd

### 为什么行玩家是最大化最小值，而不是最小化极大值？

问得特别好！这正是理解零和博弈中双方战略思想的精髓所在。

简单来说，答案是：**因为他们的根本目标不同。**

- **行玩家的目标**：最大化自己的收益。
- **列玩家的目标**：最大化自己的收益，这在零和博弈中就等价于**最小化行玩家的收益**。

让我们一步一步地、站在**行玩家（Player 1）**的角度来思考他为什么必须是“最大化最小值 (Maximin)”。

### 1. 行玩家的思考过程：“我如何为最坏的情况做准备？”

行玩家是一个理性的决策者，他想让自己赚得尽可能多。但他知道两件事：

- 他的收益不只取决于自己，还取决于对手的选择。
- 他的对手（列玩家）的目标和他完全相反。他每多赚一块钱，就意味着对手要多亏一块钱。所以，对手会想尽一切办法让他赚得最少。

基于这个“对手会尽力坑我”的假设，行玩家必须采取一种非常谨慎和保守的策略。他的思考逻辑如下：

1. 审视自己的第一个选项（比如“行1”）：

   “如果我选择‘行1’，我的对手会怎么做？他会看遍‘行1’的所有结果，然后选择那个能让我收益最低的选项。所以，如果我出‘行1’，我能得到的保底收益就是这一行里的最小值。”

2. 审视自己的第二个选项（比如“行2”）：

   “同理，如果我选择‘行2’，理性的对手也会选择那一列来让我获得‘行2’里的最低收益。这个最小值就是我出‘行2’的保底收益。”

3. 做出最终决策：

   现在，行玩家手上有一份“保底收益清单”，清单上的每一项都对应着他选择某一行后，在最坏情况下能得到的最低收益。作为一个想最大化自己收益的人，他会看着这份“保底清单”，然后选择那个能提供**最高（Maximum）**保底收益的选项。

整个过程串起来就是：他先找出每一行的**最小值 (Minimum)**，然后在这些最小值中，选择一个**最大值 (Maximum)**。这就是**最大化最小值 (Maximin)**。

这是一种“在最坏的情况里，为自己争取最好的结果”的策略。

------

### 2. 为什么“最小化极大值 (Minimax)”是对手的策略？

现在我们换位思考，站到**列玩家（Player 2）**的角度。

1. **他的目标**：他的目标是最小化自己的损失，也就是最小化行玩家的收益。
2. **他的思考过程**：
   - **审视他的第一个选项（比如“列1”）**：“如果我出‘列1’，我的对手会怎么做？他会看遍‘列1’的所有结果，然后选择那个能让他自己收益**最高**的选项。所以，如果我出‘列1’，我可能遭受的**最大损失**（也就是行玩家的最大收益）就是这一列里的**最大值**。”
   - **审视他的第二个选项（比如“列2”）**：“同理，如果我出‘列2’，我可能遭受的最大损失就是‘列2’里的**最大值**。”
   - **做出最终决策**：现在，列玩家手上也有一份“最大损失清单”。作为一个想最小化自己损失的人，他会看着这份清单，然后选择那个能让他的**最大损失变得最小 (Minimum)** 的选项。

整个过程串起来就是：他先找出每一列的**最大值 (Maximum)**，然后在这些最大值中，选择一个**最小值 (Minimum)**。这就是**最小化极大值 (Minimax)**。

### 总结

|          | **行玩家 (Player 1)**            | **列玩家 (Player 2)**                    |
| -------- | -------------------------------- | ---------------------------------------- |
| **目标** | 最大化自己的收益                 | 最小化自己的损失（即最小化行玩家的收益） |
| **思想** | 在最坏的情况里为自己争取最好结果 | 让对手在最好的情况下也占不到太大便宜     |
| **策略** | **最大化最小值 (Maximin)**       | **最小化极大值 (Minimax)**               |

所以，行玩家之所以是“最大化最小值”，是因为他的出发点是**为自己求利**，同时假设对手会让他**利无可利**；而“最小化极大值”则是从对手的视角出发，目标是**让行玩家的利益最小化**。冯·诺依曼的惊人发现在于，在两人零和博弈中，这两种看似不同的思考路径，最终会指向同一个均衡解。

![image-20250630215303870](C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250630215303870.png)

这张幻灯片介绍了“序贯决策”框架中那个最普适、最强大的模型——**随机博弈（或称马尔可夫博弈）**。它完美地将我们之前讨论过的“矩阵博弈”和“马尔可夫决策过程(MDP)”融合在了一起，是理解现代**多智能体强化学习 (MARL)** 的理论基石。

#### 1. 集大成者：融合了博弈论与强化学习

我们可以这样理解随机博弈的构成：

- 它借鉴了**矩阵博弈 (Matrix Games)** 的核心思想：在任何一个时刻，都有**多个智能体**在进行策略互动。
- 它又借鉴了**马尔可夫决策过程 (MDPs)** 的核心思想：整个系统存在**多个状态**，环境会根据参与者的行动从一个状态转移到另一个状态。

**随机博弈 = 多智能体的MDP = 随状态变化的重复博弈**

#### 2. 图示解读：一场动态演变的博弈

这张图非常直观地展示了随机博弈是如何运作的。我们可以把它想象成一场动态的冒险游戏：

1. **身处状态**：假设你和你的对手（参与者1和2）当前身处“**状态1 (State 1)**”。
2. **进行博弈**：在这个状态下，你们必须玩“状态1”对应的那个2x2矩阵博弈。比如，你（P1）选择了“下”，对手（P2）选择了“右”。
3. **获得即时回报**：根据矩阵，这个`(下, 右)`的联合行动会给你们带来 `(1, 1)` 的即时回报。
4. **世界发生改变（状态转移）**：这是最关键的一步。你们的联合行动 `(下, 右)` 触发了状态转移。紫色的箭头告诉我们，接下来会发生什么：
   - 有 **40%** 的概率，你们会进入“**状态2**”。
   - 有 **60%** 的概率，你们会进入“**状态3**”。
5. **进入新博弈**：假设你们进入了“状态2”。现在，你们面对的是一个**全新的2x2矩阵博弈**，有着完全不同的收益规则。你们需要在这个新规则下再次决策，然后获得新的回报，并再次触发新的状态转移。

这个过程会一直持续下去。

#### 3. 参与者的目标：深谋远虑

在一个随机博弈中，一个理性的参与者不会只盯着当前这一轮的得失。他的决策必须是**深谋远虑**的。

- 例如，在“状态1”中，`(上, 左)`这个选择能立刻带来`(2, 2)`的高回报。但如果这个选择有90%的概率会让你转移到一个对你极其不利的“惩罚状态”，那你可能就不会选它。
- 反之，你可能会选择一个即时回报较低的行动，如果它有很大概率把你带到一个未来回报极高的“天堂状态”。
- 这就是**折扣回报 (discounted rewards)** 的作用。玩家的目标是最大化未来所有回报的“总现值”，即找到一个在**所有状态下**都最优的策略（Policy），而不仅仅是当前状态。

**总结**：随机博弈为我们描绘了一幅最接近真实世界复杂性的图景——多个决策者在不断变化的环境中持续互动。我们之前讨论的**多智能体学习（MAL）的困难**，例如“环境的非平稳性”，正是源于这个框架。对于任何一个智能体来说，环境之所以“不稳定”，就是因为状态的转移和回报不仅取决于自己的行动，还取决于其他所有同样在学习和适应的智能体的行动。

![image-20250630220528288](C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250630220528288.png)

**标题：随机博弈 vs. MDP (Stochastic Games vs. MDP)**

- 在一个随机博弈中，如果除了一个参与者之外的所有其他参与者都采取**固定的 (fixed)** 策略，那么对于剩下的那个智能体来说，这个问题就**退化 (reverts back)** 回了一个MDP。
  - — 这是因为，固定其他智能体的策略（即使这些策略是随机的），会使得状态转移变得**马尔可夫化 (Markovian)**，即只取决于剩下的那个参与者的行动。

------

这张幻灯片通过一个“思想实验”，精准地指出了**多智能体学习（MARL）与单智能体学习（RL）的根本区别到底在哪里**。它告诉我们，多智能体问题的核心困难，并不在于“有多个会动的个体”，而在于“有多个会**学习和适应**的个体”。

#### 1. 问题复杂性的根源：变化的“游戏规则”

我们之前讨论，多智能体学习之所以困难，是因为环境的“非平稳性”（Non-stationarity）。对于任何一个智能体“我”来说，其他的智能体都是环境的一部分。当其他智能体也在学习、也在改变他们的策略时，就相当于“我”所面对的游戏规则本身在不断变化，这让学习变得极为困难。

#### 2. “固定策略”意味着什么？—— 从“对手”到“自然规律”

这张幻灯片提出的“固定其他所有人的策略”这个条件，是问题的关键。这意味着什么呢？让我们用一个生动的例子来说明：

- 情景A：随机博弈 (Stochastic Game)

  你是一个新手出租车司机（剩下的那个智能体），在一个大城市里学习如何最快地接送客人。城里还有成千上万的老司机（其他智能体），他们经验丰富，每天都在根据实时路况、新闻、个人习惯等调整自己的路线。这是一个极度复杂的多智能体学习问题，因为你的“环境”（即其他司机的行为）每天都在变。

- 情景B：退化为MDP

  现在，假设奇迹发生，城里所有其他司机都被换成了简单的机器人。这些机器人的行为遵循一套永不改变的固定程序。例如：“在周一上午8点的A路口，这群机器人有70%会右转，30%会直行”。这个概率是固定的，机器人不会再学习或改变了。

在这个时刻，对于你（唯一的人类司机）来说，发生了什么？

其他司机不再是具有主观能动性的“对手”了，他们变成了城市交通中一个虽然是随机的、但却是稳定的、可预测的自然规律。

#### 3. “马尔可夫化”：游戏规则被重新稳定下来

一旦其他人的策略被固定，整个系统对于“我”来说，就重新满足了**马尔可夫性质**。

- 当“我”在某个路口（状态s），选择直行（动作a），下一分钟会到达哪里、会花多长时间（下一个状态s'），其概率 **p(s′∣s,a)** 现在只取决于“我”的动作`a`和那群机器人固定的行为模式。
- 这个转移概率**不再随时间变化**了，因为机器人的“想法”不会变。
- 于是，这个复杂的、不可预测的多智能体博弈，就**退化**成了一个经典的、可解的**单智能体马尔可夫决策过程（MDP）**。

总结：

这张幻灯片通过“固定他人策略”这一巧妙的设定，帮助我们隔离并识别了多智能体问题的“困难之源”。困难不在于环境中存在多个行动者，而在于这些行动者策略的动态演化。这个洞见是许多高级多智能体学习算法的基础，例如，有些算法就是通过“轮流学习”（一个学习，其他暂时固定）或者对其他智能体的策略进行建模和预测，来试图克服这种“非平稳性”带来的挑战。

![image-20250630222547667](C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250630222547667.png)

这张幻灯片将原本简单的单次博弈，升级成了一个更复杂、也更贴近现实的**双状态随机博弈 (Two-state Stochastic Game)**。这里的“纳什均衡”不再是单个的行动组合，而是一个**策略组合 (a profile of policies)**，即每个厂商在**每种状态下**应该如何行动的完整计划。

这个问题的解取决于一个幻灯片上未给出但至关重要的参数——**折扣因子 γ (discount factor)**，它代表了厂商对未来收益的重视程度。

#### 1. 分解两个“子博弈”

我们首先分析在每个状态下，只考虑当前一轮收益的“短视”均衡是什么。

- 在状态1（无税收）：

  正如我们上一题分析的，双方都有一个占优策略：“污染”。因此，该状态下的短视纳什均衡是 (污染, 污染)。但这个选择会带来一个后果：根据转移概率 (0,1)，游戏将100%转移到状态2（有税收）。

- 在状态2（有税收）：

  我们分析这个新的利润矩阵：

  - 对厂商1：如果厂商2选择“清洁”，厂商1会选“污染”(4>1)；如果厂商2选“污染”，厂商1还是会选“污染”(3>0)。**“污染”是厂商1的占优策略**。
  - 对厂商2：如果厂商1选择“清洁”，厂商2会选“污染”(5>2)；如果厂商1选“污染”，厂商2还是会选“污染”(4>1)。**“污染”也是厂商2的占优策略**。
  - 因此，该状态下的短视纳什均衡也是 **(污染, 污染)**。这个选择的后果是，游戏将**100%留在状态2**，继续被征税。

#### 2. 长期战略的困境：短视 vs. 远见

分析完子博弈后，真正的战略困境浮现了。我们以厂商1在**状态1**的决策为例：

- **短视的选择（选择“污染”）**：可以立刻获得很高的收益（如果对方也污染，能得6）。但代价是，从下一轮开始，将永远陷入低收益的“状态2”。
- **远见的选择（选择“清洁”）**：会牺牲掉一部分即时收益（如果对方也清洁，只能得4）。但好处是，游戏将**100%留在高收益的“状态1”**，未来每一轮都可以继续获得高收益。

厂商会如何选择，完全取决于他们有多“远视”，即折扣因子 `γ` 有多大。

#### 3. 可能存在的纳什均衡

这个随机博弈可能存在多个纳什均衡。

**均衡A：“悲观”的污染均衡**

- **策略**：无论在哪种状态，双方都选择“污染”。
- **分析**：如果对方的策略是“永远污染”，那么你最好的应对也是“永远污染”。因为如果你单方面选择“清洁”，在状态1你会获得更低的即时收益(3 vs 6)然后还是会进入状态2；在状态2你单方面“清洁”的收益(0 vs 3)也更低。因此，没有任何一方有单方面改变策略的动机。
- **结论**：**（策略1=污染, 策略2=污染）是一个纳什均衡**。在这个均衡下，厂商们第一轮在状态1获得(6,7)的收益，然后永久地陷入状态2，每轮获得(3,4)的收益。这是一个低效的、“双输”的均衡。

**均衡B：“合作”的清洁均衡**

- **策略**：双方约定，只要在状态1，就都选择“清洁”。

- **分析**：要让这个“君子协定”成为一个稳定的纳什均衡，就必须保证“背叛”是无利可图的。

  - **遵守协定**的收益流（以厂商1为例）：4+4γ+4γ2+⋯=1−γ4

  - **单方面背叛**的收益流：在状态1选择“污染”获得一次性的高收益7，但之后游戏进入状态2，双方陷入“永远污染”的均衡，后续每轮收益为3。其收益流为: 7+3γ+3γ2+⋯=7+1−γ3γ

  - 要让大家遵守协定，必须满足“遵守的收益 ≥ 背叛的收益”：

    1−γ4≥7+1−γ3γ

    4≥7(1−γ)+3γ⟹4≥7−4γ⟹4γ≥3⟹γ≥43

- **结论**：**如果厂商们对未来的重视程度足够高（即折扣因子 γ ≥ 3/4），那么双方在状态1都选择“清洁”也可以成为一个纳什均衡**。因为对未来的长期高收益的渴望，足以抑制住当前“背叛”以获取短期利益的诱惑。

**总结**：这个引入了税收和状态转移的随机博弈模型，比单次博弈要复杂和深刻得多。它的均衡不再是唯一的，而是可能存在多个——一个“坏”的均衡和一个“好”的均衡。系统最终会落入哪个均衡，取决于参与者对未来的耐心和期望（由`γ`体现）。这也为政府政策的设计提供了启示：一个好的制度，应该让“合作”的门槛（即所需的`γ`值）尽可能低，让参与者更容易达成对社会有利的结果。

![image-20250630223314800](C:\Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250630223314800.png)

**标题：例子：“大冒险”游戏 (Example: the game of Dare)**

- 参与者1，**领导者 (the leader)**，和参与者2，**挑战者 (the challenger)**，同时选择“**放弃 (pass)**”或“**挑战 (dare)**”。

  - — 如果双方都选择“放弃”，支付为零（且游戏结束）。
  - — 如果参与者1“放弃”而参与者2“挑战”，参与者1赢得1。
  - — 如果参与者1“挑战”而参与者2“放弃”，参与者1赢得3。
  - — 如果双方都选择“挑战”，这个基础游戏将**角色互换**后重新进行。
    - (领导者变成挑战者，反之亦然)。
  - — 如果参与者们永远地持续“挑战”下去，则支付为零。

- 博弈矩阵 G:

  $$G = \bordermatrix{ & \text{放弃} & \text{挑战} \cr \text{放弃} & 0 & 1 \cr \text{挑战} & 3 & -G^T }</script><p>  其中 -Gᵀ 代表角色互换后的游戏。（它的矩阵是G矩阵的转置的负数。）-Gᵀ的价值是G的价值的负数。</p>
<hr>
<p>这张幻灯片描述了一个非常有趣的<strong>递归博弈 (recursive game)</strong>，它是一种特殊的<strong>随机博弈 (stochastic game)</strong>。这个博弈只有两个状态：“P1是领导者”和“P2是领导者”。当出现 (挑战, 挑战) 的结果时，游戏就在这两个状态之间切换。</p>
<h4 id="1-问题的核心：递归的价值"><a href="#1-问题的核心：递归的价值" class="headerlink" title="1. 问题的核心：递归的价值"></a>1. 问题的核心：递归的价值</h4><p>这个问题的精髓在于右下角的那个支付 <code>-Gᵀ</code>。</p>
<ul>
<li><p>让我们设这个博弈对于<strong>当前的领导者</strong>来说，其<strong>价值 (Value)</strong> 为 <strong>V</strong>。</p>
</li>
<li><p>那么，领导者的支付矩阵就可以写成：</p>
<p>G=(031Value(subgame))</p>
</li>
<li><p>当双方都选择“挑战”时，游戏进入子博弈。在这个子博弈中，原先的挑战者（P2）变成了新的领导者。由于游戏的对称性，这个子博弈对于<strong>新的领导者（P2）</strong>来说，价值也应该是 <strong>V</strong>。</p>
</li>
<li><p>既然对于新的领导者（P2）价值是 <code>V</code>，那么对于<strong>新的挑战者（也就是原来的P1）</strong>来说，价值就是 <strong>-V</strong>（因为是零和博弈）。</p>
</li>
<li><p>因此，原领导者（P1）的支付矩阵可以写成一个包含其自身价值 V 的形式：</p>
<p>G=(031−V)</p>
</li>
</ul>
<h4 id="2-求解博弈价值-V"><a href="#2-求解博弈价值-V" class="headerlink" title="2. 求解博弈价值 V"></a>2. 求解博弈价值 V</h4><p>现在，问题转化为了：求解这个特殊矩阵的价值 V，并且这个价值 V 必须等于它自身。</p>
<script type="math/tex; mode=display">V = \text{value} \begin{pmatrix} 0 & 1 \ 3 & -V \end{pmatrix}$$我们可以使用之前推导出的 2x2 零和博弈的通用价值公式：$$V = \frac{ac - bd}{(a+c) - (b+d)}$$其中，a=0, b=1, d=3, c=-V。代入公式：$$V = \frac{(0)(-V) - (1)(3)}{(0)+(-V) - (1)-(3)} = \frac{-3}{-V-4}$$现在我们得到了一个关于 V 的方程，求解它：

$$V(-V - 4) = -3$$$$-V^2 - 4V = -3$$$$V^2 + 4V - 3 = 0</script><p>这是一个一元二次方程。使用求根公式 </p>
<p>我们得到了两个可能的解：V1=7−2≈0.646 和 V2=−7−2≈−4.646。</p>
<h4 id="3-选择正确的解"><a href="#3-选择正确的解" class="headerlink" title="3. 选择正确的解"></a>3. 选择正确的解</h4><p>哪个才是这个博弈真正的价值呢？</p>
<p>我们看领导者的支付矩阵，他有一个“放弃”的选项。如果他选择“放弃”，最坏的结果是对手选择“挑战”，此时他的收益是1。这意味着，领导者至少可以为自己保证一个非负的收益。因此，一个负数（比如-4.646）不可能是这个博弈的理性价值。</p>
<p>所以，这个博弈对于领导者的价值是：根号7减去2</p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250630224225904.png" alt="image-20250630224225904"></p>
<p>求解博弈G1和G2的价值：</p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250630224137356.png" alt="image-20250630224137356"></p>
<h2 id="7-2-值迭代与策略迭代"><a href="#7-2-值迭代与策略迭代" class="headerlink" title="7.2 值迭代与策略迭代"></a>7.2 值迭代与策略迭代</h2><p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250701210516613.png" alt="image-20250701210516613"></p>
<p>1、学习随机博弈中状态值函数贝尔曼方程推导</p>
<p>2、与单智能体MDP的关键区别</p>
<p>这个公式虽然形式上与单智能体MDP的贝尔曼方程很像，但幻灯片的最后一点指出了两个根本性的区别，这也是多智能体问题复杂性的根源：</p>
<ol>
<li><strong>价值是个人化的 (for each agent)</strong>：在MDP中，只有一个价值函数 <code>V(s)</code>。但在随机博弈中，<strong>每个参与者 <code>i</code> 都有自己的一套价值函数 Vi(s)</strong>。同一个状态 <code>s</code>，对我来说可能是天堂（Vi很高），对你来说可能却是地狱（Vj很低）。这体现了参与者之间合作与冲突并存的关系。</li>
<li><strong>价值依赖于联合策略 (on the joint policy)</strong>：这是最致命的区别。在MDP中，价值函数 Vπ(s) 只取决于我自己的策略 <code>π</code>。但在这里，Viπ(s) 的值不仅取决于我的策略 πi，还取决于<strong>其他所有人的策略</strong> π−i，因为是<strong>联合行动</strong>决定了回报和状态转移。<ul>
<li>这就导致了我们之前讨论的<strong>“非平稳性”</strong>或<strong>“移动靶心”</strong>问题。如果我的对手改变了他的策略，那么即使我的策略和当前状态都没变，我整个的价值函数 Vi(s) 也会跟着改变。我原以为很有价值的状态，可能因为对手策略的改变而突然变得一文不值。</li>
</ul>
</li>
</ol>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250701211212340.png" alt="image-20250701211212340"></p>
<p>这张幻灯片在重申了状态价值的定义之后，提出了一个关于它的非常重要的数学性质：<strong>有界性 (Boundedness)</strong>。这个性质是随机博弈能够被分析和求解的理论基础之一。</p>
<h4 id="1-核心思想：无限过程，有限价值"><a href="#1-核心思想：无限过程，有限价值" class="headerlink" title="1. 核心思想：无限过程，有限价值"></a>1. 核心思想：无限过程，有限价值</h4><p>这条结论的核心思想是：尽管一个随机博弈的过程可能永远持续下去，但从任何一个状态开始，任何一个参与者能够获得的<strong>总的“折扣”价值都不是无限的，而是一个有限的、有上限的数值</strong>。</p>
<p>幻灯片给出了这个上限的计算公式：1−γM。</p>
<ul>
<li><strong>M</strong>: 代表在整个游戏所有可能的情况下，任何参与者在<strong>单一一轮</strong>中所能获得的<strong>最大绝对收益</strong>。可以理解为这个游戏里“单次操作的最大奖励或最大惩罚（的绝对值）”。</li>
<li><strong>γ (gamma)</strong>: 是我们熟悉的折扣因子（0 &lt; γ &lt; 1），代表了我们对未来收益的耐心程度。</li>
</ul>
<h4 id="2-这个上限公式是怎么来的？（几何级数）"><a href="#2-这个上限公式是怎么来的？（几何级数）" class="headerlink" title="2. 这个上限公式是怎么来的？（几何级数）"></a>2. 这个上限公式是怎么来的？（几何级数）</h4><p>这个公式的推导非常直观，它基于我们熟知的等比数列（几何级数）求和。</p>
<ol>
<li><p>根据定义，状态价值是所有未来折扣回报的总和：</p>
<p>Viπ(s)=Eπ[r0+γr1+γ2r2+γ3r3+…]</p>
</li>
<li><p>在任何一步 <code>k</code>，我们能获得的即时回报 rk 的绝对值，都不可能超过定义好的最大单轮回报 <code>M</code>。即 ∣rk∣≤M。</p>
</li>
<li><p>因此，总价值 Viπ(s) 必然小于或等于一种最极端、最理想的情况：假设我们在未来的每一步，都能幸运地获得最大的正回报 M。</p>
<p>Viπ(s)≤M+γM+γ2M+γ3M+…</p>
</li>
<li><p>将 M 提取出来：</p>
<p>Viπ(s)≤M(1+γ+γ2+γ3+…)</p>
</li>
<li><p>括号里的部分是一个公比为 <code>γ</code> 的无穷等比数列。因为 <code>γ &lt; 1</code>，这个级数是收敛的，其和为 1−γ1。</p>
</li>
<li><p>因此，我们得到了最终的边界：</p>
<p>Viπ(s)≤1−γM</p>
</li>
</ol>
<h4 id="3-这个性质为什么重要？"><a href="#3-这个性质为什么重要？" class="headerlink" title="3. 这个性质为什么重要？"></a>3. 这个性质为什么重要？</h4><ol>
<li><strong>保证问题“有解”</strong>：这个有界性保证了我们要求解的状态价值函数是一个“行为良好”的函数，它的值不会发散到无穷大。这是所有后续分析和算法能够成立的数学前提。如果没有这个保证，我们可能都无法定义“最优策略”，因为所有策略的总回报都是无穷大，无法比较。</li>
<li><strong>为算法提供基础</strong>：在很多求解随机博弈的算法（例如价值迭代）中，这个边界可以用于初始化价值函数，或者作为算法收敛性的一个判断依据。它确保了算法的计算过程会在一个有限的数值空间内进行，最终能够稳定下来。</li>
</ol>
<p><strong>总结</strong>：这张幻灯片的核心是告诉我们，尽管随机博弈的博弈过程可能是无限的，但其价值是有限的。这个有界性不仅为问题的“可解性”提供了理论保障，也为实际的计算算法奠定了基础。</p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250701215722881.png" alt="image-20250701215722881"></p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250701215827727.png" alt="image-20250701215827727"></p>
<h4 id="结论与洞察"><a href="#结论与洞察" class="headerlink" title="结论与洞察"></a>结论与洞察</h4><p>这个例子深刻地揭示了<strong>短期利益与长期战略</strong>之间的权衡。</p>
<ul>
<li>从短期看，第1列对列玩家更有利（即时损失更小）。</li>
<li>但从长期看，第1列有更高的概率让游戏继续下去，这意味着他未来要持续地向行玩家支付价值为<code>v</code>的收益。而第2列能更快地结束游戏，从而“止损”。</li>
<li>最终的均衡策略显示，<strong>长期战略（避免未来损失）的重要性压倒了短期利益</strong>。因此，列玩家的最优策略是<strong>更频繁地选择那个能更快结束游戏的第2列</strong>，尽管它眼前的损失看起来更大。</li>
</ul>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250702003904470.png" alt="image-20250702003904470"></p>
<p><strong>标题：价值迭代 (Value Iteration)</strong></p>
<ul>
<li>夏普利证明了 vn(s) 会收敛到从s开始的随机博弈的<strong>真实价值 v(s)</strong>。<ul>
<li>— 首先，收敛是以<strong>指数速率 (exponential rate)</strong> 进行的：最大误差至少以 γn 的速度下降。</li>
<li>— 其次，在第 n+1 阶段的最大误差，至多是“<strong>从n到n+1阶段的最大变化量</strong>”乘以 γ/(1−γ)。</li>
</ul>
</li>
</ul>
<hr>
<p>这张幻灯片深入探讨了“价值迭代”算法的<strong>性能保证</strong>。它告诉我们，夏普利不仅证明了价值迭代这个方法是<strong>可行</strong>的（即最终能找到正确答案），更证明了它是<strong>高效</strong>和<strong>可靠</strong>的。这使得价值迭代从一个理论上的概念，变成了一个可以在实践中应用的强大工具。</p>
<h4 id="1-指数速率收敛：为什么说它“高效”？"><a href="#1-指数速率收敛：为什么说它“高效”？" class="headerlink" title="1. 指数速率收敛：为什么说它“高效”？"></a>1. 指数速率收敛：为什么说它“高效”？</h4><p>“指数速率收敛”听起来很抽象，但它的意思是，算法的精确度在每一步迭代后都会得到一个“质的飞跃”。</p>
<ul>
<li><strong>一个比喻</strong>：想象你在寻宝，宝藏在1公里外。有一个向导，你每走一步，他都会告诉你：“你现在离宝藏的距离，是你上一步距离的90%”。<ul>
<li>你的误差（与宝藏的距离）在每一步都会乘以一个固定的因子（0.9）。</li>
<li>第一次迭代后，误差是 1×0.9。</li>
<li>第二次迭代后，误差是 1×0.92。</li>
<li>第n次迭代后，误差是 1×0.9n。</li>
<li>误差以 0.9n 的速度急剧缩小，这就是指数级的衰减。</li>
</ul>
</li>
<li><strong>在价值迭代中</strong>：折扣因子 <code>γ</code> (一个小于1的数) 就扮演了这个“0.9”的角色。每迭代一次，我们估算的价值函数 vn(s) 与真实价值 v(s) 之间的最大误差，都会大致缩小一个 <code>γ</code> 倍。因为 <code>γ</code> 小于1，所以经过多次迭代后，误差会变得非常小，算法能很快地逼近真实解。这背后的数学原理是，夏普利证明了价值迭代的更新算子是一个<strong>压缩映射 (Contraction Mapping)</strong>。</li>
</ul>
<h4 id="2-误差边界：为什么说它“可靠”？"><a href="#2-误差边界：为什么说它“可靠”？" class="headerlink" title="2. 误差边界：为什么说它“可靠”？"></a>2. 误差边界：为什么说它“可靠”？</h4><p>第二点结论解决了一个非常实际的问题：“我怎么知道什么时候可以停止算法，并且保证我的答案足够精确了？”</p>
<ul>
<li><p><strong>面临的困境</strong>：我们希望我们的误差，即 <code>|我们的估算值 vₙ - 真实值 v*|</code>，小于一个我们能接受的阈值（比如0.001）。但问题是，我们并不知道那个神秘的“真实值 <code>v*</code>”到底是多少，所以无法直接计算这个误差。</p>
</li>
<li><p>夏普利提供的解决方案：他给出了一个可计算的误差上限。公式告诉我们：</p>
<p>真实的未知误差≤可计算的最大单步变化量×1−γγ</p>
<p>这里的“可计算的最大单-步变化量”指的是 ∣vn+1(s)−vn(s)∣max，也就是在你最近一次迭代中，所有状态的价值估算值变化最大的那一个。这个值我们在计算过程中是完全知道的。</p>
</li>
<li><p><strong>实际应用（停止条件）</strong>：</p>
<ol>
<li>我们设定一个目标精度 <code>ε</code>，比如我希望我的最终答案与真实值的误差不超过0.001。</li>
<li>根据公式，只要我们能让 <code>(可计算的最大单步变化量) * γ/(1-γ)</code> 这个上限小于 <code>ε</code>，那么真实的误差就一定小于 <code>ε</code>。</li>
<li>因此，我们的算法停止条件就变成了：<strong>持续迭代，直到我们观察到的最大单步变化量小于 ϵ⋅γ1−γ</strong>。</li>
<li>一旦满足这个条件，我们就可以放心地停止迭代，并宣布当前的估算值 vn+1 就是一个足够精确的解。</li>
</ol>
</li>
</ul>
<p><strong>总结</strong>：这张幻灯片从理论上为价值迭代算法的有效性提供了强有力的背书。<strong>指数速率收敛</strong>保证了它的计算速度，而<strong>实用的误差边界</strong>则为它在现实中的应用提供了可靠的停止准则，使其成为求解两人零和随机博弈的核心算法之一。</p>
<p><img src="Users\zcl\AppData\Roaming\Typora\typora-user-images\image-20250702005255997.png" alt="image-20250702005255997"></p>
<p><strong>为什么最小停止概率是0.5？</strong></p>
<h3 id="1-“最小停止概率是0-5”的来源分析"><a href="#1-“最小停止概率是0-5”的来源分析" class="headerlink" title="1. “最小停止概率是0.5”的来源分析"></a>1. “最小停止概率是0.5”的来源分析</h3><p>这个结论来自于对 G₁ 和 G₂ 两个矩阵中所有单元格的<strong>“持续概率”</strong>的分析。</p>
<p>在一个随机博弈的支付单元格中，形如 <code>“即时回报 + 概率 × 未来价值”</code> 的结构，那个<strong>概率</strong>就代表了游戏<strong>继续下去的可能性</strong>。而<strong>“停止概率”</strong>则等于 <strong>1 - 继续概率</strong>。</p>
<p>让我们来逐一检查两个矩阵中所有结果的“停止概率”：</p>
<p>对于博弈 G₁:</p>
<p>G(1)=(4+0.3G(1)1+0.4G(2)0+0.4G(2)3+0.5G(1))</p>
<ul>
<li>(行1, 列1): 继续概率是 0.3。 <strong>停止概率 = 1 - 0.3 = 0.7</strong></li>
<li>(行1, 列2): 继续概率是 0.4。 <strong>停止概率 = 1 - 0.4 = 0.6</strong></li>
<li>(行2, 列1): 继续概率是 0.4。 <strong>停止概率 = 1 - 0.4 = 0.6</strong></li>
<li>(行2, 列2): 继续概率是 0.5。 <strong>停止概率 = 1 - 0.5 = 0.5</strong></li>
</ul>
<p>对于博弈 G₂:</p>
<p>G(2)=(0+0.5G(1)−4−51+0.5G(2))</p>
<ul>
<li>(行1, 列1): 继续概率是 0.5。 <strong>停止概率 = 1 - 0.5 = 0.5</strong></li>
<li>(行1, 列2): 支付是-5（没有未来价值项）。继续概率是 0。<strong>停止概率 = 1 - 0 = 1</strong></li>
<li>(行2, 列1): 支付是-4。继续概率是 0。<strong>停止概率 = 1 - 0 = 1</strong></li>
<li>(行2, 列2): 继续概率是 0.5。<strong>停止概率 = 1 - 0.5 = 0.5</strong></li>
</ul>
<p>现在，我们把所有计算出的停止概率放在一起：{ 0.7, 0.6, 0.5, 1 }。</p>
<p>在所有这些可能性中，最小的那个值，就是 0.5。</p>
<p>这就是“最小停止概率是0.5”这句话的直接来源。</p>
<h3 id="2-这个数字为什么如此重要？"><a href="#2-这个数字为什么如此重要？" class="headerlink" title="2. 这个数字为什么如此重要？"></a>2. 这个数字为什么如此重要？</h3><p>这个“最小停止概率”反过来告诉了我们整个系统的<strong>“最大继续概率”</strong>。</p>
<ul>
<li>最小停止概率 = 0.5</li>
<li>最大继续概率 = 1 - 最小停止概率 = 1 - 0.5 = 0.5</li>
</ul>
<p>这个<strong>“最大继续概率”</strong>，可以被看作是整个随机博弈系统的<strong>有效折扣因子 γ (effective discount factor)</strong>。</p>
<p>为什么呢？</p>
<p>价值迭代算法的收敛速度，取决于其更新算子是不是一个“压缩映射”，而其“压缩程度”就由折扣因子γ决定。为了保证整个系统一定收敛，我们必须考虑最坏的情况。</p>
<ul>
<li><strong>收敛的最坏情况是什么？</strong> 就是收敛得最慢的情况。</li>
<li><strong>什么时候收敛得最慢？</strong> 就是“折扣”打得最少的时候，也就是游戏最不容易结束、<strong>继续下去的概率最大</strong>的时候。</li>
</ul>
<p>在这个博弈中，游戏继续下去的最大概率是0.5。因此，整个价值迭代算法的收敛速度就由这个0.5来决定。</p>
<ul>
<li><strong>收敛速率</strong>：因为有效折扣因子<code>γ</code>是0.5，所以算法的误差是以 (0.5)n 的指数速率下降的。</li>
<li><strong>误差边界</strong>：根据我们之前学过的误差边界公式 <code>真实误差 ≤ 最大单步变化量 × γ/(1-γ)</code>，代入<code>γ=0.5</code>，我们得到 <code>γ/(1-γ) = 0.5/0.5 = 1</code>。这意味着，真实的未知误差，不会超过我们在上一步迭代中能观测到的最大变化量。幻灯片中说v₆的误差至多是0.0002，就是基于这个原理计算出来的（从v₅到v₆的最大变化量约为0.0001，其上限0.0002是完全正确的）。</li>
</ul>
<p><strong>总结</strong>： “最小停止概率是0.5”这个结论，是通过分析所有可能结果得出的。它的真正意义在于，它为我们确定了整个动态系统的有效折扣因子 <code>γ=0.5</code>，从而为算法的<strong>收敛速度</strong>和<strong>误差分析</strong>提供了坚实的理论依据。</p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/05/15/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRESENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/" rel="prev" title="ATTENTION-GUIDED CONTRASTIVE ROLE REPRESENTATIONS FOR MULTI-AGENT REINFORCEMENT LEARNING">
                  <i class="fa fa-angle-left"></i> ATTENTION-GUIDED CONTRASTIVE ROLE REPRESENTATIONS FOR MULTI-AGENT REINFORCEMENT LEARNING
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">GGBond</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"cdn":"https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML","tags":"none","src":"custom_mathjax_source","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
