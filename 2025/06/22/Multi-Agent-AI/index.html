<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/avatar-32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/avatar-16.png">
  <link rel="mask-icon" href="/images/emoji-smile.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Code:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"zcl0219.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":null,"show_result":true},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="引言 多智能体强化学习系统特点 智能体数量非单个 智能体之间存在一定联系：合作、对抗、合作+对抗（多智能体多动机） 智能体不仅需要考虑到环境，还需要考虑到其他智能体的行动 学习环境动态变化">
<meta property="og:type" content="article">
<meta property="og:title" content="Multi-Agent AI">
<meta property="og:url" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="引言 多智能体强化学习系统特点 智能体数量非单个 智能体之间存在一定联系：合作、对抗、合作+对抗（多智能体多动机） 智能体不仅需要考虑到环境，还需要考虑到其他智能体的行动 学习环境动态变化">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/png1.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250622215001166.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250622215250842.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250622215526657.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250622221246041.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250622221913906.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250622222258083.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250622222540677.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250622223016230.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250622223222044.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250622225345099.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250623201854412.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250623202222337.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250623202435065.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250623202916496.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250623203105804.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250623203455003.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250623203658761.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250623203835329.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250623204324562.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250623204526705.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250623204705478.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250623204723342.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250623210315029.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250623211203839.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250623212809384.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250623213442013.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250623214724248.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250624002522709.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250624003714695.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250624004513150.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250624194410787.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250624200511139.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250624201207684.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250624201541524.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250624202953517.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250624203148105.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250624204837963.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250624205950398.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250624211206447.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250624211304691.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250624232921182.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250624233936853.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250625000231647.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250625213116525.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250625214139041.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250625215726625.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250625220404379.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250625220536283.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250625222636285.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250625223646101.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250630205121221.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250630205734872.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250630214131295.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250630215303870.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250630220528288.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250630222547667.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250630223314800.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250630224225904.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250630224137356.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250701210516613.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250701211212340.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250701215722881.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250701215827727.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250702003904470.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250702005255997.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250708201920820.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250708202817638.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250708204114402.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250708205136340.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250709000352490.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250709003711705.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250709004802516.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250709011441787.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250709013039785.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250709014313654.png">
<meta property="og:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/image-20250709014504249.png">
<meta property="article:published_time" content="2025-06-22T13:04:54.000Z">
<meta property="article:modified_time" content="2025-07-09T05:55:03.150Z">
<meta property="article:author" content="GGBond">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/png1.png">


<link rel="canonical" href="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/","path":"2025/06/22/Multi-Agent-AI/","title":"Multi-Agent AI"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Multi-Agent AI | Hexo</title>
  







<link href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap" rel="stylesheet">

  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Hexo</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%95%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">引言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%9A%E5%BC%88%E8%AE%BA"><span class="nav-number">2.</span> <span class="nav-text">博弈论</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%9A%E5%BC%88%E7%B1%BB%E5%9E%8B"><span class="nav-number">3.</span> <span class="nav-text">博弈类型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%8D%E5%A4%8D%E5%8D%9A%E5%BC%88"><span class="nav-number">3.1.</span> <span class="nav-text">重复博弈</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%89%A9%E5%B1%95%E5%BD%A2%E5%BC%8F%E5%8D%9A%E5%BC%88"><span class="nav-number">3.2.</span> <span class="nav-text">扩展形式博弈</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%BF%E5%8D%9A%E5%BC%88"><span class="nav-number">3.3.</span> <span class="nav-text">势博弈</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%B3%E4%BB%80%E5%9D%87%E8%A1%A1%E6%B1%82%E8%A7%A3"><span class="nav-number">4.</span> <span class="nav-text">纳什均衡求解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9B%B6%E5%92%8C%E5%8D%9A%E5%BC%88%E5%8F%8A%E7%BA%B3%E4%BB%80%E5%9D%87%E8%A1%A1%E8%AE%A1%E7%AE%97"><span class="nav-number">4.1.</span> <span class="nav-text">零和博弈及纳什均衡计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9E%81%E5%A4%A7%E6%9E%81%E5%B0%8F%E5%8D%9A%E5%BC%88"><span class="nav-number">4.2.</span> <span class="nav-text">极大极小博弈</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%B3%E4%BB%80%E5%9D%87%E8%A1%A1%E7%9A%84%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92%E8%A7%A3%E6%B3%95"><span class="nav-number">4.3.</span> <span class="nav-text">纳什均衡的线性规划解法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E4%BA%92%E8%A1%A5%E9%97%AE%E9%A2%98"><span class="nav-number">4.4.</span> <span class="nav-text">线性互补问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-number">5.</span> <span class="nav-text">多智能体强化学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-number">5.1.</span> <span class="nav-text">7.1
多智能体强化学习介绍及基本概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%80%BC%E8%BF%AD%E4%BB%A3%E4%B8%8E%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3"><span class="nav-number">5.2.</span> <span class="nav-text">7.2 值迭代与策略迭代</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9D%87%E8%A1%A1%E5%AD%A6%E4%B9%A0"><span class="nav-number">5.3.</span> <span class="nav-text">7.3 均衡学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%80%E4%BD%B3%E5%AF%B9%E7%AD%96"><span class="nav-number">5.4.</span> <span class="nav-text">7.4 最佳对策</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="GGBond"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">GGBond</p>
  <div class="site-description" itemprop="description">Doing the tough things sets winners apart from losers</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">34</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://zcl0219.github.io/2025/06/22/Multi-Agent-AI/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="GGBond">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="Doing the tough things sets winners apart from losers">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Multi-Agent AI | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Multi-Agent AI
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-06-22 21:04:54" itemprop="dateCreated datePublished" datetime="2025-06-22T21:04:54+08:00">2025-06-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-07-09 13:55:03" itemprop="dateModified" datetime="2025-07-09T13:55:03+08:00">2025-07-09</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="引言">引言</h2>
<p><strong>多智能体强化学习系统特点</strong></p>
<p>智能体数量非单个</p>
<p>智能体之间存在一定联系：合作、对抗、合作+对抗（多智能体多动机）</p>
<p>智能体不仅需要考虑到环境，还需要考虑到其他智能体的行动</p>
<p>学习环境动态变化</p>
<span id="more"></span>
<h2 id="博弈论">博弈论</h2>
<p>minmax</p>
<p>nash equilibrium</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/png1.png" alt="1">
<figcaption aria-hidden="true">1</figcaption>
</figure>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250622215001166.png" alt="1-2">
<figcaption aria-hidden="true">1-2</figcaption>
</figure>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250622215250842.png" alt="1-3">
<figcaption aria-hidden="true">1-3</figcaption>
</figure>
<p>上图说明了制定出合理的策略需要智能体之间communication</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250622215526657.png" alt="1-4">
<figcaption aria-hidden="true">1-4</figcaption>
</figure>
<p>局部最优并不代表全局最优</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250622221246041.png" alt="1-5">
<figcaption aria-hidden="true">1-5</figcaption>
</figure>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250622221913906.png" alt="1-6">
<figcaption aria-hidden="true">1-6</figcaption>
</figure>
<p>对彼此的最佳对策被称为纳什均衡</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250622222258083.png" alt="1-7">
<figcaption aria-hidden="true">1-7</figcaption>
</figure>
<p>纳什均衡可以有多个</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250622222540677.png" alt="1-8">
<figcaption aria-hidden="true">1-8</figcaption>
</figure>
<p>博弈定义</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250622223016230.png" alt="1-9">
<figcaption aria-hidden="true">1-9</figcaption>
</figure>
<p>占优策略：对于一个玩家来说，拥有一个策略a，不论其他玩家选择任何策略，选择策略a都会使得他的效用u最大，这个策略a就被称为占优策略</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250622223222044.png" alt="1-10">
<figcaption aria-hidden="true">1-10</figcaption>
</figure>
<p>n-玩家纳什均衡定义</p>
<p>q2：是不是所有博弈情况都存在纳什均衡？</p>
<ol start="3" type="1">
<li><p>混合策略纳什均衡</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250622225345099.png" alt="1-11">
<figcaption aria-hidden="true">1-11</figcaption>
</figure></li>
</ol>
<h2 id="博弈类型">博弈类型</h2>
<h3 id="重复博弈">重复博弈</h3>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250623201854412.png" alt="2-1">
<figcaption aria-hidden="true">2-1</figcaption>
</figure>
<ul>
<li><p>为什么要研究重复博弈？</p>
<p>生活中我们参与的许多战略互动都是持续进行的，比如，我们会与相同的人重复互动等。重复博弈理论提供了一个框架用以研究这种重复行为</p></li>
</ul>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250623202222337.png" alt="2-2">
<figcaption aria-hidden="true">2-2</figcaption>
</figure>
<ul>
<li>重复博弈定义：同一个基础博弈（阶段博弈）被相同的参与者多次重复进行。换句话说，在重复博弈中，一个标准式博弈被同样的参与者反复进行。</li>
</ul>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250623202435065.png" alt="2-3">
<figcaption aria-hidden="true">2-3</figcaption>
</figure>
<ul>
<li><p>惩罚的威胁：</p>
<p>惩罚的威胁是理解重复博弈引入折扣因子的关键概念。基本思想是参与者可能会因为<strong>“惩罚”的“威胁”</strong>而被阻止利用其短期优势，这种威胁的直观效果就是会降低其长期收益。</p>
<p>通过未来惩罚的威胁来维持合作。参与者在做决策时不仅要考虑当前收益，还要考虑背叛行为可能招致的未来惩罚，从而在长期利益考量下选择合作而非短期的机会主义行为。</p></li>
</ul>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250623202916496.png" alt="2-4">
<figcaption aria-hidden="true">2-4</figcaption>
</figure>
<ul>
<li><p>重复博弈：囚徒困境</p>
<p>在单次博弈中，背叛是占优策略，但在重复博弈中，未来惩罚的威胁可能使合作成为可能。这为分析现实中的长期合作关系提供了理论基础。</p></li>
</ul>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250623203105804.png" alt="2-5">
<figcaption aria-hidden="true">2-5</figcaption>
</figure>
<ul>
<li><p>囚徒困境有一个唯一的纳什均衡：每个参与者都选D（背叛）</p></li>
<li><p>现在引入冷酷触发策略：</p>
<ul>
<li><p>只要对方参与者选择C，就选择C；</p></li>
<li><p>如果在任何时期对方参与者选择D，那么在<strong>此后的每个时期</strong>都选择D</p></li>
</ul></li>
<li><p>另一个参与者应该怎么做？</p>
<ul>
<li>只要她对未来收益的重视程度与当前收益相比不是太小，她最好在每个时期都选择C</li>
</ul></li>
<li><p>冷酷触发策略的工作原理：通过”一旦背叛，永远惩罚”的威胁来维持合作。关键在于参与者对未来收益的重视程度（贴现因子）必须足够高，使得长期合作的收益超过短期背叛的收益。</p></li>
</ul>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250623203455003.png" alt="2-6">
<figcaption aria-hidden="true">2-6</figcaption>
</figure>
<ul>
<li>每次都选择C的策略是应对冷酷触发策略最好的策略</li>
<li>为什么？
<ul>
<li>如果她在每个时期都选择C，那么每个时期的结果都是(C,C)，她在每个时期获得收益2</li>
<li>如果她在某个时期转向D，那么她在该时期获得收益3，在此后的每个时期获得收益1</li>
<li>然而，只要她对未来收益的重视程度与当前收益相比不是太小，收益流(3,1,1,…)对她来说比收益流(2,2,2,…)更差
• 因此她最好在每个时期都选择C</li>
</ul></li>
<li>为什么冷酷触发策略能够维持合作：虽然背叛能带来一次性的更高收益（3 vs
2），但随后的永久惩罚（每期收益1）使得总体收益低于持续合作（每期收益2）。关键条件是参与者必须足够重视未来收益。</li>
</ul>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250623203658761.png" alt="2-7">
<figcaption aria-hidden="true">2-7</figcaption>
</figure>
<ul>
<li>这张幻灯片指出了重复博弈中的多重均衡问题。除了通过冷酷触发策略维持的合作均衡外，还存在”总是背叛”的均衡。在这种均衡中，由于对方无论如何都会背叛，自己也最好选择背叛，这与单次囚徒困境的结果相同。这说明<strong>重复博弈可能存在多个纳什均衡</strong>，既有合作的也有非合作的。</li>
</ul>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250623203835329.png" alt="2-8">
<figcaption aria-hidden="true">2-8</figcaption>
</figure>
<ul>
<li><p>两个关键问题：</p>
<ul>
<li><p><strong>耐心程度的量化</strong>：要维持合作均衡，参与者需要多重视未来收益？这涉及贴现因子的临界值计算。</p></li>
<li><p><strong>均衡结果的多样性</strong>：除了完全合作(C,C)和完全背叛(D,D)之外，还有哪些策略组合和结果可以构成纳什均衡？</p></li>
</ul></li>
</ul>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250623204324562.png" alt="2-9">
<figcaption aria-hidden="true">2-9</figcaption>
</figure>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250623204526705.png" alt="2-10">
<figcaption aria-hidden="true">2-10</figcaption>
</figure>
<ul>
<li>通过冷酷触发策略结合折扣因子来解释折扣因子的边界取值。分别是一直选择合作（C）以及中途换选择（D），分别计算其策略收益，最后比较收益值，即可计算出折扣因子的边界值。</li>
<li><strong>当δ &lt; 1/2时，“一直背叛”是纳什均衡</strong>：
<ul>
<li>无论δ值如何，“一直背叛”策略<strong>总是纳什均衡</strong>：
<ul>
<li>如果玩家1总是背叛，玩家2的最佳响应是总是背叛（因为如果玩家2合作，支付为0；如果背叛，支付为1）。</li>
<li>同样，如果玩家2总是背叛，玩家1的最佳响应也是总是背叛。</li>
<li>支付为每期(1,1)，现值为11−δ1−<em>δ</em>1。</li>
<li>没有玩家能通过单方面改变策略（如尝试合作）获得更高支付，因为合作会被立即剥削（支付0），且未来收益折现后不足以补偿。</li>
</ul></li>
<li>这个均衡总是存在，但它导致<strong>帕累托低效的结果</strong>（支付(1,1)低于合作时的(2,2)）。</li>
</ul></li>
</ul>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250623204705478.png" alt="2-11">
<figcaption aria-hidden="true">2-11</figcaption>
</figure>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250623204723342.png" alt="2-12">
<figcaption aria-hidden="true">2-12</figcaption>
</figure>
<ul>
<li>有限步惩罚策略与“以牙还牙”策略计算折扣因子边界值思想与冷酷触发策略计算类似，这里不再详细解释。</li>
</ul>
<h3 id="扩展形式博弈">扩展形式博弈</h3>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250623210315029.png" alt="2-13">
<figcaption aria-hidden="true">2-13</figcaption>
</figure>
<ul>
<li>上述内容对比了策略型博弈和扩展式博弈的核心区别，并定义了一种特定类型的扩展式博弈<strong>（多阶段可观测行动博弈）</strong>。以下是我对关键点的理解：
<ol type="1">
<li><strong>核心区别 (Sequentiality &amp; Information):</strong>
<ul>
<li><strong>策略型博弈 (战略式博弈):</strong>
强调<strong>同时决策</strong>。玩家在不知道对手选择的情况下做出一次性决策（如石头剪刀布、静态
Cournot 模型）。收益矩阵是其主要表示形式。</li>
<li><strong>扩展式博弈 (扩展式):</strong> 强调<strong>行动的先后顺序
(序列性)</strong>
和<strong>信息结构</strong>。玩家在不同时间点行动，并且后行动的玩家可能（但不一定）能观察到先行动玩家的选择（如象棋、动态
Stackelberg 模型、序贯议价）。博弈树是其核心表示工具。</li>
</ul></li>
<li><strong>关注类型 (Multi-stage with Observed Actions):</strong>
<ul>
<li><strong>多阶段 (Multi-stage):</strong>
博弈过程被划分为不同的阶段。</li>
<li><strong>可观测行动 (Observed Actions):</strong>
这是定义中<strong>最关键的信息假设</strong>。它意味着在每个阶段开始时，<strong>所有玩家都完全知道之前所有阶段中所有玩家选择的所有行动</strong>。这被称为<strong>完美信息
(Perfect
Information)</strong>，但PPT的表述更一般化，因为它允许同一阶段内的玩家<strong>同时行动
(Simultaneous Moves)</strong>。</li>
<li><strong>完美信息 vs. 可观测行动：</strong>
“完美信息”通常指在<em>每个决策点</em>，玩家确切知道之前发生的<em>所有</em>行动（即知道整个历史，知道当前处于哪个决策节点）。PPT定义的“具有可观测行动的多阶段博弈”在阶段之间是完美信息的（玩家知道之前所有阶段的所有行动），但在一个阶段内部，如果存在同时行动，则在该阶段内行动时，玩家可能不知道同阶段其他玩家的<em>即时</em>选择（但在下一阶段开始前，这些行动会被揭示）。这比严格的“完美信息”博弈（要求每个决策点都无同时行动且完全知晓历史）范围更广。</li>
</ul></li>
<li><strong>表示工具 (Game Trees):</strong>
<ul>
<li>博弈树是表示扩展式博弈最直观的方式。树根代表起点，树枝代表玩家可能的行动，树节点代表决策点（轮到哪个或哪些玩家行动），树叶代表终点（对应收益/结果）。</li>
<li>它天然地刻画了<strong>行动顺序</strong>和可能的<strong>路径
(历史)</strong>。</li>
</ul></li>
<li><strong>关键概念 (Histories):</strong>
<ul>
<li><strong>历史记录 (Histories)</strong>
是扩展式博弈模型中的<strong>基础构件</strong>。一个历史就是一个从博弈开始到某个时间点为止，所有玩家按顺序采取的行动的序列。</li>
<li>每个决策点（博弈树节点）都对应一个<strong>唯一的历史</strong>，该历史描述了到达该节点所经过的路径。</li>
<li>玩家的<strong>信息集 (Information Sets)</strong>
通常由其无法区分的一组历史（节点）来定义。但在PPT定义的“具有可观测行动的多阶段博弈”中，由于行动完全可观测，每个信息集通常只包含一个节点（即玩家总是确切知道自己处于哪个决策点），除非在同一个阶段内存在同时行动（此时玩家可能不知道同阶段对手的<em>即时</em>选择）。</li>
</ul></li>
</ol></li>
</ul>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250623211203839.png" alt="2-14">
<figcaption aria-hidden="true">2-14</figcaption>
</figure>
<ul>
<li><strong>玩家角色：</strong>
<ul>
<li><strong>玩家1 - 进入者 (Entrant):</strong>
考虑是否进入一个新市场（通常是一个已有在位企业的市场）。</li>
<li><strong>玩家2 - 在位者 (Incumbent):</strong>
是市场现有的主导企业，对进入者的行动做出反应。</li>
</ul></li>
<li><strong>行动顺序与信息：</strong>
<ul>
<li>这是一个<strong>序贯博弈</strong>：玩家1先行动，玩家2后行动。</li>
<li><strong>关键信息假设：</strong>
玩家2在做出决策（容纳还是斗争）之前，<strong>完全观察到了玩家1的选择</strong>（进入或不进入）。这意味着玩家2知道博弈进行到了哪个决策点（即玩家1选了哪个行动）。</li>
<li>这符合之前定义的“<strong>具有可观测行动</strong>”的多阶段博弈。在这里，只有一个阶段玩家1行动，紧接着一个阶段玩家2行动，且玩家2的行动是基于完全知晓玩家1行动的情况下做出的。</li>
</ul></li>
<li><strong>博弈树表示：</strong>
<ul>
<li>这个例子非常适合用<strong>博弈树</strong>来表示：
<ul>
<li><strong>根节点 (Root):</strong> 玩家1的决策点（进入 /
不进入）。</li>
<li><strong>中间节点 (Decision Nodes):</strong>
玩家1选择“进入”后，会到达玩家2的决策点（容纳 /
斗争）。玩家1选择“不进入”后，博弈直接结束。</li>
<li><strong>叶节点 / 终点节点 (Terminal Nodes / Leaves):</strong>
代表博弈结束的点，标有收益向量 ((x,
y))。每个叶节点对应一个<strong>完整的历史</strong>（行动序列）和最终的收益结果。</li>
</ul></li>
</ul></li>
<li><strong>收益：</strong>
<ul>
<li>收益 ((x, y))
的具体数值决定了博弈的结果和均衡。虽然没有给出具体数字，但典型的设定可能是：
<ul>
<li><code>(不进入)</code>:
进入者收益为0（无成本无收入），在位者收益较高（维持垄断利润）。</li>
<li><code>(进入, 容纳)</code>:
进入者获得正利润（但低于垄断利润），在位者利润下降（但仍为正，因为共享市场）。</li>
<li><code>(进入, 斗争)</code>:
进入者亏损（因在位者发起价格战等），在位者也亏损（价格战成本）。虽然进入者损失可能更大，但斗争通常对双方都不利，是两败俱伤的结果。</li>
</ul></li>
<li>玩家2（在位者）的决策取决于哪种行动（容纳或斗争）在给定玩家1已进入的前提下，能给他带来更高的收益
(y)。</li>
</ul></li>
<li>要分析这个博弈的均衡（例如，子博弈精炼纳什均衡），就需要具体设定收益值，并逆向归纳求解玩家2在观察到“进入”后的最优反应，以及玩家1预测到玩家2的最优反应后，最初是否选择“进入”。</li>
</ul>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250623212809384.png" alt="2-15">
<figcaption aria-hidden="true">2-15</figcaption>
</figure>
<ul>
<li>扩展型博弈的一些基本符号定义</li>
</ul>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250623213442013.png" alt="2-16">
<figcaption aria-hidden="true">2-16</figcaption>
</figure>
<ul>
<li><strong>纯策略 (Pure Strategy):</strong>
<ul>
<li>在扩展式博弈中，纯策略不仅仅是玩家在博弈开始时的一个单一选择。它是一个<strong>完整的行动计划</strong>。</li>
<li>策略必须规定玩家在<strong>博弈的每一个可能阶段
(k)</strong>，面对<strong>每一个可能到达该阶段的历史路径 (hᵏ ∈
Hᵏ)</strong> 时，他会选择哪个可用的行动 (sᵢᵏ(hᵏ) ∈ Sᵢ(hᵏ)。</li>
<li><strong>为什么需要这么复杂？</strong>
<ul>
<li>因为博弈是序贯的，并且玩家在决策时可能面临不同的局面（由不同的历史
hᵏ 描述）。一个完整的策略必须说明玩家在 <em>所有可能遇到的情况</em>
下会怎么做，即使某些情况在博弈实际进行中可能不会发生（如果玩家遵循这个策略的话）。</li>
</ul></li>
<li>形式上，玩家 i 的策略 sᵢ 是一个<strong>函数集合</strong> {sᵢ⁰, sᵢ¹,
…, sᵢᴷ}，其中每个函数 sᵢᵏ 将阶段 k 的<em>历史集合 Hᵏ</em>
映射到该玩家在该历史下<em>可用的行动集合 Sᵢ(Hᵏ)</em>
中的一个具体行动。</li>
</ul></li>
<li><strong>策略组合生成博弈路径:</strong>
<ul>
<li>当所有玩家都选定他们的纯策略 (s₁, s₂, …, sₗ)
后，博弈的实际进行路径就被唯一确定了。</li>
<li>路径是通过<strong>递归应用</strong>所有玩家的策略函数来生成的：
<ul>
<li><strong>阶段 0:</strong> 从初始历史 h⁰ = ∅
开始。所有玩家根据他们的策略 sᵢ⁰(∅) 选择行动，形成行动组合 a⁰ = (s₁⁰(∅),
s₂⁰(∅), …, sₗ⁰(∅))。阶段 0 后的历史变为 h¹ = a⁰。</li>
<li><strong>阶段 1:</strong> 面对历史 h¹ = a⁰。所有玩家根据他们的策略
sᵢ¹(a⁰) 选择行动，形成行动组合 a¹ = (s₁¹(a⁰), s₂¹(a⁰), …, sₗ¹(a⁰))。阶段
1 后的历史变为 h² = (a⁰, a¹)。</li>
<li><strong>后续阶段:</strong> 以此类推，直到最终阶段 K。阶段 K 后的历史
hᴷ⁺¹ = (a⁰, a¹, …, aᴷ) 就是终端历史。</li>
</ul></li>
<li>策略组合 s <strong>完全决定了</strong>终端历史 hᴷ⁺¹。</li>
</ul></li>
<li><strong>收益 (Payoffs):</strong>
<ul>
<li>玩家的收益取决于博弈的最终<strong>结果
(outcome)</strong>，即终端历史 hᴷ⁺¹。</li>
<li>每个玩家 i 有一个<strong>效用函数 (utility function)</strong>
uᵢ，该函数将每个可能的终端历史 hᴷ⁺¹ 映射到一个实数，表示玩家 i
在该结果下获得的收益（或效用）。</li>
<li>由于策略组合 s 决定了终端历史 hᴷ⁺¹，因此我们也可以说策略组合 s
决定了每个玩家的收益，记作 uᵢ(s)。uᵢ(s) 本质上是 uᵢ 在由 s
决定的那个特定终端历史 hᴷ⁺¹ 上的取值。</li>
</ul></li>
<li>扩展式博弈中的核心概念：
<ul>
<li><strong>纯策略:</strong>
是玩家针对<strong>所有可能历史</strong>制定的完整应变计划，表现为一组映射函数
{sᵢᵏ}，每个函数为特定阶段 k 的每个可能历史 hᵏ 指定一个行动。</li>
<li><strong>策略组合决定路径:</strong>
当所有玩家选定策略后，博弈的路径（行动序列 a⁰, a¹, …,
aᴷ）和最终的终端历史 hᴷ⁺¹ 就被策略函数递归地、确定性地生成。</li>
<li><strong>收益定义在结果上:</strong>
玩家的收益由终端历史（博弈的最终结果）决定。效用函数 uᵢ
量化了玩家对每个可能结果的偏好。策略组合 s 通过决定终端历史 hᴷ⁺¹
来间接决定每个玩家的收益 uᵢ(s)。</li>
</ul></li>
<li>理解纯策略的这种“完备应变计划”性质对于分析扩展式博弈的均衡（如子博弈精炼纳什均衡）至关重要，因为它要求玩家即使在“偏离均衡路径”的历史下（即如果博弈意外地到达了那里），也要指定一个行动。</li>
</ul>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250623214724248.png" alt="2-17">
<figcaption aria-hidden="true">2-17</figcaption>
</figure>
<ul>
<li><strong>关键总结 (PPT最后一句):</strong></li>
</ul>
<blockquote>
<p>一个玩家的策略规定了该玩家在其<strong>轮到行动的每一个历史</strong>（例如，玩家2在<code>&#123;C&#125;</code>之后或<code>&#123;D&#125;</code>之后）处所选择的行动。</p>
</blockquote>
<ul>
<li>玩家 1 的策略只需要规定在唯一的历史 <code>∅</code> 处选择
<code>C</code> 或 <code>D</code>。</li>
<li>玩家 2 的策略必须规定在 <em>两个</em> 可能的历史 <code>&#123;C&#125;</code> 和
<code>&#123;D&#125;</code> 处分别选择什么行动（<code>E/F</code> 和
<code>G/H</code> 的组合）。</li>
</ul>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250624002522709.png" alt="2-18">
<figcaption aria-hidden="true">2-18</figcaption>
</figure>
<ul>
<li><p>在给定的扩展式博弈及其转换后的标准式（策略型）博弈中，策略组合（L,
RL）不是纳什均衡（Nash
equilibrium）。以下基于提供的收益矩阵和纳什均衡的定义，逐步解释原因。</p></li>
<li><p><strong>博弈的收益矩阵</strong>：</p>
<table>
<thead>
<tr>
<th>玩家1  玩家2</th>
<th>LL</th>
<th>LR</th>
<th>RL</th>
<th>RR</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>L</strong></td>
<td>3,2</td>
<td>3,2</td>
<td>2,3</td>
<td>2,3</td>
</tr>
<tr>
<td><strong>R</strong></td>
<td>4,1</td>
<td>0,1</td>
<td>4,1</td>
<td>0,1</td>
</tr>
</tbody>
</table></li>
<li><p><strong>玩家2的策略含义</strong>（RL策略）：</p>
<ul>
<li>RL =
如果玩家1选择L，则玩家2选择R；如果玩家1选择R，则玩家2选择L。</li>
</ul></li>
<li><p><strong>策略组合（L, RL）的含义</strong>：</p>
<ul>
<li>玩家1选择L。</li>
<li>玩家2选择RL策略（因此，当玩家1选L时，玩家2选R）。</li>
<li>实际发生的行动路径：玩家1选L → 玩家2选R。</li>
</ul></li>
<li><p>对应收益：从收益矩阵中，行L、列RL的单元格为（2,3），即玩家1收益为2，玩家2收益为3。</p></li>
<li><p><strong>纳什均衡的定义</strong></p>
<ul>
<li>纳什均衡要求：在给定其他玩家的策略下，没有任何一个玩家能通过单方面改变自己的策略而获得更高的收益。也就是说：</li>
<li>玩家1的策略必须是对玩家2策略的最优响应（best response）。</li>
<li>玩家2的策略必须是对玩家1策略的最优响应。
如果任何一个玩家有激励偏离当前策略，则该组合不是纳什均衡。</li>
</ul></li>
<li><p><strong>为什么（L, RL）不是纳什均衡？</strong></p>
<p>在策略组合（L,
RL）下，玩家1有激励单方面改变策略。具体分析如下：</p></li>
</ul>
<ol type="1">
<li><strong>给定玩家2的策略RL，玩家1的收益比较</strong>：
<ul>
<li>如果玩家1保持选择L（当前策略）：
<ul>
<li>玩家2的RL策略规定：当玩家1选L时，玩家2选R。</li>
<li>因此，收益为（2,3），玩家1获得收益<strong>2</strong>。</li>
</ul></li>
<li>如果玩家1单方面改为选择R：
<ul>
<li>玩家2的RL策略规定：当玩家1选R时，玩家2选L（因为RL策略在玩家1选R时对应选L）。</li>
<li>因此，行动路径为（R,
L），收益为（4,1）（从收益矩阵的行R、列RL单元格可得）。</li>
<li>玩家1获得收益<strong>4</strong>。</li>
</ul></li>
</ul></li>
<li><strong>玩家1的激励分析</strong>：
<ul>
<li>玩家1的收益从<strong>2</strong>（选L）变为<strong>4</strong>（选R），收益增加（4
&gt; 2）。</li>
<li>因此，玩家1有严格激励（strict
incentive）偏离策略L，改为选择R。因为收益更高，且这是单方面改变（玩家2的策略RL保持不变）。</li>
</ul></li>
<li><strong>玩家2的响应（虽不必要，但完整性分析）</strong>：
<ul>
<li>在（L, RL）下，玩家2的收益为3（给定玩家1选L，玩家2选R）。</li>
<li>如果玩家1保持选L，玩家2改变策略（如改为LL、LR或RR）：
<ul>
<li>例如，改为LL：当玩家1选L时，玩家2选L，收益为2（行L、列LL单元格为（3,2），玩家2收益2
&lt; 3）。</li>
<li>改为LR：当玩家1选L时，玩家2选L，收益为2（行L、列LR单元格为（3,2），玩家2收益2
&lt; 3）。</li>
<li>改为RR：当玩家1选L时，玩家2选R，收益为3（与当前相同）。</li>
<li>因此，玩家2无严格激励偏离RL（因为改变策略要么收益降低，要么不变），但这不是关键，因为玩家1的偏离已足够破坏均衡。</li>
</ul></li>
</ul></li>
</ol>
<ul>
<li><strong>为什么PPT中列出的（R, RL）是纳什均衡，而（L,
RL）不是？</strong>
<ul>
<li><strong>（R, RL）是纳什均衡</strong>（如PPT所列）：
<ul>
<li>玩家1选R，玩家2选RL（当玩家1选R时，玩家2选L）。</li>
<li>收益为（4,1）。</li>
<li>给定玩家2的RL策略，玩家1：选R收益4，选L收益2（因为如果玩家1选L，玩家2选R，收益2），4
&gt; 2，因此玩家1无激励偏离。</li>
<li>给定玩家1选R，玩家2：任何改变（如改为LL、LR或RR），在玩家1选R时收益均为1（无严格增加），因此玩家2无激励偏离。</li>
<li>所以（R, RL）满足纳什均衡条件（弱均衡）。</li>
</ul></li>
<li><strong>（L, RL）不是纳什均衡</strong>：
<ul>
<li>如上所述，玩家1有严格激励偏离L到R，因此不满足“无玩家有激励偏离”的条件。</li>
</ul></li>
</ul></li>
<li><strong>总结</strong>
<ul>
<li><strong>（L,
RL）不是纳什均衡，因为玩家1可以通过单方面改变策略（从L到R）将收益从2提高到4。</strong>
给定玩家2的RL策略，玩家1选择L不是最优响应。</li>
<li>这体现了纳什均衡的核心要求：每个玩家的策略必须是对其他玩家策略的最优响应。在扩展式博弈中，这种分析也揭示了为什么有些策略组合在标准式中看似可行，但因动态不一致而被排除（如这里玩家1的偏离激励）。</li>
</ul></li>
</ul>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250624003714695.png" alt="2-19">
<figcaption aria-hidden="true">2-19</figcaption>
</figure>
<p>这张PPT探讨了进入博弈的均衡合理性，通过扩展式和标准式表示揭示了纳什均衡的局限性。以下是详细分析：</p>
<hr>
<p><strong>博弈结构与收益</strong></p>
<p><strong>扩展式表示（博弈树）：</strong></p>
<ul>
<li><strong>玩家1（进入者）</strong>：先行动，选择：
<ul>
<li><strong>In</strong>（进入市场）</li>
<li><strong>Out</strong>（不进入市场）→ 收益 (1,2)</li>
</ul></li>
<li><strong>玩家2（在位者）</strong>：观察到进入者行动后选择：
<ul>
<li><strong>A</strong>（容纳）→ 收益 (2,1)</li>
<li><strong>F</strong>（斗争）→ 收益 (0,0)</li>
</ul></li>
</ul>
<p><strong>标准式表示（收益矩阵）：</strong></p>
<table>
<thead>
<tr>
<th>进入者  在位者</th>
<th>容纳 (A)</th>
<th>斗争 (F)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>进入 (In)</strong></td>
<td>(2,1)</td>
<td>(0,0)</td>
</tr>
<tr>
<td><strong>不进入 (Out)</strong></td>
<td>(1,2)</td>
<td>(1,2)</td>
</tr>
</tbody>
</table>
<hr>
<p><strong>纳什均衡分析</strong></p>
<ol type="1">
<li><strong>(In, A)</strong>：
<ul>
<li>进入者收益：2（若改为Out，收益1&lt;2）→
<strong>无偏离激励</strong></li>
<li>在位者收益：1（若改为F，收益0&lt;1）→
<strong>无偏离激励</strong><br>
✅ <strong>是纳什均衡</strong></li>
</ul></li>
<li><strong>(Out, F)</strong>：
<ul>
<li>进入者收益：1（若改为In，在位者选F则收益0&lt;1）→
<strong>无偏离激励</strong></li>
<li>在位者收益：2（无论选A或F，收益均为2）→
<strong>无偏离激励</strong><br>
✅ <strong>是纳什均衡（弱均衡）</strong></li>
</ul></li>
</ol>
<hr>
<p><strong>均衡合理性检验</strong></p>
<p><strong>为什么 (Out, F)不合理？</strong></p>
<p>尽管 <strong>(Out, F)</strong>
是纳什均衡，但它在动态博弈中<strong>不可信</strong>（缺乏子博弈精炼性）：
1. <strong>在位者的空威胁</strong>： -
在位者声称“若你进入，我会斗争（F）”，但若进入者真的选择
<strong>In</strong>： - 在位者选 <strong>A</strong> 收益为
<strong>1</strong> - 在位者选 <strong>F</strong> 收益为
<strong>0</strong>（更差） → 理性在位者实际会选择
<strong>A</strong>，而非 <strong>F</strong>。</p>
<ol start="2" type="1">
<li><strong>进入者的理性决策</strong>：
<ul>
<li>进入者知道在位者的威胁不可信（一旦进入，在位者必选
<strong>A</strong>）。</li>
<li>因此进入者应选择 <strong>In</strong>（收益2 &gt;
选Out的收益1）。</li>
</ul></li>
</ol>
<p><strong>逆向归纳验证：</strong></p>
<ol type="1">
<li>若进入者选 <strong>In</strong>，在位者在子博弈中：
<ul>
<li>选 <strong>A</strong> → 收益 <strong>1</strong></li>
<li>选 <strong>F</strong> → 收益 <strong>0</strong><br>
→ 最优选择是 <strong>A</strong>。</li>
</ul></li>
<li>进入者预判：
<ul>
<li>选 <strong>In</strong> → 收益
<strong>2</strong>（因在位者会选A）</li>
<li>选 <strong>Out</strong> → 收益 <strong>1</strong><br>
→ 最优选择是 <strong>In</strong>。</li>
</ul></li>
</ol>
<hr>
<p><strong>结论</strong></p>
<table>
<thead>
<tr>
<th>均衡</th>
<th>合理性</th>
<th>原因</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>(In, A)</strong></td>
<td>✅ 合理</td>
<td>威胁可信，符合序贯理性</td>
</tr>
<tr>
<td><strong>(Out, F)</strong></td>
<td>❌ 不合理</td>
<td>依赖不可信的空威胁（动态不一致）</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>核心问题</strong>：纳什均衡在扩展式博弈中可能包含<strong>不可信威胁</strong>，需通过<strong>子博弈精炼纳什均衡</strong>（逆向归纳法）剔除不合理的均衡。</li>
<li><strong>本例唯一合理均衡</strong>：<strong>(In,
A)</strong>，即进入者进入，在位者容纳。</li>
</ul>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250624004513150.png" alt="2-20">
<figcaption aria-hidden="true">2-20</figcaption>
</figure>
<ul>
<li>回顾匹配硬币博弈（matching pennies game）的两阶段扩展式版本。</li>
<li>在这个博弈中，存在两个<strong>真子博弈（proper subgames）</strong>
以及博弈本身（它也是一个子博弈），因此总共有<strong>三个子博弈</strong>。</li>
</ul>
<blockquote>
<p><strong>定义：</strong>
在一个（完美信息）博弈树中，<strong>每一个节点</strong>，连同从该节点到达之后所剩余的博弈部分，被称为一个<strong>子博弈（subgame）</strong>。<br>
即，每一个<strong>非终端历史（non-terminal history）</strong> ( h )
都对应一个子博弈。</p>
</blockquote>
<hr>
<p><strong>关键概念理解：</strong></p>
<ol type="1">
<li><p><strong>子博弈 (Subgame):</strong></p>
<ul>
<li>子博弈是<strong>原博弈的一部分</strong>，它起始于博弈树中的<strong>某个单一决策节点</strong>（该节点代表一个特定的非终端历史
( h^k
)），并<strong>包含该节点之后的所有后续决策节点、行动分支和终端节点</strong>。</li>
<li>它本身必须构成一个<strong>完整的、独立的博弈</strong>，拥有明确的起点（该决策节点）、后续行动规则和最终收益。</li>
<li>子博弈继承了原博弈的所有规则（玩家、行动、信息、收益函数）。</li>
</ul></li>
<li><p><strong>非终端历史 (Non-terminal History):</strong></p>
<ul>
<li>指那些<strong>不是博弈最终结果</strong>的历史 ( h^k )（即 ( k &lt;
K+1 )，其中 ( K+1 ) 是终端历史的索引）。</li>
<li>每个这样的历史 ( h^k )
都标志着博弈进行到了一个<strong>尚未结束的决策点</strong>。</li>
<li><strong>关键联系：</strong> 每个非终端历史 ( h^k )
都<strong>唯一确定了一个子博弈的起始点</strong>。这个子博弈就是从历史 (
h^k ) 所对应的那个决策节点开始的剩余游戏部分。</li>
</ul></li>
<li><p><strong>子博弈的数量:</strong></p>
<ul>
<li>在<strong>完美信息（perfect information）</strong>
博弈（即每个信息集只包含一个节点）中，子博弈的数量等于<strong>非终端决策节点的数量</strong>。</li>
<li>示例中提到匹配硬币两阶段博弈有<strong>三个子博弈</strong>：
<ul>
<li><strong>子博弈 1：</strong>
起始于<strong>玩家1的第一个决策节点</strong>（对应初始历史 ( h^0 =
)）。<strong>这就是整个博弈本身。</strong></li>
<li><strong>子博弈 2：</strong>
起始于<strong>玩家2在玩家1选择“正面”之后的决策节点</strong>（对应历史 (
h^1 = (Heads) )）。</li>
<li><strong>子博弈 3：</strong>
起始于<strong>玩家2在玩家1选择“反面”之后的决策节点</strong>（对应历史 (
h^1 = (Tails) )）。</li>
</ul></li>
<li>后两个是<strong>真子博弈（proper
subgames）</strong>，因为它们严格包含在原博弈之内且不等于原博弈。</li>
</ul></li>
<li><p><strong>为什么子博弈概念重要？</strong></p>
<ul>
<li><strong>均衡精炼：</strong>
子博弈概念是定义<strong>子博弈精炼纳什均衡（Subgame Perfect Nash
Equilibrium, SPNE）</strong> 的核心。SPNE
要求均衡策略不仅在原博弈上构成纳什均衡，而且在<strong>每一个可能的子博弈</strong>上也构成纳什均衡。这旨在剔除那些依赖<strong>不可信威胁或承诺</strong>的纳什均衡（例如之前进入博弈中的
<code>(Out, F)</code> 均衡）。</li>
<li><strong>分析工具：</strong>
子博弈结构使得我们可以使用<strong>逆向归纳法（backward
induction）</strong> 来求解完美信息有限博弈的
SPNE。我们从最小的子博弈（最接近终点的）开始求解，将其解（收益或最优行动）代入其父节点，再逐步倒推回根节点。</li>
<li><strong>模块化分析：</strong>
允许将大型复杂博弈分解为更小的、可管理的子部分进行分析。</li>
</ul></li>
<li><p><strong>子博弈求解方法解析</strong></p>
<p>https://zhuanlan.zhihu.com/p/199055190</p></li>
</ol>
<h3 id="势博弈">势博弈</h3>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250624194410787.png" alt="2-21">
<figcaption aria-hidden="true">2-21</figcaption>
</figure>
<ul>
<li><strong>请注意，对于所有厂商 i 和所有 q−i&gt;0 的情况：</strong>
<ul>
<li>ui(qi,q−i)−ui(qi′,q−i)&gt;0 <strong>当且仅当 (iff)</strong>
Φ(qi,q−i)−Φ(qi′,q−i)&gt;0 ，此条件对于所有 qi,qi′&gt;0 均成立。</li>
<li>说明：
<ul>
<li>这条关键的陈述指出，在其他厂商产量不变的情况下，厂商 i 从产量 qi′
转换到 qi 能否增加自身利润，与这个转换能否增加函数 Φ
的值，两者是完全等价的。</li>
</ul></li>
</ul></li>
</ul>
<p>这张PPT的核心思想是为古诺竞争模型引入一个称为<strong>“势函数”(Potential
Function)</strong> 的概念，并证明此模型是一个<strong>“势博弈”(Potential
Game)</strong>。</p>
<ol type="1">
<li><p>什么是古诺竞争？</p>
<p>这是一个经典的经济学模型，描述了在一个寡占市场中，几家厂商如何进行产量竞争。每家厂商都假设其竞争对手的产量是固定的，然后选择能让自己利润最大化的产量。当所有厂商都达到一个状态，即没有任何一家厂商可以单方面改变产量来增加自己的利润时，市场就达到了纳什均衡
(Nash Equilibrium)。</p></li>
<li><p>为什么要引入奇怪的函数 Φ？</p>
<p>支付函数 ui 非常直观，它就是厂商 i 的利润。然而，函数 Φ
看起来很抽象，它将“所有厂商产量的乘积”与“单位利润”相乘，并没有直接的经济学意义。</p>
<p>它的真正价值在于其数学性质。投影片的最后一条结论揭示了这个性质：任何单一厂商
i 调整策略 (产量 qi) 所带来的自身利润变化方向，都和函数 Φ
的变化方向完全一致。</p></li>
<li><p>这意味着什么？(核心理解)</p>
<p>这意味着，原本一个复杂的多人博弈问题（每个厂商都在最大化自己的
ui），可以被转化为一个相对简单的<strong>“寻找单一函数 Φ
最大值”</strong>的问题。</p>
<ul>
<li><strong>简化分析</strong>：我们不再需要同时追踪 I
个不同厂商的利润函数，只需要分析一个共同的“势函数”Φ 即可。</li>
<li><strong>保证均衡存在</strong>：在势博弈中，势函数 Φ
的局部最大值点对应着该博弈的纳什均衡。因为一个有界的连续函数必然存在最大值，这就为证明古诺均衡的存在性提供了一条优雅的路径。</li>
<li><strong>收敛性</strong>：势博弈还有一个重要特性，即如果玩家们轮流进行“最优反应”（每次都选择当下能让自己利润最大化的策略），这个过程最终一定会收敛到一个纳什均衡。函数
Φ
就像一个山坡，玩家们的每一步调整都像是在爬坡，最终必然会走到一个山顶（均衡点）。</li>
</ul></li>
</ol>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250624200511139.png" alt="2-22">
<figcaption aria-hidden="true">2-22</figcaption>
</figure>
<p>这两张PPT分别介绍了<strong>序数势博弈</strong>和<strong>精确势博弈</strong>。</p>
<ol type="1">
<li><p>与“序数势”的核心区别</p>
<p>理解“精确势”的关键在于将其与上一张幻灯片中的“序数势”进行对比：</p>
<ul>
<li><strong>序数势 (Ordinal
Potential)</strong>：只要求参与者收益变化的<strong>方向</strong>与势函数变化的<strong>方向</strong>一致。
<ul>
<li>通俗地说：“只要我换策略能多赚钱，<code>Φ</code> 的值就一定会变大。”
它不关心你多赚了1块钱还是100块钱。</li>
</ul></li>
<li><strong>精确势 (Exact
Potential)</strong>：要求参与者收益变化的<strong>确切数值</strong>与势函数变化的<strong>确切数值</strong>完全相等。
<ul>
<li>通俗地说：“如果我换策略能多赚10块钱，<code>Φ</code>
的值就也必须不多不少，正好增加10。”</li>
</ul></li>
</ul></li>
<li><p>更严格的条件</p>
<p>显然，“精确势”是一个比“序数势”严格得多的条件。如果一个博弈是精确势博弈，那么它必然也是一个序数势博弈（因为如果两个改变量的数值相等，它们的正负号必然相同）。但反过来不成立，很多序数势博弈并不能满足精确势的苛刻条件。</p></li>
<li><p>指正幻灯片中的笔误</p>
<p>需要指出，幻灯片的最后一行存在一个明显的笔误。它写着 “G is called an
exact potential game if it admits an ordinal
potential.”（如果一个博弈拥有一个序数势，它就被称为一个精确势博弈）。这在逻辑上是错误的。</p>
<ul>
<li><strong>正确表述应为</strong>：“如果一个博弈 G
拥有一个<strong>精确势函数 (exact
potential)</strong>，那么它就被称为一个<strong>精确势博弈</strong>。”</li>
</ul></li>
</ol>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250624201207684.png" alt="2-23">
<figcaption aria-hidden="true">2-23</figcaption>
</figure>
<p>这张幻灯片通过一个具体的2x2矩阵博弈（囚徒困境的一个变体）的例子，非常直观地展示了“势函数”是如何运作的。这张幻灯片的核心目的是让我们<strong>验证</strong>所给出的矩阵
<code>P</code> 是否真的是博弈 <code>G</code> 的一个势函数。</p>
<p>我们可以通过检验定义来验证。让我们看看当某个参与者单方面改变策略时，他个人收益的变化量是否与势函数
<code>P</code> 的变化量相匹配。</p>
<p><strong>1. 验证过程</strong></p>
<p>我们分别检查行参与者（玩家1）和列参与者（玩家2）的决策。</p>
<ul>
<li><p><strong>检验玩家1（行选择者）</strong></p>
<ul>
<li><p>当玩家2选择“左”时</p>
<p>：玩家1在“上”（收益1）和“下”（收益0）之间选择。</p>
<ul>
<li>收益变化: u1(上, 左)−u1(下, 左)=1−0=1</li>
<li>势函数变化: P(上, 左)−P(下, 左)=4−3=1</li>
<li>两者<strong>完全相等</strong>。</li>
</ul></li>
<li><p>当玩家2选择“右”时</p>
<p>：玩家1在“上”（收益9）和“下”（收益6）之间选择。</p>
<ul>
<li>收益变化: u1(上, 右)−u1(下, 右)=9−6=3</li>
<li>势函数变化: P(上, 右)−P(下, 右)=3−0=3</li>
<li>两者<strong>完全相等</strong>。</li>
</ul></li>
</ul></li>
<li><p><strong>检验玩家2（列选择者）</strong></p>
<ul>
<li><p>当玩家1选择“上”时</p>
<p>：玩家2在“左”（收益1）和“右”（收益0）之间选择。</p>
<ul>
<li>收益变化: u2(上, 左)−u2(上, 右)=1−0=1</li>
<li>势函数变化: P(上, 左)−P(上, 右)=4−3=1</li>
<li>两者<strong>完全相等</strong>。</li>
</ul></li>
<li><p>当玩家1选择“下”时</p>
<p>：玩家2在“左”（收益9）和“右”（收益6）之间选择。</p>
<ul>
<li>收益变化: u2(下, 左)−u2(下, 右)=9−6=3</li>
<li>势函数变化: P(下, 左)−P(下, 右)=3−0=3</li>
<li>两者<strong>完全相等</strong>。</li>
</ul></li>
</ul></li>
</ul>
<p><strong>2. 结论</strong></p>
<p>由于<strong>任何</strong>参与者单方面改变策略所带来的收益变化，都<strong>精确地等于</strong>势函数
<code>P</code> 中对应数值的变化，因此，矩阵 <code>P</code> 是博弈
<code>G</code> 的一个<strong>精确势函数 (exact potential
function)</strong>。这个博弈 <code>G</code>
是一个<strong>精确势博弈</strong>。</p>
<p><strong>3. 势函数的威力：寻找纳什均衡</strong></p>
<p>这个例子的美妙之处在于，它展示了势函数如何简化均衡的寻找过程。寻找博弈
<code>G</code> 的纯策略纳什均衡，现在等价于寻找势函数矩阵 <code>P</code>
的“稳定点”。</p>
<p>观察势函数矩阵 P：</p>
<p>P=(4330)</p>
<p>矩阵中的最大值是4，位于（上，左）位置。让我们看看这个点是否稳定：</p>
<ul>
<li>如果从
<code>P=4</code>（上，左）出发，玩家1单方面移到“下”，<code>P</code>的值会从4变成3，他不会移动。</li>
<li>如果从
<code>P=4</code>（上，左）出发，玩家2单方面移到“右”，<code>P</code>的值会从4变成3，他也不会移动。</li>
</ul>
<p>因为没有任何一方有动机从（上，左）这个位置离开，所以它是一个纳什均衡。这对应于原博弈
<code>G</code>
中的（1,1）这个结果。这个例子清晰地表明，<strong>势函数的局部最大值（在此例中也是全局最大值）对应着博弈的一个纳什均衡</strong>。这正是势函数在博弈分析中的核心价值。</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250624201541524.png" alt="2-24">
<figcaption aria-hidden="true">2-24</figcaption>
</figure>
<p>有限序数势博弈的结果是有限的，势函数会为每一个博弈策略赋予一个值，因此必然存在一个最大值，这个最大值就是纳什均衡点。</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250624202953517.png" alt="2-25">
<figcaption aria-hidden="true">2-25</figcaption>
</figure>
<p><strong>1. 核心结论</strong></p>
<p>结论是最后一条：当厂商 <code>i</code> 单方面将自己的产量从
<code>q'_i</code> 变为 <code>q_i</code>
时，他<strong>个人利润的变化量</strong>，与我们构造的那个看起来很复杂的函数
<code>Φ*</code>
的<strong>变化量是完全相等的</strong>。这正是“精确势函数”的定义。</p>
<p><strong>2. 数学证明（为什么这个结论成立？）</strong></p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250624203148105.png" alt="2-26">
<figcaption aria-hidden="true">2-26</figcaption>
</figure>
<p>最后比较两个Δ即可</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250624204837963.png" alt="2-27">
<figcaption aria-hidden="true">2-27</figcaption>
</figure>
<p><strong>标题：有限序数势博弈中的简单动态 (Simple Dynamics in Finite
Ordinal Potential Games)</strong></p>
<p>定义 (Definition)</p>
<p>策略空间 S 中的一条路径 (path) 是一个策略向量序列
(s0,s1,⋯)，其中任意两个连续的策略只在一个坐标上不同（即，每一次恰好只有一名参与者改变了他的策略）。</p>
<p>一条<strong>改进路径 (improvement path)</strong> 是一条路径
(s0,s1,⋯)，它满足：</p>
<ul>
<li>uik(sk)&lt;uik(sk+1)，其中策略 sk 和 sk+1 在第 ik
个坐标上不同。换句话说，对于那个改变了策略的参与者来说，他的收益得到了改善。</li>
</ul>
<p>一条改进路径可以被认为是<strong>“短视参与者” (myopic
players)</strong> 动态生成的结果。</p>
<hr>
<p>这张幻灯片将我们的视角从静态的“均衡分析”转向了动态的“演化过程分析”。它为我们描述“一个博弈是如何一步步演变的”提供了正式的语言。这对于理解系统如何达到稳定状态至关重要。</p>
<p><strong>1. “路径”与“改进路径”的通俗解释</strong></p>
<ul>
<li><strong>路径
(Path)</strong>：你可以把它想象成一盘棋的“棋谱”或一个游戏的“回合记录”。它记录了博弈状态是如何一步步变化的，其核心规则是<strong>“一次只动一个”</strong>。在
sk 这一步，只有一名参与者会改变他的行动，从而进入 sk+1
状态。这是一种分析动态过程的合理简化。</li>
<li><strong>改进路径 (Improvement
Path)</strong>：这是一种特殊的、由理性驱动的路径。它不仅记录了系统的演变，还说明了<strong>为什么</strong>会这样演变。每一步的发生，都是因为某个参与者发现“如果我单方面改变策略，我的收益会立刻增加”，于是他就这么做了。这完美地描述了那些只顾眼前利益的参与者的行为。</li>
</ul>
<p><strong>2. “短视参与者” (Myopic Players) 的概念</strong></p>
<p>这是理解“改进路径”背后行为动机的关键。</p>
<ul>
<li><strong>“短视”</strong>意味着参与者们并不深谋远虑，他们不会去预测对手的对手的反应。</li>
<li>他们的决策逻辑非常简单：“在当前这个局面下，我有没有一个别的选择能让我马上赚得更多？”</li>
<li>如果答案是“有”，那么某个参与者就会采取行动，从而推动整个系统沿着“改进路径”向前走一步。这个过程可以被看作是一系列“更优反应
(better response)”的链式反应。</li>
</ul>
<p><strong>3. 为什么要在势博弈中讨论这个？（核心洞见）</strong></p>
<p>定义这些概念的最终目的，是为了引出势博弈最强大的性质之一：<strong>动态收敛性</strong>。</p>
<p>我们可以设想一下：</p>
<ol type="1">
<li><strong>在普通博弈中</strong>：一条“改进路径”可能会没完没了地走下去，甚至可能陷入一个循环（比如A动、B动、C动，结果又回到了A动之前的局面），永远无法达到一个稳定的纳什均衡。</li>
<li><strong>但在势博弈中</strong>：奇迹发生了。我们知道，在势博弈中，只要某个参与者
<code>i</code> 的收益 <code>uᵢ</code> 增加了，全局的势函数
<code>Φ</code> 的值也<strong>必须增加</strong>。</li>
<li>因此，在势博弈里，<strong>每一条“改进路径”都必然是一条“势函数
<code>Φ</code> 值不断增加的路径”</strong>。</li>
<li>在一个<strong>有限</strong>博弈中，势函数 <code>Φ</code>
的可能取值是有限的，它必然有一个最大值。<code>Φ</code>
的值不可能无限地增加下去。</li>
</ol>
<p><strong>结论</strong>：在有限势博弈中，任何“改进路径”走不了几步就<strong>必然会停止</strong>。它会在哪里停下呢？它会停在一个任何人都无法再单方面改善自己收益的地方——而这个地方，根据定义，<strong>就是一个纯策略纳什均衡</strong>。</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250624205950398.png" alt="2-28">
<figcaption aria-hidden="true">2-28</figcaption>
</figure>
<p><strong>标题：有限精确势博弈的特征描述 (Characterization of Finite
Exact Potential Games)</strong></p>
<ul>
<li><p>对于一条有限路径 γ=(s0,s1,…,sN)，我们令：</p>
<p>I(γ)=i=1∑N(umi(si)−umi(si−1))</p>
<p>其中，mi 指代在路径的第 i 步改变其策略的参与者。</p></li>
<li><p>如果 s0=sN，则路径 γ=(s0,…,sN) 是<strong>闭合的
(closed)</strong>。如果在此外对于每一个 0≤l&lt;k≤N−1 都有
sl\=sk，那么它就是一条<strong>简单闭合路径 (simple closed
path)</strong>。</p></li>
</ul>
<p>定理 (Theorem)</p>
<p>一个博弈 G 是一个精确势博弈，当且仅当对于所有有限简单闭合路径 γ，都有
I(γ) = 0。此外，只检验长度为4的简单闭合路径就足够了。</p>
<hr>
<p>这个数学定理，它为我们提供了一个<strong>“试金石”</strong>，用来检验任何一个有限博弈到底是不是一个“精确势博弈”，而无需我们去猜测或构造那个势函数
<code>Φ</code>。</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250624211206447.png" alt="2-29">
<figcaption aria-hidden="true">2-29</figcaption>
</figure>
<p><strong>标题：网络拥堵博弈 (Network Congestion Games)</strong></p>
<ul>
<li>一个包含 n 个用户的<strong>有向图 (directed graph)</strong>
G=(V,E)，</li>
<li>图 G 中的每一条<strong>边 (edge)</strong> e 都有一个<strong>延迟函数
(delay function)</strong> fₑ，</li>
<li>用户 i 的<strong>策略 (Strategy)</strong> 是选择一条从<strong>起点
(source)</strong> sᵢ 到<strong>终点 (destination)</strong> tᵢ
的<strong>路径 (path)</strong> Aᵢ，</li>
<li>一条路径的延迟是该路径上所有边的延迟之和，</li>
<li>每个用户都想通过选择最佳路径来<strong>最小化 (minimize)</strong>
其自身的延迟。</li>
</ul>
<hr>
<p>这张幻灯片介绍了一类在现实世界中应用极其广泛的博弈模型——<strong>网络拥堵博弈</strong>。这是理解交通堵塞、互联网数据包路由、供应链物流等众多问题的核心理论框架。</p>
<p><strong>1. 博弈的核心要素</strong></p>
<p>这个模型非常直观，它完美地捕捉了“拥堵”现象的本质：</p>
<ul>
<li><strong>参与者 (Players)</strong>：n 个用户（比如，n 位司机）。</li>
<li><strong>策略
(Strategies)</strong>：每个司机可以选择的路线（例如，从家
<code>sᵢ</code> 到公司 <code>tᵢ</code> 的不同路径）。</li>
<li><strong>成本
(Cost)</strong>：每个司机在路上花费的时间，即“延迟”。</li>
</ul>
<p>这个博弈最关键、最有趣的地方在于成本（延迟）的计算方式。一条路（边
<code>e</code>）的延迟<code>fₑ</code><strong>不是一个固定的数，而是一个函数</strong>。它的值取决于有多少人同时在使用这条路。</p>
<p><strong>2. 博弈的内在冲突</strong></p>
<p>每个司机都想自私地选择一条“最快”的路。但正是因为所有人都这么想，才导致了问题的产生：</p>
<ul>
<li>如果有一条近路看起来最快，所有司机可能都会涌向这条路。</li>
<li>结果，这条路变得极度拥堵，它的延迟函数 <code>fₑ(x)</code> 因为
<code>x</code>（用户数）变得很大而给出一个非常高的延迟值。</li>
<li>这条所谓的“最快”的路，实际上可能比其他更长但无人问津的路要慢得多。</li>
</ul>
<p><strong>每个人的最优选择都依赖于其他所有人的选择</strong>。我的决策影响你的成本，你的决策也影响我的成本。这就是博弈的核心所在。</p>
<p><strong>3. 与“势博弈”的深刻联系（核心洞见）</strong></p>
<p>这个模型最惊人的特性是，<strong>网络拥堵博弈是“精确势博弈”的经典范例</strong>。这是由学者
Rosenthal 在1973年发现的里程碑式成果。</p>
<p>存在一个全局的势函数
Φ（通常称为罗森塔尔势函数），它能够完美地刻画整个系统的动态。这个势函数的定义方式非常巧妙：</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250624211304691.png" alt="2-30">
<figcaption aria-hidden="true">2-30</figcaption>
</figure>
<p>其中，xe 是当前选择了边 e
的总用户数。这个公式的含义是，把网络中每一条边的“从第1个用户到第 xe
个用户的延迟依次加起来”，然后再把所有边的这个值汇总。</p>
<p>可以被严格证明：<strong>当任何一个用户 i
单方面改变自己的路径时，他个人延迟的变化量，与这个全局势函数
<code>Φ</code> 的变化量是完全相等的！</strong></p>
<p><strong>4. 重要推论</strong></p>
<p>既然拥堵博弈是精确势博弈，那么我们之前讨论过的所有优美性质就都可以应用在这里：</p>
<ol type="1">
<li><strong>均衡必然存在</strong>：任何一个网络拥堵博弈都<strong>至少存在一个纯策略纳什均衡</strong>。在交通模型里，这被称为“瓦德罗普均衡
(Wardrop
equilibrium)”。这意味着，总会存在一种稳定的交通分配格局，在这种格局下，没有单个司机可以通过单方面改变路线来缩短自己的通勤时间。</li>
<li><strong>动态必然收敛</strong>：如果司机们是“短视的”（例如，每天根据昨天的路况尝试寻找更快的路），这个不断调整、学习的过程<strong>必然会收敛到一个稳定的均衡状态</strong>，而不会永久地混乱或振荡下去。</li>
</ol>
<h2 id="纳什均衡求解">纳什均衡求解</h2>
<h3 id="零和博弈及纳什均衡计算">零和博弈及纳什均衡计算</h3>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250624232921182.png" alt="3-1">
<figcaption aria-hidden="true">3-1</figcaption>
</figure>
<p><strong>练习：斯塔克尔伯格双寡头模型 (Exercise: The Stackelberg model
of Duopoly)</strong></p>
<ul>
<li>斯塔克尔伯格双寡头模型 (1934):
<ul>
<li>一个参与者，被称为主导者或<strong>领导者
(leader)</strong>，首先行动，并且该参与者的选择结果在另一位参与者（<strong>跟随者,
follower</strong>）做出选择之前就已告知对方。</li>
<li>例如，通用汽车公司（General
Motors）在美国历史上的某些时期，就曾在汽车行业中扮演了如此主导的角色。</li>
</ul></li>
</ul>
<p><em>（来源：Game Theory, Second Edition, 2014. Thomas S.
Ferguson）</em></p>
<p><strong>标题：练习：斯塔克尔伯格双寡头模型</strong></p>
<ul>
<li>厂商1首先选择一个生产数量 q1，其单位成本为 c。</li>
<li>这个数量会被告知厂商2，然后厂商2再选择自己的生产数量
q2，其单位成本同样为 c。</li>
<li>之后，市场的单位价格 P 由以下公式决定： P(Q)={a−Q0if 0≤Q≤aif
Q&gt;a=(a−Q)+ 其中 Q=q1+q2，并且 a 是一个常数。</li>
<li>参与者们获得如下支付（利润）：
u1(q1,q2)=q1P(q1+q2)−cq1=q1(a−q1−q2)+−cq1
u2(q1,q2)=q2P(q1+q2)−cq2=q2(a−q1−q2)+−cq2 其中单位成本 c&lt;a。</li>
</ul>
<p>斯塔克尔伯格模型是产业组织理论中一个基石性的模型。它与我们之前讨论的古诺模型（Cournot
model）最大的不同在于，它将“同时行动”改为了<strong>“序贯行动”
(Sequential Moves)</strong>，从而引入了<strong>先手优势 (First-mover
Advantage)</strong> 的概念。</p>
<p><strong>1. 与古诺模型的根本区别</strong></p>
<ul>
<li><strong>古诺模型</strong>：两家厂商<strong>同时</strong>决定产量，谁也不知道对方会生产多少，是一个静态的、猜对手心思的博弈。</li>
<li><strong>斯塔克尔伯格模型</strong>：两家厂商有明确的行动顺序。一家是“领导者”（先动），另一家是“跟随者”（后动）。领导者率先公布自己的产量，跟随者在<strong>观察到</strong>领导者的产量后再决定自己的最优产量。这是一个动态博弈。</li>
</ul>
<p><strong>2. 如何求解？—— 逆向归纳法 (Backward Induction)</strong></p>
<p>对于这种有先后顺序的博弈，标准的解法是“逆向归纳”，即从后往前推。</p>
<p><strong>第一步：求解跟随者（厂商2）的问题</strong></p>
<p>我们先站在厂商2的角度。此时，厂商1的产量 q1
已经确定，是一个已知的数字。厂商2的目标是选择自己的产量 q2
来最大化自身利润 u2。</p>
<p>q2max u2(q1,q2)=q2(a−q1−q2)−cq2</p>
<p>为了求最大值，我们对 q2 求导并令其为0：</p>
<p>∂q2∂u2=a−q1−2q2−c=0</p>
<p>解出 q2，我们就得到了厂商2的反应函数 (Reaction Function)：</p>
<p>q2∗(q1)=2a−c−q1</p>
<p>这个函数告诉我们：不论领导者厂商1生产多少
(q1)，跟随者厂商2的最优应对策略是什么。</p>
<p><strong>第二步：求解领导者（厂商1）的问题</strong></p>
<p>厂商1非常“聪明”，它完全知道厂商2会如何根据它的 q1
来做出反应。因此，厂商1在做决策时，会把厂商2的反应函数直接代入自己的利润公式中，以此来预测自己选择不同
q1 的最终后果。</p>
<p>厂商1的利润函数变为：</p>
<p>u1(q1)=q1(a−q1−q2∗(q1))−cq1=q1(a−q1−2a−c−q1)−cq1</p>
<p>化简括号内的部分：</p>
<p>u1(q1)=q1(2a−c−q1)−cq1=2aq1−cq1−q12−cq1</p>
<p>厂商1的目标是选择 q1 来最大化这个新的利润函数。我们对 q1
求导并令其为0：</p>
<p>∂q1∂u1=2a−c−2q1=0⟹a−c−2q1=0</p>
<p>解出领导者厂商1的最优产量：</p>
<p>q1∗=2a−c</p>
<p><strong>第三步：得出最终均衡结果</strong></p>
<p>将厂商1的最优产量代入厂商2的反应函数，得到厂商2的产量：</p>
<p>q2∗=2a−c−q1∗=2a−c−(a−c)/2=2(a−c)/2=4a−c</p>
<p><strong>斯塔克尔伯格均衡解为：(领导者产量 q1∗=2a−c， 跟随者产量
q2∗=4a−c)</strong></p>
<p><strong>3. 先手优势</strong></p>
<ul>
<li>在斯塔克尔伯格均衡中，领导者产量(2a−c)是跟随者(4a−c)的两倍。</li>
<li>我们可以对比一下古诺均衡的结果：在古诺模型中，两家厂商的产量相同，均为
3a−c。</li>
<li>比较可知：q1Stackelberg(2a−c)&gt;qCournot(3a−c)&gt;q2Stackelberg(4a−c)。</li>
<li>这意味着，通过率先行动并承诺一个较高的产量，领导者可以有效地“挤压”跟随者的市场空间，迫使跟随者选择一个较低的产量，从而为自己攫取更高的市场份额和利润。这就是<strong>先手优势</strong>的体现。</li>
</ul>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250624233936853.png" alt="3-2">
<figcaption aria-hidden="true">3-2</figcaption>
</figure>
<p><strong>零和博弈 (Zero-Sum Games)</strong></p>
<p>极小化极大定理 (Minimax Theorem) (约翰·冯·诺伊曼, 1928):</p>
<p>对于每一个具有有限个纯策略的两人零和博弈，都存在一个适用于各方参与者的混合策略和一个价值
V，使得：</p>
<ul>
<li>给定参与者2的策略，参与者1可能获得的最佳支付为 V。</li>
<li>给定参与者1的策略，参与者2可能获得的最佳支付为 -V。</li>
</ul>
<p>策略存在的部分是<strong>纳什定理的一个特例</strong>，也是其先驱。</p>
<p>这基本上是说，参与者1可以保证自己获得<strong>至少 V</strong>
的支付，而参与者2可以保证自己获得<strong>至少 -V</strong>
的支付。如果双方都采取最优策略，这恰好就是他们将得到的结果。</p>
<p>它之所以被称为“极小化极大
(minimax)”，是因为参与者是通过一种试图<strong>最小化 (minimize)
对手可能获得的最大 (maximum)
支付</strong>的策略来获得这个价值的。我们稍后会再回到这一点。</p>
<p><strong>定义</strong>：价值 V 被称为该博弈的<strong>价值
(value)</strong>（或回报、支付）。</p>
<p><strong>例如</strong>：石头剪刀布的价值是0；假设参与者2采取最优策略（以1/3的概率出每一种手势），参与者1能期望获得的最好结果是0的支付。</p>
<hr>
<p>这张幻灯片介绍了博弈论的奠基性概念之一——<strong>两人零和博弈</strong>，以及该领域第一个里程碑式的定理——冯·诺伊曼的<strong>极小化极大定理</strong>。</p>
<p><strong>1. 什么是零和博弈？</strong></p>
<p>首先，零和博弈指的是在一个博弈中，所有参与者的收益（或亏损）加起来永远等于零。这意味着，<strong>一方的所得，必然是另一方的所失</strong>。这是一个纯粹冲突、完全竞争的模型，没有任何合作共赢的可能。经典的例子包括：</p>
<ul>
<li><strong>棋类游戏</strong>：如象棋、围棋，一方赢就是另一方输。</li>
<li><strong>石头剪刀布</strong>：一方赢一分，另一方就输一分。</li>
<li><strong>竞技体育</strong>：大多数只有两方对阵的比赛。</li>
</ul>
<p><strong>2. “极小化极大”定理的通俗解释</strong></p>
<p>这个定理解决了一个核心问题：在这样你死我活的纯冲突中，“理性”的策略是什么？冯·诺伊曼给出了一个天才的答案，其思考逻辑如下：</p>
<p><strong>从参与者1（P1）的角度（最大化最小值, Maximin）</strong>：</p>
<ol type="1">
<li>P1必须假设P2是完全理性的，并且会尽一切努力损害P1的利益。</li>
<li>P1会思考：“对于我可能采取的每一个策略，P2都会用对我最不利的方式来回应。我先列出每一种策略下，我最坏会得到什么结果（我的<strong>最小</strong>收益）。”</li>
<li>“然后，在所有这些‘最坏结果’中，我选择那个能让我得到最好结果的策略。”</li>
<li>这个过程，就是<strong>最大化自己的最小保证收益 (Maximize a minimum
payoff)</strong>，简称 <strong>Maximin</strong>。</li>
</ol>
<p><strong>从参与者2（P2）的角度（极小化极大, Minimax）</strong>：</p>
<ol type="1">
<li>P2同样假设P1会尽全力损害自己。</li>
<li>P2会思考：“对于P1的每一个策略，我最坏会损失多少（即P1能获得的最大收益）？”</li>
<li>“然后，我选择一个策略，能让P1可能获得的最大收益变得最小。”</li>
<li>这个过程，就是<strong>最小化自己的最大可能损失 (Minimize a maximum
loss)</strong>，简称 <strong>Minimax</strong>。</li>
</ol>
<p><strong>定理的“奇迹”</strong>：冯·诺伊曼证明，对于任何两人零和博弈，P1通过“最大化最小值”策略能保证得到的收益
<code>V</code>，与P2通过“极小化极大”策略能保证让P1得到的收益
<code>V</code>，是<strong>完全同一个数值</strong>！这个 <code>V</code>
就是该博弈的“价值”。</p>
<p>这意味着，这类纯冲突博弈存在一个绝对理性的、稳定的解。双方的最佳策略将会在这个点上交汇。</p>
<p><strong>3. 混合策略的重要性</strong></p>
<p>这个定理的成立，往往需要<strong>混合策略</strong>的引入，即以一定的概率随机地选择不同的行动。</p>
<ul>
<li>以“石头剪刀布”为例，如果你只出“石头”（一个纯策略），对手会立刻发现并一直出“布”来打败你。</li>
<li>你唯一能保证自己不输的策略，就是完全随机地出招（石头、剪刀、布各1/3概率）。</li>
<li>当你的对手也采取这种最优的混合策略时，你期望的平均收益就是0。因此，这个博弈的价值
<code>V=0</code>。</li>
</ul>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250625000231647.png" alt="3-3">
<figcaption aria-hidden="true">3-3</figcaption>
</figure>
<p><strong>计算纳什均衡：两人零和博弈 (Computing Nash Equilibria:
2-person, Zero-Sum Games)</strong></p>
<ul>
<li>这个博弈没有纯策略纳什均衡。</li>
<li>根据纳什定理，它必然拥有一个<strong>混合策略</strong>纳什均衡。</li>
<li>我们该如何找到它呢？</li>
</ul>
<p><em>(注：这是一个两人零和博弈)</em></p>
<hr>
<p>这张幻灯片提出了一个核心问题：对于一个没有纯策略均衡的博弈，我们如何具体计算出它的混合策略纳什均衡？</p>
<p>下面是详细的计算步骤：</p>
<p><strong>第一步：验证不存在纯策略纳什均衡</strong></p>
<p>我们可以通过“划线法”或“最优反应法”来快速验证。</p>
<ol type="1">
<li>如果参与者2选择“列1”，参与者1的最优选择是“行2”（因为收益 +3 &gt;
-2）。</li>
<li>如果参与者2选择“列2”，参与者1的最优选择是“行1”（因为收益 +3 &gt;
-4）。</li>
<li>如果参与者1选择“行1”，参与者2的最优选择是“列1”（因为收益 +2 &gt;
-3）。</li>
<li>如果参与者1选择“行2”，参与者2的最优选择是“列2”（因为收益 +4 &gt;
-3）。</li>
</ol>
<p>我们发现，没有任何一个单元格是双方共同的最优选择，因此该博弈确实没有纯策略纳什均衡。</p>
<p><strong>第二步：设定混合策略</strong></p>
<p>在混合策略均衡中，核心思想是<strong>“无差异原则” (Indifference
Principle)</strong>：每个参与者选择自己的混合策略（即概率），目的是让<strong>对方</strong>在自己的几个纯策略选择之间感到<strong>无所谓/无差异</strong>（即期望收益完全相等）。</p>
<ul>
<li>我们假设<strong>参与者1</strong>以概率 <strong>p</strong>
选择“行1”，以概率 <strong>(1-p)</strong> 选择“行2”。</li>
<li>我们假设<strong>参与者2</strong>以概率 <strong>q</strong>
选择“列1”，以概率 <strong>(1-q)</strong> 选择“列2”。</li>
</ul>
<p><strong>第三步：计算参与者1的混合策略 p</strong></p>
<p>为了让<strong>参与者2</strong>感到无差异，参与者2选择“列1”的期望收益必须等于他选择“列2”的期望收益。</p>
<ul>
<li>参与者2选择“列1”的期望收益 E(列1) = p<em>(+2)+(1−p)</em>(−3)</li>
<li>参与者2选择“列2”的期望收益 E(列2) = p<em>(−3)+(1−p)</em>(+4)</li>
</ul>
<p>令 E(列1) = E(列2):</p>
<p>可得p=7/12</p>
<p>所以，参与者1的最优策略是：以 7/12 的概率选择“行1”，以 5/12
的概率选择“行2”。</p>
<p><strong>第四步：计算参与者2的混合策略 q</strong></p>
<p>同样，为了让<strong>参与者1</strong>感到无差异，参与者1选择“行1”的期望收益必须等于他选择“行2”的期望收益。</p>
<ul>
<li>参与者1选择“行1”的期望收益 E(行1) = q<em>(−2)+(1−q)</em>(+3)</li>
<li>参与者1选择“行2”的期望收益 E(行2) = q<em>(+3)+(1−q)</em>(−4)</li>
</ul>
<p>令 E(行1) = E(行2):</p>
<p>q=7/12</p>
<p>所以，参与者2的最优策略是：以 7/12 的概率选择“列1”，以 5/12
的概率选择“列2”。</p>
<p><strong>第五步：结论与博弈的价值</strong></p>
<ol type="1">
<li><p><strong>混合策略纳什均衡</strong>：该博弈的唯一纳什均衡是：参与者1采取混合策略
( 7/12 ,5/12)，参与者2采取混合策略 (7/12 ,5/12)。</p></li>
<li><p>博弈的价值 (Value of the
Game)：在均衡状态下，参与者1的期望收益是多少？我们可以把 q = 7/12 代入
E(行1) 的公式中计算：</p>
<p>E(P1) = 1/12</p>
<p>因此，这个博弈对参与者1的<strong>价值是
+1/12</strong>，对参与者2的价值是
-1/12。这意味着，如果双方都采取最优的随机策略，长期来看，参与者1平均每次能赢1/12。</p></li>
</ol>
<p><strong>为什么目的是让对方在自己的几个纯策略选择之间感到无所谓/无差异？</strong></p>
<p>简单来说，<strong>让对方“无差异”并不是我们的最终目的，而是我们为了实现自身利益最大化，所必须达到的一个“结果”或“条件”。</strong></p>
<p>这是一种非常高明的策略思想，我们可以从三个层面来理解它：</p>
<hr>
<p><strong>1. 核心思想：消除对方的确定性最优解</strong></p>
<p>在一个博弈中，如果你采取的策略让你的对手有一个明确的、唯一的“最优选择”，那你就输了一半。因为：</p>
<ol type="1">
<li>一个理性的对手，一定会采取那个对他来说最优的选择。</li>
<li>这样一来，对手的行动就变得<strong>完全可以预测</strong>了。</li>
<li>一旦对手的行动是可预测的，你就可以反过来调整自己的策略，去专门“克制”他那个可预测的行动，从而让自己获利更多。</li>
<li>但这就产生了一个矛盾：如果你能调整策略获利更多，说明你最初的策略就不是最优的。</li>
</ol>
<p>这个矛盾循环说明，一个稳定的均衡状态，不应该让任何一方有“唯一的、确定的”最优解。而要做到这一点，你唯一的方法就是调整自己的策略组合（即概率
<code>p</code>），直到你的对手觉得“选A或选B，反正期望收益都一样，我无所谓了”。</p>
<p><strong>当你让对手“无所谓”时，你就消除了他行动的确定性，他才不得不也用一种随机的方式来对抗你。这才是对自己最有利的局面。</strong></p>
<hr>
<p><strong>2. 反向思考：如果不让对方无差异会怎样？</strong></p>
<p>我们用上一张幻灯片的例子来思考：</p>
<ul>
<li>你的策略是：以概率 <code>p</code> 出“行1”，概率 <code>(1-p)</code>
出“行2”。</li>
<li>假设你选择的 <code>p</code>
没有让对手无差异，而是让对手觉得<strong>“出‘列1’比出‘列2’的期望收益更高”</strong>。</li>
</ul>
<p>接下来会发生什么？</p>
<ol type="1">
<li><strong>对手的反应</strong>：理性的对手会想：“既然出‘列1’更好，我为什么还要费事去随机出‘列2’呢？”
于是，他会100%地出“列1”。他的策略就不再是混合策略了。</li>
<li><strong>你的反应</strong>：你看到对手只会出“列1”，那你还会坚持你原来的概率
<code>p</code>
吗？当然不会！你会看支付矩阵的“列1”那一栏，发现你出“行1”收益是-2，出“行2”收益是+3。你显然会100%地出“行2”来应对他。</li>
</ol>
<p><strong>结论</strong>：你最初那个让对手“有差异”的策略
<code>p</code>，最终导致了你自己也想改变策略。这就说明，那个初始状态<strong>根本不稳定</strong>，因此<strong>不是纳什均衡</strong>。</p>
<p>唯一的稳定状态，就是你选择的概率
<code>p</code>，正好让对手觉得“出‘列1’和出‘列2’没差”，他没有理由偏向任何一方，所以他才愿意继续以一定概率
<code>q</code> 来混合他的策略。</p>
<hr>
<p><strong>3. 一个直观的例子：点球大战</strong></p>
<p>想象一下足球比赛中的点球大战：</p>
<ul>
<li><strong>你的角色</strong>：踢球手。你可以选择踢左边或右边。</li>
<li><strong>对手的角色</strong>：守门员。他可以选择扑左边或右边。</li>
</ul>
<p>你的目标是什么？是让守门员<strong>对于“扑左还是扑右”感到无差异</strong>。</p>
<ul>
<li><strong>为什么？</strong>
如果你总喜欢踢左边（比如70%的概率），守门员就会发现这个规律，然后更倾向于扑向左边，这样你的进球率就会下降。你的行为变得“可预测”了。</li>
<li><strong>你的最优策略</strong>：你必须调整自己踢左边和右边的概率（比如各50%），使得守门员觉得“反正我扑左扑右，猜对的概率都一样，期望丢球数也一样，我无所谓了，只能瞎猜”。</li>
<li><strong>最终结果</strong>：只有当你成功地让守门员陷入“无所谓”的境地时，他才无法预测你的行动，你才能最大化你的进球率。</li>
</ul>
<p><strong>总结：</strong></p>
<p>在混合策略中，让对方“无所谓”，并不是一种善意的妥协，而是一种<strong>最顶级的进攻策略</strong>。它本质上是：</p>
<ul>
<li><strong>为了防止自己被预测和被针对。</strong></li>
<li><strong>为了迫使对方也必须采取不确定的、随机的策略来应对。</strong></li>
<li><strong>为了最终能在一个充满不确定性的稳定均衡中，保障自己获得最大化的期望收益。</strong></li>
</ul>
<p>所以，“让对方无差异”是<strong>你</strong>实现最优策略的<strong>必要条件</strong>，而不是你的目的本身。</p>
<h3 id="极大极小博弈">极大极小博弈</h3>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250625213116525.png" alt="3-4">
<figcaption aria-hidden="true">3-4</figcaption>
</figure>
<p>这张幻灯片用一个政治竞选的例子，构建了一个经典的两人零和博弈。它给出了问题设定，但没有给出解。核心任务就是根据这些信息，计算出这场博弈的均衡解以及博弈的价值。</p>
<p><strong>第一步：检查是否存在纯策略均衡</strong></p>
<p>我们首先检查是否存在一个稳定的单元格，使得任何一方都不想单方面改变策略。</p>
<ol type="1">
<li>如果列玩家（候选人2）选择“道德”，行玩家（候选人1）会选择“经济”（因为收益3
&gt; -2）。</li>
<li>如果列玩家选择“减税”，行玩家会选择“社会”（因为收益1 &gt; -1）。</li>
<li>如果行玩家选择“经济”，列玩家会选择“减税”（因为收益1 &gt; -3）。</li>
<li>如果行玩家选择“社会”，列玩家会选择“道德”（因为收益2 &gt; -1）。</li>
</ol>
<p>我们发现，不存在任何一个稳定的策略组合。例如，在（经济,
道德）这个组合，行玩家很满意，但列玩家会想换到“减税”策略以获得更好的收益。因此，<strong>该博弈没有纯策略纳什均衡</strong>。我们必须寻找混合策略均衡。</p>
<p><strong>第二步：计算混合策略纳什均衡</strong></p>
<p>我们将使用<strong>无差异原则</strong>来求解。</p>
<ul>
<li>设行玩家（候选人1）以概率 <strong>x</strong> 选择“经济”，以概率
<strong>(1-x)</strong> 选择“社会”。</li>
<li>设列玩家（候选人2）以概率 <strong>y</strong> 选择“道德”，以概率
<strong>(1-y)</strong> 选择“减税”。</li>
</ul>
<p><strong>A. 计算行玩家的策略 x</strong></p>
<p>行玩家需要选择一个概率
<code>x</code>，使得列玩家对于选择“道德”还是“减税”感到<strong>无差异</strong>（期望收益相等）。</p>
<ul>
<li>列玩家选择“道德”的期望收益 = x⋅(−3)+(1−x)⋅(2)</li>
<li>列玩家选择“减税”的期望收益 = x⋅(1)+(1−x)⋅(−1)</li>
</ul>
<p>令二者相等：</p>
<div style="display:none">
<pre><code>$$-3x + 2 - 2x =  - 1 + x$$$$2 - 5x = 2x - 1$$$$3 = 7x$$x=73</code></pre>
</div>
<p>所以，<strong>候选人1的最优策略</strong>是：以 <strong>3/7</strong>
的概率主打“经济”议题，以 <strong>4/7</strong> 的概率主打“社会”议题。</p>
<p><strong>B. 计算列玩家的策略 y</strong></p>
<p>同样，列玩家需要选择一个概率
<code>y</code>，使得行玩家对于选择“经济”还是“社会”感到<strong>无差异</strong>。</p>
<ul>
<li>行玩家选择“经济”的期望收益 = y⋅(3)+(1−y)⋅(−1)</li>
<li>行玩家选择“社会”的期望收益 = y⋅(−2)+(1−y)⋅(1)</li>
</ul>
<p>令二者相等：</p>
<div style="display:none">
<span class="math display">3<em>y</em> − 1 + <em>y</em> = −2<em>y</em> + 1 − <em>y</em></span><span class="math display">4<em>y</em> − 1 = 1 − 3<em>y</em></span><span class="math display">7<em>y</em> = 2</span>y=72
</div>
<p>所以，<strong>候选人2的最优策略</strong>是：以 <strong>2/7</strong>
的概率主打“道德”议题，以 <strong>5/7</strong> 的概率主打“减税”议题。</p>
<p><strong>第三步：计算博弈的价值</strong></p>
<p>博弈的价值（Value）就是指在双方都采取最优策略时，行玩家的期望收益。我们可以将
y = 2/7 代入行玩家的任一期望收益公式：</p>
<p>V=3y−1+y=4y−1=4⋅(72)−1=78−1=71</p>
<p><strong>结论</strong></p>
<ul>
<li><strong>纳什均衡</strong>：该博弈的唯一纳什均衡是，候选人1采取混合策略
(73,74)，候选人2采取混合策略
(72,75)。这意味着两位候选人都应该以特定的概率随机选择自己的竞选焦点，让对手无法预测。</li>
<li><strong>博弈价值</strong>：该博弈对行玩家（候选人1）的价值是
<strong>+1/7</strong>。在竞选这个议题上，如果双方都采取最理性的策略，<strong>候选人1平均可以期望净赚
1/7
百万（约14万）的选民</strong>。这表明在这个特定的战略矩阵中，候选人1拥有微弱的优势。</li>
</ul>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250625214139041.png" alt="3-5">
<figcaption aria-hidden="true">3-5</figcaption>
</figure>
<p>备注：下面这段话有提到“这代表了<strong>列玩家的理性反应</strong>。由于这是一个零和博弈，列玩家的目标是让行玩家的收益尽可能小。”，结合上面一张PPT的最后一个恒等式，可知，由于这是零和博弈，行玩家收益最少等价于列玩家收益最大。</p>
<hr>
<p>这张幻灯片从一个更形式化、更根本的角度，展示了作为“领导者”（被迫先宣布策略的一方）应该如何思考，并将这个问题转化为了一个标准的数学优化问题。最终，它揭示了一个关于零和博弈的深刻结论。</p>
<p><strong>1. Maximin 公式解读：“在最坏的情况里做到最好”</strong></p>
<p>幻灯片给出的第一个公式</p>
<p>maxmin(3x1−2x2,−x1+x2)</p>
<p>是“最大化最小值 (Maximin)”思想的完美数学体现。我们来拆解它：</p>
<ul>
<li><strong>内部的两个表达式</strong>：
<ul>
<li>3x1−2x2：这是当列玩家选择“道德”时，行玩家的期望收益。</li>
<li>−x1+x2：这是当列玩家选择“减税”时，行玩家的期望收益。</li>
</ul></li>
<li><strong>min(…)
部分</strong>：这代表了<strong>列玩家的理性反应</strong>。由于这是一个零和博弈，列玩家的目标是让行玩家的收益尽可能小。所以，当行玩家宣布了一个策略
(x1,x2)
后，列玩家会审视这两个可能的收益，并选择那个能让行玩家收益<strong>更小
(min)</strong> 的策略来应对。这代表了行玩家在宣布策略 (x1,x2)
后，所能得到的<strong>最坏结果保证</strong>。</li>
<li><strong>max(…)
部分</strong>：这代表了<strong>行玩家的决策</strong>。行玩家知道对手会这样针对他。所以，他在选择自己的策略
(x1,x2)
时，目标就是让这个“最坏的结果保证”变得尽可能好。也就是要<strong>最大化
(max)</strong> 那个最小的收益。</li>
</ul>
<p>这整个公式的逻辑就是：“我（行玩家）要选择一个策略
(x1,x2)，来最大化‘在我宣布这个策略后，对手尽最大努力打压我，我能得到的那个保底收益’”。</p>
<p><strong>2. 线性规划的转换：从博弈论到标准数学优化</strong></p>
<p>Max-min
问题在数学上直接求解不方便，但可以非常巧妙地转化为一个标准的<strong>线性规划
(Linear Programming, LP)</strong> 问题。</p>
<ul>
<li>我们引入一个新变量 <code>z</code>，让它代表那个“保底收益”。</li>
<li>我们的目标是 <code>最大化 z</code>。</li>
<li>约束条件是什么？<code>z</code> 必须小于等于所有可能的结果。所以：
<ul>
<li>z≤3x1−2x2 (保底收益不能超过对手选“道德”时我的收益)</li>
<li>z≤−x1+x2 (保底收益不能超过对手选“减税”时我的收益)</li>
</ul></li>
<li>再加上概率本身的基本约束 x1+x2=1 和 x1,x2≥0。</li>
</ul>
<p>这样，我们就把一个博弈问题，变成了一个可以用标准算法（如单纯形法）求解的数学问题。</p>
<p><strong>3. 最终结论：结果与之前完全相同！</strong></p>
<p>幻灯片最后指出，解这个线性规划得到的结果是
x1=3/7，x2=4/7，并且最优的保底收益 z=1/7。</p>
<p>这正是我们之前用“无差异原则”为<strong>同时博弈</strong>计算出的<strong>纳什均衡解</strong>和<strong>博弈价值
V</strong>！</p>
<p>为什么会这样？</p>
<p>这并非巧合，而是冯·诺伊曼极小化极大定理 (Minimax Theorem)
的直接体现。该定理的核心是：</p>
<p>Maximin=Minimax</p>
<ul>
<li>我们这张幻灯片求解的“领导者-跟随者”问题，正是行玩家的
<strong>Maximin（最大化最小值）</strong> 问题。</li>
<li>我们之前求解的“同时博弈”问题，得到的是<strong>Minimax（极小化极大值）</strong>
均衡解。</li>
</ul>
<p>这个定理保证了，在任何两人零和博弈中，这两个值是相等的。这意味着，<strong>在两人零和博弈中，不存在先手优势或后手优势</strong>。你提前公布策略，虽然给了对方信息，但你也可以利用这一点来选择一个最稳妥的策略；对方虽然能看到你的策略，但也只能在你设定的框架内做出反应。最终双方的力量会完美抵消，达到同一个均衡结果。</p>
<h3 id="纳什均衡的线性规划解法">纳什均衡的线性规划解法</h3>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250625215726625.png" alt="3-6">
<figcaption aria-hidden="true">3-6</figcaption>
</figure>
<p>这张幻灯片介绍了一个在线性规划乃至整个优化理论中，都极具美感和威力的核心概念——<strong>对偶性</strong>。它揭示了每个优化问题都存在一个“影子问题”或“镜像问题”，而理解这个镜像能为我们提供关于原问题全新的、深刻的洞察。</p>
<p><strong>1. 什么是对偶性？一个直观的例子</strong></p>
<p>要理解对偶，与其陷入复杂的数学转换，不如看一个经济学例子：</p>
<ul>
<li><strong>原问题 (Primal Problem)</strong>：
<ul>
<li>想象你是一家工厂的老板，你要决定生产多少桌子和椅子，来<strong>最大化你的总利润</strong>。</li>
<li>你面临一些<strong>约束</strong>：你拥有的木材、劳动力、设备时间都是有限的。</li>
<li>这就是一个典型的线性规划问题：最大化一个目标（利润），同时满足一系列约束（资源）。</li>
</ul></li>
<li><strong>对偶问题 (Dual Problem)</strong>：
<ul>
<li>现在，想象一个商人想来收购你所有的资源（木材、劳动力、设备）。他想<strong>最小化他的收购总成本</strong>。</li>
<li>但他面临一个<strong>约束</strong>：他为每种资源开出的“影子价格”组合，必须能让你觉得“卖掉资源比我自己生产产品更划算”。例如，生产一张桌子需要消耗的资源，他打包收购的价格，必须不能低于你自己生产这张桌子能获得的利润。</li>
<li>这个商人的问题——在一定约束下最小化成本——就是你那个最大化利润问题的“对偶问题”。</li>
</ul></li>
</ul>
<p><strong>2. 对偶性的“魔力”：原问题与对偶问题的关系</strong></p>
<p>对偶理论中有两个核心定理，它们揭示了原问题和对偶问题之间的奇妙关系：</p>
<ol type="1">
<li><p><strong>弱对偶定理 (Weak
Duality)</strong>：对偶问题的最优解，永远是原问题最优解的一个“界限”。在上面的例子里，就是说：商人收购资源的<strong>最小成本</strong>，必然<strong>大于等于</strong>工厂老板自己生产的<strong>最大利润</strong>。这很直观，因为如果收购成本低于你的利润，你肯定不会卖。</p></li>
<li><p>强对偶定理 (Strong
Duality)：在绝大多数情况下，这个“大于等于”实际上是<strong>“完全等于”</strong>！也就是说：</p>
<p>工厂能实现的最大利润=商人收购资源的最小成本</p>
<p>这是一个非常深刻的结论。它意味着，你资源的内在价值，恰好等于你能用它们创造的最大利润。对偶问题中的变量（资源的“影子价格”），精确地量化了每一种稀缺资源的边际价值。</p></li>
</ol>
<p><strong>3. 对偶性与“零和博弈”的惊人联系</strong></p>
<p>现在，我们可以把这个概念带回我们之前讨论的博弈论了。这正是引入对偶概念的关键所在。</p>
<ul>
<li>在前几张幻灯片中，我们把<strong>行玩家</strong>的问题构建成了一个线性规划：<strong>最大化</strong>他的保底收益
<code>V</code>。这可以看作是我们的<strong>原问题
(Primal)</strong>。</li>
<li>那么，<strong>列玩家</strong>的问题是什么？列玩家的目标是<strong>最小化</strong>行玩家能获得的最大收益
<code>W</code>。我们同样可以把列玩家的这个问题也构建成一个线性规划。</li>
<li><strong>最关键的结论是</strong>：<strong>列玩家的“最小化极大值”线性规划问题，恰好就是行玩家“最大化最小值”线性规划问题的对偶问题！</strong></li>
</ul>
<p>因此，线性规划的“强对偶定理”（最大值 =
最小值），在零和博弈的语境下，就直接变成了冯·诺伊曼的“极小化极大定理”！</p>
<p>max(行玩家的保底收益)=min(列玩家的最大损失)Maximin=Minimax</p>
<p><strong>总结</strong>：对偶性不仅是线性规划的强大工具，它还为博弈论的基石——极小化极大定理——提供了最坚实的数学证明。它优美地揭示了，一个参与者的最大化问题和其对手的最小化问题，实际上是同一个数学结构的两个不同侧面，如同一枚硬币的两面，其价值必然相等。</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250625220404379.png" alt="3-7">
<figcaption aria-hidden="true">3-7</figcaption>
</figure>
<p>线性规划对偶性实例。</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250625220536283.png" alt="3-8">
<figcaption aria-hidden="true">3-8</figcaption>
</figure>
<p>弱对偶性和强对偶性</p>
<h3 id="线性互补问题">线性互补问题</h3>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250625222636285.png" alt="3-9">
<figcaption aria-hidden="true">3-9</figcaption>
</figure>
<p><strong>标题：计算两人一般和博弈的纳什均衡 (Computing Nash equilibria
of two-player, general-sum games)</strong></p>
<ul>
<li>不幸的是，寻找一个两人<strong>一般和博弈 (general-sum game)</strong>
的纳什均衡，<strong>无法</strong>被构建成一个线性规划问题。
<ul>
<li>— 两名参与者的利益不再是*<strong>完全*对立的 (completely
opposed)</strong>。</li>
<li>— 然而，我们仍然可以将我们的问题表述为某种优化问题。</li>
</ul></li>
</ul>
<hr>
<p>这张幻灯片标志着一个重要的转折点。从“零和博弈”进入了更普遍、也更复杂的“一般和博弈”（或称“非零和博弈”）的世界。幻灯片的核心信息是：之前强大而高效的线性规划（LP）工具，在这里失效了。</p>
<p><strong>1. 什么是一般和博弈？</strong></p>
<p>一般和博弈指的是，在任何一个结果下，所有参与者的收益之和<strong>不一定为零</strong>。这意味着博弈的结果可以是双赢、双输，或者一方赢多、一方输少。参与者的关系不再是“你死我活”的纯粹冲突，而是<strong>冲突与合作并存</strong>。</p>
<ul>
<li><p>经典例子1：囚徒困境</p>
<p>如果两个囚犯都背叛对方，他们可能各判5年（总收益-10）。如果他们都保持沉默，可能各判1年（总收益-2）。这是一个双输的“负和”博弈。</p></li>
<li><p>经典例子2：性别大战 (Battle of the Sexes)</p>
<p>一对情侣都想待在一起（合作），但一人想看歌剧，另一人想看球赛（冲突）。如果他们去同一个地方，双方都能获得高收益（比如（5,2）或（2,5）），总收益为7。如果去不同地方，则两人都很不开心（0,0），总收益为0。这是一个“正和”博弈。</p></li>
</ul>
<p><strong>2. 为什么线性规划会失效？—— 对偶性的崩塌</strong></p>
<p>这是理解这张幻灯片最关键的地方。线性规划之所以能在零和博弈中大显神威，是因为其背后优美的“对偶性”，而这种对偶性源于双方利益的<strong>完全对立</strong>。</p>
<ul>
<li><strong>在零和博弈中</strong>：
<ul>
<li>行玩家的目标是：最大化自己的收益 <code>u₁</code>。</li>
<li>列玩家的目标是：最大化自己的收益 <code>u₂</code>。</li>
<li>由于
<code>u₂ = -u₁</code>，所以列玩家“最大化<code>u₂</code>”就<strong>等价于</strong>“最小化<code>u₁</code>”。</li>
<li>因此，行玩家的“最大化最小值 (Maximin)”问题和列玩家的“最小化极大值
(Minimax)”问题，形成了一对完美的数学<strong>对偶</strong>。它们就像一枚硬币的两面，可以用同一个线性规划框架来解决。</li>
</ul></li>
<li><strong>在一般和博弈中</strong>：
<ul>
<li><code>u₂</code> 不再等于 <code>-u₁</code>。</li>
<li>列玩家的目标——最大化他自己的 <code>u₂</code>——与行玩家的收益
<code>u₁</code> <strong>没有直接的、负相关的关系</strong>。</li>
<li>列玩家不再是处心积虑地要让行玩家的收益最小化，他只关心自己的收益。</li>
<li>这样一来，那种“我之所得即你之所失”的完美对偶关系就<strong>彻底崩塌</strong>了。我们无法再构建出一个单一的线性规划问题来同时描述双方的决策并找到那个共同的解。</li>
</ul></li>
</ul>
<p><strong>3. 那问题变成了什么？</strong></p>
<p>虽然不能用线性规划，但寻找纳什均衡依然是一个数学优化问题，只是变得更复杂了。</p>
<ul>
<li>寻找两人一般和博弈的纳什均衡，在数学上等价于求解一个<strong>线性互补问题
(Linear Complementarity Problem,
LCP)</strong>。这是一个比线性规划更复杂的数学结构。</li>
<li>从计算复杂性的角度看，求解两人零和博弈是“容易”的（属于
<strong>P</strong>
问题），而求解两人一般和博弈则被证明是<strong>PPAD-完全
(PPAD-complete)</strong>
问题。这通常被认为是一个“更难”的计算等级，意味着找到解需要更复杂的算法，计算效率也更低。</li>
</ul>
<p><strong>总结</strong>：从“零和”到“一般和”的转变，是博弈论中一次巨大的复杂性飞跃。它让我们失去了线性规划这个简洁高效的工具，迫使我们进入一个更困难的计算领域。这也反过来凸显了两人零和博弈及其“极小化极大定理”在理论上的简洁与优美。</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250625223646101.png" alt="3-10">
<figcaption aria-hidden="true">3-10</figcaption>
</figure>
<p><strong>标题：计算两人一般和博弈的纳什均衡</strong></p>
<ul>
<li><p>我们首先考虑一个<strong>内部的 (inner)</strong>
或<strong>完全混合的 (totally mixed)</strong> 纳什均衡
(X∗,Y∗)，即对于所有的 i 和 j，都有 xi∗&gt;0 以及
yj∗&gt;0（所有的纯策略都以正概率被使用）。</p></li>
<li><p>让 ai 表示行玩家的支付矩阵A的各行，让 bj
表示列玩家的支付矩阵B的各列。</p></li>
<li><p>利用“<strong>在一个纳什均衡策略的支持集(support)中，所有纯策略都产生相同的支付</strong>”这一事实，并且该支付大于或等于支持集之外的策略的支付，我们得到：</p>
<ul>
<li><p>aiy∗=aky∗,i,k=1,2,…,m.</p></li>
<li><p>(x∗)Tbj=(x∗)Tbk,j,k=1,2,…,n.</p>
<p>(注：此处公式经过订正以符合标准表达)</p>
<p>(绿色文字) 假设每个纯策略都以正概率被使用。</p></li>
</ul></li>
<li><p>上述构成了一个可以被高效求解的线性方程组。</p></li>
</ul>
<p><strong>下图内容</strong></p>
<ul>
<li><p>然而，“每个策略都以正概率被使用”的假设是有限制性的。大多数博弈并不拥有完全混合的纳什均衡；对于它们而言：</p></li>
<li><p>我们计算一个有限两人博弈的所有纳什均衡：</p>
<p>一个混合策略组合 (x∗,y∗) 是一个具有支持集 S1,S2
的纳什均衡，当且仅当：</p>
<ul>
<li>u=aiy∗,∀i∈S1 （对于支持集内的策略i，收益都等于均衡收益u）</li>
<li>u≥aiy∗,∀i∈/S1 （对于支持集外的策略i，收益不高于u）</li>
<li>v=(x∗)Tbj,∀j∈S2 （对于支持集内的策略j，收益都等于均衡收益v）</li>
<li>v≥(x∗)Tbj,∀j∈/S2 （对于支持集外的策略j，收益不高于v）</li>
<li>xi∗=0,∀i∈/S1, yj∗=0,∀j∈/S2 （u, v 是NE中的收益值）</li>
</ul></li>
<li><p>要让上述过程可行，我们需要找到正确的<strong>支持集
(supports)</strong>。我们需要遍历所有可能的支持集组合。由于存在 2n+2m
种不同的支持集，这会导致算法具有指数级的复杂度。</p></li>
<li><p><strong>备注
(Remark)</strong>：计算有限博弈的纳什均衡的<strong>计算复杂度</strong>，就在于<strong>找到正确的支持集</strong>。</p></li>
</ul>
<hr>
<p>这两张幻灯片讲述了一个关于“求解一般和博弈”的完整故事：从一个理想化的、简单的特例，到一个普遍的、困难的现实。</p>
<p><strong>1. 理想情况：完全混合均衡（上图）</strong></p>
<p>幻灯片的上半部分描绘了一种“完美”的均衡状态，即<strong>完全混合均衡</strong>。在这种均衡里，每一个参与者都认为对手的所有可选策略都值得提防，因此自己的最优策略是给自己的每一个选项都分配一个<strong>大于零</strong>的概率。</p>
<ul>
<li><p>为什么这种情况简单？</p>
<p>因为它使得“无差异原则”可以应用到所有策略上。为了让对手混合他的所有策略，你必须让你的对手在选择他的任何一个策略时，期望收益都完全相等。</p></li>
<li><p>如何求解？</p>
<p>这就产生了一个完整的线性方程组（m-1个关于行玩家收益的等式，n-1个关于列玩家收益的等式，再加上两个概率和为1的等式）。这是一个标准的、可以用我们熟悉的方法高效求解的数学问题。</p></li>
</ul>
<p>然而，这种所有策略都被用上的“雨露均沾”式的均衡，在现实中非常罕见。</p>
<p><strong>2. 现实情况：寻找“支持集”（下图）</strong></p>
<p>幻灯片的下半部分指出了残酷的现实：在绝大多数博弈中，通常都会有一些策略是“劣势策略”或“糟糕的选项”，一个理性的玩家是永远不会使用它们的（即使用概率为0）。</p>
<ul>
<li><strong>支持集
(Support)</strong>：在一个混合策略中，那些<strong>真正以正概率被使用的纯策略的集合</strong>，被称为这个混合策略的“支持集”。</li>
<li><strong>核心困难</strong>：求解的关键困难在于，我们<strong>事先并不知道</strong>最终的均衡解中，到底哪些策略会是“优势策略”（在支持集里），哪些是“劣势策略”（在支持集外）。</li>
</ul>
<p>这就引出了一个计算上的巨大难题，我喜欢称之为<strong>“寻找嫌疑人”的困境</strong>：</p>
<ul>
<li><strong>无差异原则</strong>就像一个完美的“审讯工具”，只要你把正确的“嫌疑人”（支持集里的策略）找来，它就能告诉你每个人的详细“作案手法”（混合策略的精确概率）。</li>
<li><strong>但问题是</strong>，你不知道谁是真正的“嫌疑人”。你面对着所有可能的策略，不知道该把哪些策略纳入“无差异”的审讯中。</li>
</ul>
<p><strong>3. “暴力搜索”算法及其指数级复杂度</strong></p>
<p>理论上，我们可以用一种“暴力”的方法来找到所有均衡：</p>
<ol type="1">
<li><strong>猜测</strong>：我们先猜一个可能的支持集组合。例如，“我猜行玩家只会用策略1和3，而列玩家只会用策略2和4”。</li>
<li><strong>求解</strong>：基于这个猜测，我们建立一个只包含这些策略的线性方程组（即只让这些策略满足无差异原则）并求解。</li>
<li><strong>验证</strong>：检查解出的结果是否是一个合法的纳什均衡。这包括两部分：
<ul>
<li>解出的概率值是否都在0和1之间？</li>
<li>对于那些我们<strong>没猜</strong>的“局外”策略，它们的期望收益是否真的<strong>不优于</strong>我们算出的均衡收益？（这是最关键的验证，确保没人想把局外策略拉进局内）。</li>
</ul></li>
<li><strong>重复</strong>：如果验证失败，就回到第一步，换一种支持集的猜测，然后重复整个过程，直到遍历完<strong>所有可能的支持集组合</strong>。</li>
</ol>
<p>这个方法的致命缺陷在于，支持集的组合数量是<strong>指数级增长</strong>的。如果一个博弈双方各有20个策略，那么可能的支持集组合数量会是一个天文数字。这就是幻灯片所说的“指数级复杂度”。</p>
<p><strong>总结</strong>：寻找一般和博弈的纳什均衡，其计算上的困难<strong>不在于解方程</strong>，而在于<strong>找到应该用哪些策略来列方程</strong>。这个“寻找正确支持集”的组合搜索过程，是该问题计算复杂度（PPAD-complete）的根源，也使其与可以通过线性规划轻松求解的两人零和博弈产生了本质的区别。</p>
<h2 id="多智能体强化学习">多智能体强化学习</h2>
<h3 id="多智能体强化学习介绍及基本概念">7.1
多智能体强化学习介绍及基本概念</h3>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250630205121221.png" alt="7-1">
<figcaption aria-hidden="true">7-1</figcaption>
</figure>
<p>这张幻灯片指出了从单智能体学习（比如我们熟知的AlphaGo下围棋的早期版本）迈向多智能体学习（比如王者荣耀或星际争霸中的AI）时，所遇到的一个根本性的、质的困难。</p>
<p><strong>1. 核心困难：移动的靶心 (The Moving-Target
Problem)</strong></p>
<ul>
<li><strong>在单智能体学习中</strong>：环境是<strong>静止的</strong>或有固定规律的。一个智能体（比如一个机器人）学习走路，它只需要掌握如何应对地板、墙壁等物理规律即可。这个“游戏规则”是不会变的。</li>
<li><strong>在多智能体学习中</strong>：情况发生了根本变化。对于任何一个智能体（我们称之为“我”）来说，<strong>其他的智能体也是环境的一部分</strong>。但问题是，这些“其他的智能体”本身也在学习、在进化、在改变他们的策略。
<ul>
<li><strong>一个生动的例子</strong>：你学习如何开车上班最快。如果只有你一个人在学习，而其他人的路线每天都固定不变，这就是一个简单的单智能体问题。但现实是，成千上万的司机（其他智能体）每天也都在尝试新的路线来优化他们自己的通勤时间。你今天发现的“最优路线”，明天可能因为有几百个和你一样的人也发现了它而变得极度拥堵。</li>
<li>你试图瞄准的那个“最优策略”的靶心，因为他人的学习而<strong>不断地移动</strong>。这就是多智能体学习的核心困难，学术上称为<strong>“环境的非平稳性”
(Non-stationarity)</strong>。</li>
</ul></li>
</ul>
<p><strong>2. 为什么简单的Q学习会失效？</strong></p>
<p>标准的单智能体强化学习算法（如Q学习）之所以能成功，是因为它们依赖于一个基本假设：<strong>马尔可夫决策过程
(MDP)</strong>，即环境是平稳的。这意味着，在同一个状态（State）下，采取同一个动作（Action），得到的奖励（Reward）和状态转移的概率应该是基本一致的。</p>
<p>但在多智能体环境中，这个假设被彻底打破了：</p>
<ul>
<li><strong>今天</strong>：在路口（状态S），你选择直行（动作A），因为其他人都选择了右转，所以一路畅通，你获得了很高的奖励。你的Q表格会更新，认为<code>(S, A)</code>是个好选择。</li>
<li><strong>明天</strong>：在同一个路口（状态S），你根据昨天的经验再次选择直行（动作A）。但昨天和你一样选择直行的其他智能体也获得了高奖励，所以今天他们也选择直行。结果造成了交通堵塞，你得到了一个很低的奖励。你的Q表格又必须更新，认为<code>(S, A)</code>是个坏选择。</li>
</ul>
<p>你的Q值会这样剧烈地来回震荡，可能永远无法收敛到一个稳定的策略，因为一个动作的“好”与“坏”不再是固定的，而是完全取决于其他智能体当前正在执行的策略。</p>
<p><strong>3. 与博弈论的联系</strong></p>
<p>这个学习过程中的“不稳定”问题，正是我们在前面博弈论部分看到的“均衡”问题的动态体现。</p>
<ul>
<li>多智能体学习的目标，往往就是让这群智能体通过学习和试错，最终能够收敛到整个博弈的<strong>纳什均衡</strong>。</li>
<li>幻灯片中的螺旋图可以这样理解：中心点是博弈的纳什均衡点。这条螺旋线代表了所有智能体的联合策略随着时间演变的轨迹。如果学习算法设计得好（例如，在<strong>势博弈</strong>中），这条轨迹就会像图中一样稳定地<strong>收敛</strong>到中心。</li>
<li>如果算法设计不当或者博弈本身就很“恶劣”（比如“石头剪刀布”），那么学习过程可能永远无法收敛，只会在策略空间中不停地“绕圈子”。</li>
</ul>
<p><strong>总结</strong>：多智能体学习的困难在于，每个智能体的学习过程都会改变其他智能体的学习环境，形成一个复杂且动态的“移动靶心”问题。简单地将单智能体算法直接套用，会因环境的“非平稳性”而失效。因此，现代多智能体学习研究的核心，就是设计出能够在这种动态博弈中稳定地学习、并最终收敛到纳什均衡等合理状态的算法。</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250630205734872.png" alt="7-2">
<figcaption aria-hidden="true">7-2</figcaption>
</figure>
<p>我们可以通过两个关键问题来区分这些模型：</p>
<ol type="1">
<li><strong>有几个决策者（智能体）？</strong> 一个还是多个？</li>
<li><strong>有多少种情况（状态）？</strong> 一种还是多种？</li>
</ol>
<p><strong>1. 马尔可夫决策过程 (MDPs): 单人探索世界</strong></p>
<ul>
<li><strong>特征</strong>：<strong>一个</strong>智能体，<strong>多个</strong>状态。</li>
<li><strong>核心问题</strong>：一个独立的决策者，在一个可以变化的环境中，如何学习一系列的动作以最大化其长期回报。</li>
<li><strong>通俗例子</strong>：一个机器人学习走迷宫。机器人是<strong>唯一的智能体</strong>，迷宫中不同的位置就是<strong>不同的状态</strong>。机器人需要学习在每个位置（状态）下，应该朝哪个方向走（动作），才能最快找到出口（最大化回报）。</li>
<li><strong>地位</strong>：这是现代<strong>单智能体强化学习
(Single-Agent Reinforcement Learning)</strong>
的数学基石。我们熟知的AlphaGo下棋，本质上也可以看作是在一个极度复杂的MDP中寻找最优策略。</li>
</ul>
<p><strong>2. 重复博弈 (Repeated Games): 多人重复同一场游戏</strong></p>
<ul>
<li><strong>特征</strong>：<strong>多个</strong>智能体，<strong>一个</strong>状态。</li>
<li><strong>核心问题</strong>：多个决策者，反复地玩<strong>同一个</strong>博弈。</li>
<li><strong>通俗例子</strong>：两家寡头公司，每个月都要决定自己的产品定价。每个月的定价博弈，其本身的收益矩阵都是一样的，所以可以看作是<strong>单一状态</strong>。但因为博弈是<strong>重复</strong>的，今天的决策会影响声誉，从而影响对手明天的决策。这就引入了如“以牙还牙
(Tit-for-Tat)”这样的动态策略。我们之前讨论的各种矩阵博弈（如囚徒困境、零和博弈），如果将它们连续玩很多次，就构成了重复博弈。</li>
</ul>
<p><strong>3. 随机博弈 / 马尔可夫博弈 (Stochastic/Markov Games):
多人探索动态世界</strong></p>
<ul>
<li><strong>特征</strong>：<strong>多个</strong>智能体，<strong>多个</strong>状态。</li>
<li><strong>核心问题</strong>：多个决策者在一个动态变化的环境中共同决策，他们的联合行动会共同决定环境如何进入下一个状态。</li>
<li><strong>通俗例子</strong>：一场足球比赛。场上有<strong>多个智能体</strong>（双方队员）。球和所有队员在场上的位置，共同构成了一个<strong>状态</strong>。当大家做出动作（跑动、传球、射门）后，场上的局面会变成一个<strong>新的状态</strong>。在每个不同的状态下，球员们面临的“局部博弈”也是不同的。</li>
<li><strong>地位</strong>：这是最普适、最复杂的模型，它构成了<strong>多智能体强化学习
(Multi-Agent Reinforcement Learning, MARL)</strong> 的理论基础。</li>
</ul>
<p>总结与联系：</p>
<p>这张图清晰地告诉我们，随机博弈是“集大成者”，它统一了另外两个模型：</p>
<ul>
<li>当随机博弈的智能体数量减少到1个时，它就退化成了<strong>MDP</strong>。</li>
<li>当随机博弈的状态数量减少到1个时，它就退化成了<strong>重复博弈</strong>。</li>
</ul>
<p>这与我们上一张幻灯片讨论的<strong>“多智能体学习的困难”</strong>完美地衔接了起来。我们所说的“环境的非平稳性”，正是因为我们身处<strong>随机博弈</strong>的框架中：对于“我”这个智能体而言，环境之所以看起来在不停变化，是因为环境的下一个状态不仅取决于我的行动，还取决于其他所有智能体的行动，而他们本身也在学习和改变。</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250630214131295.png" alt="7-3">
<figcaption aria-hidden="true">7-3</figcaption>
</figure>
<p>这张幻灯片提出了一个非常经典的任务：为任意一个 2x2
零和博弈找到通用的解法，即用矩阵中的参数 a,b,c,d
来表达博弈的价值和双方的最优策略。</p>
<p>我们将遵循幻灯片给出的两步计划来完成这个推导。</p>
<p><strong>步骤一：检验纯策略均衡（鞍点）</strong></p>
<p>一个纯策略纳什均衡（在零和博弈中也称为“鞍点”）存在的条件是：<strong>某一个收益值，既是其所在行的最小值，又是其所在列的最大值</strong>。</p>
<p>我们可以通过比较行玩家的“最大化最小值（Maximin）”和列玩家的“最小化极大值（Minimax）”来判断：</p>
<ul>
<li>行玩家的保底收益（最大最小值）: max(min(a,b),min(d,c))</li>
<li>列玩家的保底收益（最小极大值）: min(max(a,d),max(b,c))</li>
</ul>
<p>如果这两个值相等，则存在纯策略均衡，该值就是博弈的价值。例如，如果
<code>a &lt;= b</code> 并且 <code>a &gt;= d</code>，那么 <code>a</code>
就是一个鞍点，（行1，列1）就是纯策略纳什均衡。</p>
<p><strong>步骤二：求解混合策略均衡（假设不存在鞍点）</strong></p>
<p>如果不存在鞍点，那么双方的最优策略必然是混合策略。我们将使用<strong>无差异原则</strong>来求解。</p>
<ul>
<li><p><strong>策略设定</strong>:</p>
<ul>
<li>行玩家以概率 <strong>p</strong> 选择“行1”，以概率
<strong>(1-p)</strong> 选择“行2”。</li>
<li>列玩家以概率 <strong>q</strong> 选择“列1”，以概率
<strong>(1-q)</strong> 选择“列2”。</li>
</ul></li>
<li><p>推导行玩家的策略 p:</p>
<p>行玩家选择 p 的目的是让列玩家在“列1”和“列2”之间感到无差异。</p>
<ul>
<li><p>列玩家选择“列1”的期望收益（注意，要用列玩家的收益矩阵<code>-A</code>）：p(−a)+(1−p)(−d)</p></li>
<li><p>列玩家选择“列2”的期望收益：p(−b)+(1−p)(−c)</p>
<p>令二者相等：</p>
<p><span class="math display">−<em>a</em><em>p</em> − <em>d</em>(1 − <em>p</em>) = −<em>b</em><em>p</em> − <em>c</em>(1 − <em>p</em>)</span><span class="math display">−<em>a</em><em>p</em> − <em>d</em> + <em>d</em><em>p</em> = −<em>b</em><em>p</em> − <em>c</em> + <em>c</em><em>p</em></span><span class="math display"><em>d</em> − <em>c</em> = <em>p</em>(<em>a</em> − <em>d</em> − <em>b</em> + <em>c</em>) = <em>p</em>((<em>a</em> + <em>c</em>) − (<em>b</em> + <em>d</em>))</span></p>
<p>解得行玩家的最优概率 p：</p>
<p>p∗=(a+c)−(b+d)d−c</p></li>
</ul></li>
<li><p>推导列玩家的策略 q:</p>
<p>列玩家选择 q 的目的是让行玩家在“行1”和“行2”之间感到无差异。</p>
<ul>
<li><p>行玩家选择“行1”的期望收益：q(a)+(1−q)(b)</p></li>
<li><p>行玩家选择“行2”的期望收益：q(d)+(1−q)(c)</p>
<p>令二者相等：</p>
<p><span class="math display"><em>a</em><em>q</em> + <em>b</em>(1 − <em>q</em>) = <em>d</em><em>q</em> + <em>c</em>(1 − <em>q</em>)</span><span class="math display"><em>a</em><em>q</em> + <em>b</em> − <em>b</em><em>q</em> = <em>d</em><em>q</em> + <em>c</em> − <em>c</em><em>q</em></span><span class="math display"><em>b</em> − <em>c</em> = <em>q</em>(<em>d</em> − <em>a</em> − <em>c</em> + <em>b</em>) = <em>q</em>((<em>b</em> + <em>d</em>) − (<em>a</em> + <em>c</em>))</span></p>
<p>解得列玩家的最优概率 q：</p>
<p>q∗=(b+d)−(a+c)b−c=(a+c)−(b+d)c−b</p></li>
</ul></li>
</ul>
<p><strong>推导博弈的价值 V</strong></p>
<p>博弈的价值 V 是在均衡状态下，行玩家的期望收益。我们可以将求得的 q*
代入行玩家的期望收益公式：</p>
<p>V=aq∗+b(1−q∗)=q∗(a−b)+bV=(a+c)−(b+d)c−b(a−b)+b</p>
<p>对上式进行通分并化简：</p>
<p>V=(a+c)−(b+d)(c−b)(a−b)+b((a+c)−(b+d))</p>
<p><span class="math display">$$V = \frac{ac - bc - ab + b^2 + ab + bc -
b^2 - bd}{(a+c) - (b+d)}$$</span>分子中的 -bc, +bc, -ab, +ab, +b², -b²
项都相互抵消了，最终得到一个非常优美的公式：</p>
<p>V=(a+c)−(b+d)ac−bd</p>
<hr>
<p><strong>最终通解</strong></p>
<p>对于一个不存在纯策略均衡的 2x2 零和博弈矩阵 A=(adbc)，其通解为：</p>
<ol type="1">
<li><p>行玩家的最优混合策略 (p∗,1−p∗)：</p>
<p>p∗=(a+c)−(b+d)d−c</p></li>
<li><p>列玩家的最优混合策略 (q∗,1−q∗)：</p>
<p>q∗=(a+c)−(b+d)c−b</p></li>
<li><p>该博弈对行玩家的价值 V：</p>
<p>V=(a+c)−(b+d)ac−bd</p></li>
</ol>
<p><strong>为什么行玩家是最大化最小值，而不是最小化极大值？</strong></p>
<p>问得特别好！这正是理解零和博弈中双方战略思想的精髓所在。</p>
<p>简单来说，答案是：<strong>因为他们的根本目标不同。</strong></p>
<ul>
<li><strong>行玩家的目标</strong>：最大化自己的收益。</li>
<li><strong>列玩家的目标</strong>：最大化自己的收益，这在零和博弈中就等价于<strong>最小化行玩家的收益</strong>。</li>
</ul>
<p>让我们一步一步地、站在<strong>行玩家（Player
1）</strong>的角度来思考他为什么必须是“最大化最小值 (Maximin)”。</p>
<p><strong>1.
行玩家的思考过程：“我如何为最坏的情况做准备？”</strong></p>
<p>行玩家是一个理性的决策者，他想让自己赚得尽可能多。但他知道两件事：</p>
<ul>
<li>他的收益不只取决于自己，还取决于对手的选择。</li>
<li>他的对手（列玩家）的目标和他完全相反。他每多赚一块钱，就意味着对手要多亏一块钱。所以，对手会想尽一切办法让他赚得最少。</li>
</ul>
<p>基于这个“对手会尽力坑我”的假设，行玩家必须采取一种非常谨慎和保守的策略。他的思考逻辑如下：</p>
<ol type="1">
<li><p>审视自己的第一个选项（比如“行1”）：</p>
<p>“如果我选择‘行1’，我的对手会怎么做？他会看遍‘行1’的所有结果，然后选择那个能让我收益最低的选项。所以，如果我出‘行1’，我能得到的保底收益就是这一行里的最小值。”</p></li>
<li><p>审视自己的第二个选项（比如“行2”）：</p>
<p>“同理，如果我选择‘行2’，理性的对手也会选择那一列来让我获得‘行2’里的最低收益。这个最小值就是我出‘行2’的保底收益。”</p></li>
<li><p>做出最终决策：</p>
<p>现在，行玩家手上有一份“保底收益清单”，清单上的每一项都对应着他选择某一行后，在最坏情况下能得到的最低收益。作为一个想最大化自己收益的人，他会看着这份“保底清单”，然后选择那个能提供<strong>最高（Maximum）</strong>保底收益的选项。</p></li>
</ol>
<p>整个过程串起来就是：他先找出每一行的<strong>最小值
(Minimum)</strong>，然后在这些最小值中，选择一个<strong>最大值
(Maximum)</strong>。这就是<strong>最大化最小值 (Maximin)</strong>。</p>
<p>这是一种“在最坏的情况里，为自己争取最好的结果”的策略。</p>
<hr>
<p><strong>2. 为什么“最小化极大值 (Minimax)”是对手的策略？</strong></p>
<p>现在我们换位思考，站到<strong>列玩家（Player 2）</strong>的角度。</p>
<ol type="1">
<li><strong>他的目标</strong>：他的目标是最小化自己的损失，也就是最小化行玩家的收益。</li>
<li><strong>他的思考过程</strong>：
<ul>
<li><strong>审视他的第一个选项（比如“列1”）</strong>：“如果我出‘列1’，我的对手会怎么做？他会看遍‘列1’的所有结果，然后选择那个能让他自己收益<strong>最高</strong>的选项。所以，如果我出‘列1’，我可能遭受的<strong>最大损失</strong>（也就是行玩家的最大收益）就是这一列里的<strong>最大值</strong>。”</li>
<li><strong>审视他的第二个选项（比如“列2”）</strong>：“同理，如果我出‘列2’，我可能遭受的最大损失就是‘列2’里的<strong>最大值</strong>。”</li>
<li><strong>做出最终决策</strong>：现在，列玩家手上也有一份“最大损失清单”。作为一个想最小化自己损失的人，他会看着这份清单，然后选择那个能让他的<strong>最大损失变得最小
(Minimum)</strong> 的选项。</li>
</ul></li>
</ol>
<p>整个过程串起来就是：他先找出每一列的<strong>最大值
(Maximum)</strong>，然后在这些最大值中，选择一个<strong>最小值
(Minimum)</strong>。这就是<strong>最小化极大值 (Minimax)</strong>。</p>
<p><strong>总结</strong></p>
<table>
<colgroup>
<col style="width: 10%">
<col style="width: 40%">
<col style="width: 50%">
</colgroup>
<thead>
<tr>
<th></th>
<th><strong>行玩家 (Player 1)</strong></th>
<th><strong>列玩家 (Player 2)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>目标</strong></td>
<td>最大化自己的收益</td>
<td>最小化自己的损失（即最小化行玩家的收益）</td>
</tr>
<tr>
<td><strong>思想</strong></td>
<td>在最坏的情况里为自己争取最好结果</td>
<td>让对手在最好的情况下也占不到太大便宜</td>
</tr>
<tr>
<td><strong>策略</strong></td>
<td><strong>最大化最小值 (Maximin)</strong></td>
<td><strong>最小化极大值 (Minimax)</strong></td>
</tr>
</tbody>
</table>
<p>所以，行玩家之所以是“最大化最小值”，是因为他的出发点是<strong>为自己求利</strong>，同时假设对手会让他<strong>利无可利</strong>；而“最小化极大值”则是从对手的视角出发，目标是<strong>让行玩家的利益最小化</strong>。冯·诺依曼的惊人发现在于，在两人零和博弈中，这两种看似不同的思考路径，最终会指向同一个均衡解。</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250630215303870.png" alt="7-4">
<figcaption aria-hidden="true">7-4</figcaption>
</figure>
<p>这张幻灯片介绍了“序贯决策”框架中那个最普适、最强大的模型——<strong>随机博弈（或称马尔可夫博弈）</strong>。它完美地将我们之前讨论过的“矩阵博弈”和“马尔可夫决策过程(MDP)”融合在了一起，是理解现代<strong>多智能体强化学习
(MARL)</strong> 的理论基石。</p>
<p><strong>1. 集大成者：融合了博弈论与强化学习</strong></p>
<p>我们可以这样理解随机博弈的构成：</p>
<ul>
<li>它借鉴了<strong>矩阵博弈 (Matrix Games)</strong>
的核心思想：在任何一个时刻，都有<strong>多个智能体</strong>在进行策略互动。</li>
<li>它又借鉴了<strong>马尔可夫决策过程 (MDPs)</strong>
的核心思想：整个系统存在<strong>多个状态</strong>，环境会根据参与者的行动从一个状态转移到另一个状态。</li>
</ul>
<p><strong>随机博弈 = 多智能体的MDP = 随状态变化的重复博弈</strong></p>
<p><strong>2. 图示解读：一场动态演变的博弈</strong></p>
<p>这张图非常直观地展示了随机博弈是如何运作的。我们可以把它想象成一场动态的冒险游戏：</p>
<ol type="1">
<li><strong>身处状态</strong>：假设你和你的对手（参与者1和2）当前身处“<strong>状态1
(State 1)</strong>”。</li>
<li><strong>进行博弈</strong>：在这个状态下，你们必须玩“状态1”对应的那个2x2矩阵博弈。比如，你（P1）选择了“下”，对手（P2）选择了“右”。</li>
<li><strong>获得即时回报</strong>：根据矩阵，这个<code>(下, 右)</code>的联合行动会给你们带来
<code>(1, 1)</code> 的即时回报。</li>
<li><strong>世界发生改变（状态转移）</strong>：这是最关键的一步。你们的联合行动
<code>(下, 右)</code>
触发了状态转移。紫色的箭头告诉我们，接下来会发生什么：
<ul>
<li>有 <strong>40%</strong>
的概率，你们会进入“<strong>状态2</strong>”。</li>
<li>有 <strong>60%</strong>
的概率，你们会进入“<strong>状态3</strong>”。</li>
</ul></li>
<li><strong>进入新博弈</strong>：假设你们进入了“状态2”。现在，你们面对的是一个<strong>全新的2x2矩阵博弈</strong>，有着完全不同的收益规则。你们需要在这个新规则下再次决策，然后获得新的回报，并再次触发新的状态转移。</li>
</ol>
<p>这个过程会一直持续下去。</p>
<p><strong>3. 参与者的目标：深谋远虑</strong></p>
<p>在一个随机博弈中，一个理性的参与者不会只盯着当前这一轮的得失。他的决策必须是<strong>深谋远虑</strong>的。</p>
<ul>
<li>例如，在“状态1”中，<code>(上, 左)</code>这个选择能立刻带来<code>(2, 2)</code>的高回报。但如果这个选择有90%的概率会让你转移到一个对你极其不利的“惩罚状态”，那你可能就不会选它。</li>
<li>反之，你可能会选择一个即时回报较低的行动，如果它有很大概率把你带到一个未来回报极高的“天堂状态”。</li>
<li>这就是<strong>折扣回报 (discounted rewards)</strong>
的作用。玩家的目标是最大化未来所有回报的“总现值”，即找到一个在<strong>所有状态下</strong>都最优的策略（Policy），而不仅仅是当前状态。</li>
</ul>
<p><strong>总结</strong>：随机博弈为我们描绘了一幅最接近真实世界复杂性的图景——多个决策者在不断变化的环境中持续互动。我们之前讨论的<strong>多智能体学习（MAL）的困难</strong>，例如“环境的非平稳性”，正是源于这个框架。对于任何一个智能体来说，环境之所以“不稳定”，就是因为状态的转移和回报不仅取决于自己的行动，还取决于其他所有同样在学习和适应的智能体的行动。</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250630220528288.png" alt="7-5">
<figcaption aria-hidden="true">7-5</figcaption>
</figure>
<p><strong>标题：随机博弈 vs. MDP (Stochastic Games
vs. MDP)</strong></p>
<ul>
<li>在一个随机博弈中，如果除了一个参与者之外的所有其他参与者都采取<strong>固定的
(fixed)</strong>
策略，那么对于剩下的那个智能体来说，这个问题就<strong>退化 (reverts
back)</strong> 回了一个MDP。
<ul>
<li>—
这是因为，固定其他智能体的策略（即使这些策略是随机的），会使得状态转移变得<strong>马尔可夫化
(Markovian)</strong>，即只取决于剩下的那个参与者的行动。</li>
</ul></li>
</ul>
<hr>
<p>这张幻灯片通过一个“思想实验”，精准地指出了<strong>多智能体学习（MARL）与单智能体学习（RL）的根本区别到底在哪里</strong>。它告诉我们，多智能体问题的核心困难，并不在于“有多个会动的个体”，而在于“有多个会<strong>学习和适应</strong>的个体”。</p>
<p><strong>1. 问题复杂性的根源：变化的“游戏规则”</strong></p>
<p>我们之前讨论，多智能体学习之所以困难，是因为环境的“非平稳性”（Non-stationarity）。对于任何一个智能体“我”来说，其他的智能体都是环境的一部分。当其他智能体也在学习、也在改变他们的策略时，就相当于“我”所面对的游戏规则本身在不断变化，这让学习变得极为困难。</p>
<p><strong>2. “固定策略”意味着什么？—— 从“对手”到“自然规律”</strong></p>
<p>这张幻灯片提出的“固定其他所有人的策略”这个条件，是问题的关键。这意味着什么呢？让我们用一个生动的例子来说明：</p>
<ul>
<li><p>情景A：随机博弈 (Stochastic Game)</p>
<p>你是一个新手出租车司机（剩下的那个智能体），在一个大城市里学习如何最快地接送客人。城里还有成千上万的老司机（其他智能体），他们经验丰富，每天都在根据实时路况、新闻、个人习惯等调整自己的路线。这是一个极度复杂的多智能体学习问题，因为你的“环境”（即其他司机的行为）每天都在变。</p></li>
<li><p>情景B：退化为MDP</p>
<p>现在，假设奇迹发生，城里所有其他司机都被换成了简单的机器人。这些机器人的行为遵循一套永不改变的固定程序。例如：“在周一上午8点的A路口，这群机器人有70%会右转，30%会直行”。这个概率是固定的，机器人不会再学习或改变了。</p></li>
</ul>
<p>在这个时刻，对于你（唯一的人类司机）来说，发生了什么？</p>
<p>其他司机不再是具有主观能动性的“对手”了，他们变成了城市交通中一个虽然是随机的、但却是稳定的、可预测的自然规律。</p>
<p><strong>3. “马尔可夫化”：游戏规则被重新稳定下来</strong></p>
<p>一旦其他人的策略被固定，整个系统对于“我”来说，就重新满足了<strong>马尔可夫性质</strong>。</p>
<ul>
<li>当“我”在某个路口（状态s），选择直行（动作a），下一分钟会到达哪里、会花多长时间（下一个状态s’），其概率
<strong>p(s′∣s,a)</strong>
现在只取决于“我”的动作<code>a</code>和那群机器人固定的行为模式。</li>
<li>这个转移概率<strong>不再随时间变化</strong>了，因为机器人的“想法”不会变。</li>
<li>于是，这个复杂的、不可预测的多智能体博弈，就<strong>退化</strong>成了一个经典的、可解的<strong>单智能体马尔可夫决策过程（MDP）</strong>。</li>
</ul>
<p>总结：</p>
<p>这张幻灯片通过“固定他人策略”这一巧妙的设定，帮助我们隔离并识别了多智能体问题的“困难之源”。困难不在于环境中存在多个行动者，而在于这些行动者策略的动态演化。这个洞见是许多高级多智能体学习算法的基础，例如，有些算法就是通过“轮流学习”（一个学习，其他暂时固定）或者对其他智能体的策略进行建模和预测，来试图克服这种“非平稳性”带来的挑战。</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250630222547667.png" alt="7-6">
<figcaption aria-hidden="true">7-6</figcaption>
</figure>
<p>这张幻灯片将原本简单的单次博弈，升级成了一个更复杂、也更贴近现实的<strong>双状态随机博弈
(Two-state Stochastic
Game)</strong>。这里的“纳什均衡”不再是单个的行动组合，而是一个<strong>策略组合
(a profile of
policies)</strong>，即每个厂商在<strong>每种状态下</strong>应该如何行动的完整计划。</p>
<p>这个问题的解取决于一个幻灯片上未给出但至关重要的参数——<strong>折扣因子
γ (discount factor)</strong>，它代表了厂商对未来收益的重视程度。</p>
<p><strong>1. 分解两个“子博弈”</strong></p>
<p>我们首先分析在每个状态下，只考虑当前一轮收益的“短视”均衡是什么。</p>
<ul>
<li><p>在状态1（无税收）：</p>
<p>正如我们上一题分析的，双方都有一个占优策略：“污染”。因此，该状态下的短视纳什均衡是
(污染, 污染)。但这个选择会带来一个后果：根据转移概率
(0,1)，游戏将100%转移到状态2（有税收）。</p></li>
<li><p>在状态2（有税收）：</p>
<p>我们分析这个新的利润矩阵：</p>
<ul>
<li>对厂商1：如果厂商2选择“清洁”，厂商1会选“污染”(4&gt;1)；如果厂商2选“污染”，厂商1还是会选“污染”(3&gt;0)。<strong>“污染”是厂商1的占优策略</strong>。</li>
<li>对厂商2：如果厂商1选择“清洁”，厂商2会选“污染”(5&gt;2)；如果厂商1选“污染”，厂商2还是会选“污染”(4&gt;1)。<strong>“污染”也是厂商2的占优策略</strong>。</li>
<li>因此，该状态下的短视纳什均衡也是 <strong>(污染,
污染)</strong>。这个选择的后果是，游戏将<strong>100%留在状态2</strong>，继续被征税。</li>
</ul></li>
</ul>
<p><strong>2. 长期战略的困境：短视 vs. 远见</strong></p>
<p>分析完子博弈后，真正的战略困境浮现了。我们以厂商1在<strong>状态1</strong>的决策为例：</p>
<ul>
<li><strong>短视的选择（选择“污染”）</strong>：可以立刻获得很高的收益（如果对方也污染，能得6）。但代价是，从下一轮开始，将永远陷入低收益的“状态2”。</li>
<li><strong>远见的选择（选择“清洁”）</strong>：会牺牲掉一部分即时收益（如果对方也清洁，只能得4）。但好处是，游戏将<strong>100%留在高收益的“状态1”</strong>，未来每一轮都可以继续获得高收益。</li>
</ul>
<p>厂商会如何选择，完全取决于他们有多“远视”，即折扣因子 <code>γ</code>
有多大。</p>
<p><strong>3. 可能存在的纳什均衡</strong></p>
<p>这个随机博弈可能存在多个纳什均衡。</p>
<p><strong>均衡A：“悲观”的污染均衡</strong></p>
<ul>
<li><strong>策略</strong>：无论在哪种状态，双方都选择“污染”。</li>
<li><strong>分析</strong>：如果对方的策略是“永远污染”，那么你最好的应对也是“永远污染”。因为如果你单方面选择“清洁”，在状态1你会获得更低的即时收益(3
vs 6)然后还是会进入状态2；在状态2你单方面“清洁”的收益(0 vs
3)也更低。因此，没有任何一方有单方面改变策略的动机。</li>
<li><strong>结论</strong>：<strong>（策略1=污染,
策略2=污染）是一个纳什均衡</strong>。在这个均衡下，厂商们第一轮在状态1获得(6,7)的收益，然后永久地陷入状态2，每轮获得(3,4)的收益。这是一个低效的、“双输”的均衡。</li>
</ul>
<p><strong>均衡B：“合作”的清洁均衡</strong></p>
<ul>
<li><p><strong>策略</strong>：双方约定，只要在状态1，就都选择“清洁”。</p></li>
<li><p><strong>分析</strong>：要让这个“君子协定”成为一个稳定的纳什均衡，就必须保证“背叛”是无利可图的。</p>
<ul>
<li><p><strong>遵守协定</strong>的收益流（以厂商1为例）：4+4γ+4γ2+⋯=1−γ4</p></li>
<li><p><strong>单方面背叛</strong>的收益流：在状态1选择“污染”获得一次性的高收益7，但之后游戏进入状态2，双方陷入“永远污染”的均衡，后续每轮收益为3。其收益流为:
7+3γ+3γ2+⋯=7+1−γ3γ</p></li>
<li><p>要让大家遵守协定，必须满足“遵守的收益 ≥ 背叛的收益”：</p>
<p>1−γ4≥7+1−γ3γ</p>
<p>4≥7(1−γ)+3γ⟹4≥7−4γ⟹4γ≥3⟹γ≥43</p></li>
</ul></li>
<li><p><strong>结论</strong>：<strong>如果厂商们对未来的重视程度足够高（即折扣因子
γ ≥
3/4），那么双方在状态1都选择“清洁”也可以成为一个纳什均衡</strong>。因为对未来的长期高收益的渴望，足以抑制住当前“背叛”以获取短期利益的诱惑。</p></li>
</ul>
<p><strong>总结</strong>：这个引入了税收和状态转移的随机博弈模型，比单次博弈要复杂和深刻得多。它的均衡不再是唯一的，而是可能存在多个——一个“坏”的均衡和一个“好”的均衡。系统最终会落入哪个均衡，取决于参与者对未来的耐心和期望（由<code>γ</code>体现）。这也为政府政策的设计提供了启示：一个好的制度，应该让“合作”的门槛（即所需的<code>γ</code>值）尽可能低，让参与者更容易达成对社会有利的结果。</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250630223314800.png" alt="7-7">
<figcaption aria-hidden="true">7-7</figcaption>
</figure>
<p><strong>标题：例子：“大冒险”游戏 (Example: the game of
Dare)</strong></p>
<ul>
<li><p>参与者1，<strong>领导者 (the
leader)</strong>，和参与者2，<strong>挑战者 (the
challenger)</strong>，同时选择“<strong>放弃
(pass)</strong>”或“<strong>挑战 (dare)</strong>”。</p>
<ul>
<li>— 如果双方都选择“放弃”，支付为零（且游戏结束）。</li>
<li>— 如果参与者1“放弃”而参与者2“挑战”，参与者1赢得1。</li>
<li>— 如果参与者1“挑战”而参与者2“放弃”，参与者1赢得3。</li>
<li>—
如果双方都选择“挑战”，这个基础游戏将<strong>角色互换</strong>后重新进行。
<ul>
<li>(领导者变成挑战者，反之亦然)。</li>
</ul></li>
<li>— 如果参与者们永远地持续“挑战”下去，则支付为零。</li>
</ul></li>
<li><p>博弈矩阵 G:</p>
<p><span class="math display">$$G = \bordermatrix{ &amp; \text{放弃}
&amp; \text{挑战} \cr \text{放弃} &amp; 0 &amp; 1 \cr \text{挑战} &amp;
3 &amp; -G^T }$$</span></p>
<p>其中 -Gᵀ
代表角色互换后的游戏。（它的矩阵是G矩阵的转置的负数。）-Gᵀ的价值是G的价值的负数。</p></li>
</ul>
<hr>
<p>这张幻灯片描述了一个非常有趣的<strong>递归博弈 (recursive
game)</strong>，它是一种特殊的<strong>随机博弈 (stochastic
game)</strong>。这个博弈只有两个状态：“P1是领导者”和“P2是领导者”。当出现
(挑战, 挑战) 的结果时，游戏就在这两个状态之间切换。</p>
<p><strong>1. 问题的核心：递归的价值</strong></p>
<p>这个问题的精髓在于右下角的那个支付 <code>-Gᵀ</code>。</p>
<ul>
<li><p>让我们设这个博弈对于<strong>当前的领导者</strong>来说，其<strong>价值
(Value)</strong> 为 <strong>V</strong>。</p></li>
<li><p>那么，领导者的支付矩阵就可以写成：</p>
<p>G=(031Value(subgame))</p></li>
<li><p>当双方都选择“挑战”时，游戏进入子博弈。在这个子博弈中，原先的挑战者（P2）变成了新的领导者。由于游戏的对称性，这个子博弈对于<strong>新的领导者（P2）</strong>来说，价值也应该是
<strong>V</strong>。</p></li>
<li><p>既然对于新的领导者（P2）价值是
<code>V</code>，那么对于<strong>新的挑战者（也就是原来的P1）</strong>来说，价值就是
<strong>-V</strong>（因为是零和博弈）。</p></li>
<li><p>因此，原领导者（P1）的支付矩阵可以写成一个包含其自身价值 V
的形式：</p>
<p>G=(031−V)</p></li>
</ul>
<p><strong>2. 求解博弈价值 V</strong></p>
<p>现在，问题转化为了：求解这个特殊矩阵的价值 V，并且这个价值 V
必须等于它自身。</p>
<p><span class="math display">$$V = \text{value} \begin{pmatrix} 0 &amp;
1 \ 3 &amp; -V \end{pmatrix}$$</span>我们可以使用之前推导出的 2x2
零和博弈的通用价值公式：<span class="math display">$$V = \frac{ac -
bd}{(a+c) - (b+d)}$$</span>其中，a=0, b=1, d=3, c=-V。代入公式：<span class="math display">$$V = \frac{(0)(-V) - (1)(3)}{(0)+(-V) - (1)-(3)} =
\frac{-3}{-V-4}$$</span>现在我们得到了一个关于 V 的方程，求解它：</p>
<p><span class="math display"><em>V</em>(−<em>V</em> − 4) = −3</span><span class="math display">−<em>V</em><sup>2</sup> − 4<em>V</em> = −3</span><span class="math display"><em>V</em><sup>2</sup> + 4<em>V</em> − 3 = 0</span></p>
<p>这是一个一元二次方程。使用求根公式</p>
<p>我们得到了两个可能的解：V1=7−2≈0.646 和 V2=−7−2≈−4.646。</p>
<p><strong>3. 选择正确的解</strong></p>
<p>哪个才是这个博弈真正的价值呢？</p>
<p>我们看领导者的支付矩阵，他有一个“放弃”的选项。如果他选择“放弃”，最坏的结果是对手选择“挑战”，此时他的收益是1。这意味着，领导者至少可以为自己保证一个非负的收益。因此，一个负数（比如-4.646）不可能是这个博弈的理性价值。</p>
<p>所以，这个博弈对于领导者的价值是：根号7减去2</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250630224225904.png" alt="7-8">
<figcaption aria-hidden="true">7-8</figcaption>
</figure>
<p>求解博弈G1和G2的价值：</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250630224137356.png" alt="7-9">
<figcaption aria-hidden="true">7-9</figcaption>
</figure>
<h3 id="值迭代与策略迭代">7.2 值迭代与策略迭代</h3>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250701210516613.png" alt="7-10">
<figcaption aria-hidden="true">7-10</figcaption>
</figure>
<p>1、学习随机博弈中状态值函数贝尔曼方程推导</p>
<p>2、与单智能体MDP的关键区别</p>
<p>这个公式虽然形式上与单智能体MDP的贝尔曼方程很像，但幻灯片的最后一点指出了两个根本性的区别，这也是多智能体问题复杂性的根源：</p>
<ol type="1">
<li><strong>价值是个人化的 (for each
agent)</strong>：在MDP中，只有一个价值函数
<code>V(s)</code>。但在随机博弈中，<strong>每个参与者 <code>i</code>
都有自己的一套价值函数 Vi(s)</strong>。同一个状态
<code>s</code>，对我来说可能是天堂（Vi很高），对你来说可能却是地狱（Vj很低）。这体现了参与者之间合作与冲突并存的关系。</li>
<li><strong>价值依赖于联合策略 (on the joint
policy)</strong>：这是最致命的区别。在MDP中，价值函数 Vπ(s)
只取决于我自己的策略 <code>π</code>。但在这里，Viπ(s)
的值不仅取决于我的策略 πi，还取决于<strong>其他所有人的策略</strong>
π−i，因为是<strong>联合行动</strong>决定了回报和状态转移。
<ul>
<li>这就导致了我们之前讨论的<strong>“非平稳性”</strong>或<strong>“移动靶心”</strong>问题。如果我的对手改变了他的策略，那么即使我的策略和当前状态都没变，我整个的价值函数
Vi(s)
也会跟着改变。我原以为很有价值的状态，可能因为对手策略的改变而突然变得一文不值。</li>
</ul></li>
</ol>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250701211212340.png" alt="7-11">
<figcaption aria-hidden="true">7-11</figcaption>
</figure>
<p>这张幻灯片在重申了状态价值的定义之后，提出了一个关于它的非常重要的数学性质：<strong>有界性
(Boundedness)</strong>。这个性质是随机博弈能够被分析和求解的理论基础之一。</p>
<p><strong>1. 核心思想：无限过程，有限价值</strong></p>
<p>这条结论的核心思想是：尽管一个随机博弈的过程可能永远持续下去，但从任何一个状态开始，任何一个参与者能够获得的<strong>总的“折扣”价值都不是无限的，而是一个有限的、有上限的数值</strong>。</p>
<p>幻灯片给出了这个上限的计算公式：1−γM。</p>
<ul>
<li><strong>M</strong>:
代表在整个游戏所有可能的情况下，任何参与者在<strong>单一一轮</strong>中所能获得的<strong>最大绝对收益</strong>。可以理解为这个游戏里“单次操作的最大奖励或最大惩罚（的绝对值）”。</li>
<li><strong>γ (gamma)</strong>: 是我们熟悉的折扣因子（0 &lt; γ &lt;
1），代表了我们对未来收益的耐心程度。</li>
</ul>
<p><strong>2. 这个上限公式是怎么来的？（几何级数）</strong></p>
<p>这个公式的推导非常直观，它基于我们熟知的等比数列（几何级数）求和。</p>
<ol type="1">
<li><p>根据定义，状态价值是所有未来折扣回报的总和：</p>
<p>Viπ(s)=Eπ[r0+γr1+γ2r2+γ3r3+…]</p></li>
<li><p>在任何一步 <code>k</code>，我们能获得的即时回报 rk
的绝对值，都不可能超过定义好的最大单轮回报 <code>M</code>。即
∣rk∣≤M。</p></li>
<li><p>因此，总价值 Viπ(s)
必然小于或等于一种最极端、最理想的情况：假设我们在未来的每一步，都能幸运地获得最大的正回报
M。</p>
<p>Viπ(s)≤M+γM+γ2M+γ3M+…</p></li>
<li><p>将 M 提取出来：</p>
<p>Viπ(s)≤M(1+γ+γ2+γ3+…)</p></li>
<li><p>括号里的部分是一个公比为 <code>γ</code> 的无穷等比数列。因为
<code>γ &lt; 1</code>，这个级数是收敛的，其和为 1−γ1。</p></li>
<li><p>因此，我们得到了最终的边界：</p>
<p>Viπ(s)≤1−γM</p></li>
</ol>
<p><strong>3. 这个性质为什么重要？</strong></p>
<ol type="1">
<li><strong>保证问题“有解”</strong>：这个有界性保证了我们要求解的状态价值函数是一个“行为良好”的函数，它的值不会发散到无穷大。这是所有后续分析和算法能够成立的数学前提。如果没有这个保证，我们可能都无法定义“最优策略”，因为所有策略的总回报都是无穷大，无法比较。</li>
<li><strong>为算法提供基础</strong>：在很多求解随机博弈的算法（例如价值迭代）中，这个边界可以用于初始化价值函数，或者作为算法收敛性的一个判断依据。它确保了算法的计算过程会在一个有限的数值空间内进行，最终能够稳定下来。</li>
</ol>
<p><strong>总结</strong>：这张幻灯片的核心是告诉我们，尽管随机博弈的博弈过程可能是无限的，但其价值是有限的。这个有界性不仅为问题的“可解性”提供了理论保障，也为实际的计算算法奠定了基础。</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250701215722881.png" alt="7-12">
<figcaption aria-hidden="true">7-12</figcaption>
</figure>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250701215827727.png" alt="7-13">
<figcaption aria-hidden="true">7-13</figcaption>
</figure>
<p><strong>结论与洞察</strong></p>
<p>这个例子深刻地揭示了<strong>短期利益与长期战略</strong>之间的权衡。</p>
<ul>
<li>从短期看，第1列对列玩家更有利（即时损失更小）。</li>
<li>但从长期看，第1列有更高的概率让游戏继续下去，这意味着他未来要持续地向行玩家支付价值为<code>v</code>的收益。而第2列能更快地结束游戏，从而“止损”。</li>
<li>最终的均衡策略显示，<strong>长期战略（避免未来损失）的重要性压倒了短期利益</strong>。因此，列玩家的最优策略是<strong>更频繁地选择那个能更快结束游戏的第2列</strong>，尽管它眼前的损失看起来更大。</li>
</ul>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250702003904470.png" alt="7-14">
<figcaption aria-hidden="true">7-14</figcaption>
</figure>
<p><strong>标题：价值迭代 (Value Iteration)</strong></p>
<ul>
<li>夏普利证明了 vn(s) 会收敛到从s开始的随机博弈的<strong>真实价值
v(s)</strong>。
<ul>
<li>— 首先，收敛是以<strong>指数速率 (exponential rate)</strong>
进行的：最大误差至少以 γn 的速度下降。</li>
<li>— 其次，在第 n+1
阶段的最大误差，至多是“<strong>从n到n+1阶段的最大变化量</strong>”乘以
γ/(1−γ)。</li>
</ul></li>
</ul>
<hr>
<p>这张幻灯片深入探讨了“价值迭代”算法的<strong>性能保证</strong>。它告诉我们，夏普利不仅证明了价值迭代这个方法是<strong>可行</strong>的（即最终能找到正确答案），更证明了它是<strong>高效</strong>和<strong>可靠</strong>的。这使得价值迭代从一个理论上的概念，变成了一个可以在实践中应用的强大工具。</p>
<p><strong>1. 指数速率收敛：为什么说它“高效”？</strong></p>
<p>“指数速率收敛”听起来很抽象，但它的意思是，算法的精确度在每一步迭代后都会得到一个“质的飞跃”。</p>
<ul>
<li><strong>一个比喻</strong>：想象你在寻宝，宝藏在1公里外。有一个向导，你每走一步，他都会告诉你：“你现在离宝藏的距离，是你上一步距离的90%”。
<ul>
<li>你的误差（与宝藏的距离）在每一步都会乘以一个固定的因子（0.9）。</li>
<li>第一次迭代后，误差是 1×0.9。</li>
<li>第二次迭代后，误差是 1×0.92。</li>
<li>第n次迭代后，误差是 1×0.9n。</li>
<li>误差以 0.9n 的速度急剧缩小，这就是指数级的衰减。</li>
</ul></li>
<li><strong>在价值迭代中</strong>：折扣因子 <code>γ</code>
(一个小于1的数) 就扮演了这个“0.9”的角色。每迭代一次，我们估算的价值函数
vn(s) 与真实价值 v(s) 之间的最大误差，都会大致缩小一个 <code>γ</code>
倍。因为 <code>γ</code>
小于1，所以经过多次迭代后，误差会变得非常小，算法能很快地逼近真实解。这背后的数学原理是，夏普利证明了价值迭代的更新算子是一个<strong>压缩映射
(Contraction Mapping)</strong>。</li>
</ul>
<p><strong>2. 误差边界：为什么说它“可靠”？</strong></p>
<p>第二点结论解决了一个非常实际的问题：“我怎么知道什么时候可以停止算法，并且保证我的答案足够精确了？”</p>
<ul>
<li><p><strong>面临的困境</strong>：我们希望我们的误差，即
<code>|我们的估算值 vₙ - 真实值 v*|</code>，小于一个我们能接受的阈值（比如0.001）。但问题是，我们并不知道那个神秘的“真实值
<code>v*</code>”到底是多少，所以无法直接计算这个误差。</p></li>
<li><p>夏普利提供的解决方案：他给出了一个可计算的误差上限。公式告诉我们：</p>
<p>真实的未知误差≤可计算的最大单步变化量×1−γγ</p>
<p>这里的“可计算的最大单-步变化量”指的是
∣vn+1(s)−vn(s)∣max，也就是在你最近一次迭代中，所有状态的价值估算值变化最大的那一个。这个值我们在计算过程中是完全知道的。</p></li>
<li><p><strong>实际应用（停止条件）</strong>：</p>
<ol type="1">
<li>我们设定一个目标精度
<code>ε</code>，比如我希望我的最终答案与真实值的误差不超过0.001。</li>
<li>根据公式，只要我们能让
<code>(可计算的最大单步变化量) * γ/(1-γ)</code> 这个上限小于
<code>ε</code>，那么真实的误差就一定小于 <code>ε</code>。</li>
<li>因此，我们的算法停止条件就变成了：<strong>持续迭代，直到我们观察到的最大单步变化量小于
ϵ⋅γ1−γ</strong>。</li>
<li>一旦满足这个条件，我们就可以放心地停止迭代，并宣布当前的估算值 vn+1
就是一个足够精确的解。</li>
</ol></li>
</ul>
<p><strong>总结</strong>：这张幻灯片从理论上为价值迭代算法的有效性提供了强有力的背书。<strong>指数速率收敛</strong>保证了它的计算速度，而<strong>实用的误差边界</strong>则为它在现实中的应用提供了可靠的停止准则，使其成为求解两人零和随机博弈的核心算法之一。</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250702005255997.png" alt="7-15">
<figcaption aria-hidden="true">7-15</figcaption>
</figure>
<p><strong>为什么最小停止概率是0.5？</strong></p>
<p><strong>1. “最小停止概率是0.5”的来源分析</strong></p>
<p>这个结论来自于对 G₁ 和 G₂
两个矩阵中所有单元格的<strong>“持续概率”</strong>的分析。</p>
<p>在一个随机博弈的支付单元格中，形如
<code>“即时回报 + 概率 × 未来价值”</code>
的结构，那个<strong>概率</strong>就代表了游戏<strong>继续下去的可能性</strong>。而<strong>“停止概率”</strong>则等于
<strong>1 - 继续概率</strong>。</p>
<p>让我们来逐一检查两个矩阵中所有结果的“停止概率”：</p>
<p>对于博弈 G₁:</p>
<p>G(1)=(4+0.3G(1)1+0.4G(2)0+0.4G(2)3+0.5G(1))</p>
<ul>
<li>(行1, 列1): 继续概率是 0.3。 <strong>停止概率 = 1 - 0.3 =
0.7</strong></li>
<li>(行1, 列2): 继续概率是 0.4。 <strong>停止概率 = 1 - 0.4 =
0.6</strong></li>
<li>(行2, 列1): 继续概率是 0.4。 <strong>停止概率 = 1 - 0.4 =
0.6</strong></li>
<li>(行2, 列2): 继续概率是 0.5。 <strong>停止概率 = 1 - 0.5 =
0.5</strong></li>
</ul>
<p>对于博弈 G₂:</p>
<p>G(2)=(0+0.5G(1)−4−51+0.5G(2))</p>
<ul>
<li>(行1, 列1): 继续概率是 0.5。 <strong>停止概率 = 1 - 0.5 =
0.5</strong></li>
<li>(行1, 列2): 支付是-5（没有未来价值项）。继续概率是
0。<strong>停止概率 = 1 - 0 = 1</strong></li>
<li>(行2, 列1): 支付是-4。继续概率是 0。<strong>停止概率 = 1 - 0 =
1</strong></li>
<li>(行2, 列2): 继续概率是 0.5。<strong>停止概率 = 1 - 0.5 =
0.5</strong></li>
</ul>
<p>现在，我们把所有计算出的停止概率放在一起：{ 0.7, 0.6, 0.5, 1 }。</p>
<p>在所有这些可能性中，最小的那个值，就是 0.5。</p>
<p>这就是“最小停止概率是0.5”这句话的直接来源。</p>
<p><strong>2. 这个数字为什么如此重要？</strong></p>
<p>这个“最小停止概率”反过来告诉了我们整个系统的<strong>“最大继续概率”</strong>。</p>
<ul>
<li>最小停止概率 = 0.5</li>
<li>最大继续概率 = 1 - 最小停止概率 = 1 - 0.5 = 0.5</li>
</ul>
<p>这个<strong>“最大继续概率”</strong>，可以被看作是整个随机博弈系统的<strong>有效折扣因子
γ (effective discount factor)</strong>。</p>
<p>为什么呢？</p>
<p>价值迭代算法的收敛速度，取决于其更新算子是不是一个“压缩映射”，而其“压缩程度”就由折扣因子γ决定。为了保证整个系统一定收敛，我们必须考虑最坏的情况。</p>
<ul>
<li><strong>收敛的最坏情况是什么？</strong> 就是收敛得最慢的情况。</li>
<li><strong>什么时候收敛得最慢？</strong>
就是“折扣”打得最少的时候，也就是游戏最不容易结束、<strong>继续下去的概率最大</strong>的时候。</li>
</ul>
<p>在这个博弈中，游戏继续下去的最大概率是0.5。因此，整个价值迭代算法的收敛速度就由这个0.5来决定。</p>
<ul>
<li><strong>收敛速率</strong>：因为有效折扣因子<code>γ</code>是0.5，所以算法的误差是以
(0.5)n 的指数速率下降的。</li>
<li><strong>误差边界</strong>：根据我们之前学过的误差边界公式
<code>真实误差 ≤ 最大单步变化量 × γ/(1-γ)</code>，代入<code>γ=0.5</code>，我们得到
<code>γ/(1-γ) = 0.5/0.5 = 1</code>。这意味着，真实的未知误差，不会超过我们在上一步迭代中能观测到的最大变化量。幻灯片中说v₆的误差至多是0.0002，就是基于这个原理计算出来的（从v₅到v₆的最大变化量约为0.0001，其上限0.0002是完全正确的）。</li>
</ul>
<p><strong>总结</strong>：
“最小停止概率是0.5”这个结论，是通过分析所有可能结果得出的。它的真正意义在于，它为我们确定了整个动态系统的有效折扣因子
<code>γ=0.5</code>，从而为算法的<strong>收敛速度</strong>和<strong>误差分析</strong>提供了坚实的理论依据。</p>
<h3 id="均衡学习">7.3 均衡学习</h3>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250708201920820.png" alt="7-16">
<figcaption aria-hidden="true">7-16</figcaption>
</figure>
<p><strong>标题：均衡学习器 (Equilibrium Learners)</strong></p>
<ul>
<li><strong>均衡学习器</strong>的目标是找到作为随机博弈<strong>纳什均衡</strong>的策略。
<ul>
<li>—
由于找到这类均衡很困难，它们通常专注于一个较小的问题类别，例如<strong>零和博弈</strong>或<strong>两人一般和博弈</strong>。</li>
</ul></li>
<li>找到纳什均衡的优点在于，智能体可以学到一个性能的<strong>下界 (lower
bound)</strong>，并且，
<ul>
<li>—
在这种情况下，它变得相当<strong>独立于</strong>其他智能体所采取的策略。</li>
<li>— 它<strong>至少会获得</strong>与该均衡相对应的回报量。</li>
</ul></li>
</ul>
<hr>
<p>这张幻灯片介绍了一类在多智能体学习（MAL）中非常重要的算法思想——<strong>均衡学习器</strong>。这类算法的目标非常明确和“学院派”：它们不像其他一些学习器那样只追求“打败对手”或“最大化短期回报”，而是试图直接<strong>计算并执行</strong>整个博弈的<strong>纳什均衡策略</strong>。</p>
<ol type="1">
<li><strong>什么是“均衡学习器”？（扮演博弈论家的智能体）</strong></li>
</ol>
<p>我们可以把“均衡学习器”想象成一个内置了博弈论模型的“理性人”。它的工作流程是：</p>
<ol type="1">
<li>观察和建模：通过与环境和其他智能体的交互，学习并构建出当前博弈的规则（即支付矩阵）。</li>
<li><strong>计算均衡</strong>：利用我们之前学过的算法（如线性规划、迭代法等）来计算出这个博弈的纳什均衡解。</li>
<li>执行策略：按照计算出的纳什均衡策略（通常是混合策略）来行动。</li>
</ol>
<p>这种方法与其他一些学习方法（比如，只根据对手上一步行动来调整策略的“最优反应”学习器）形成了鲜明对比。</p>
<ol start="2" type="1">
<li><strong>核心优势：稳健性与“安全网”</strong></li>
</ol>
<p>幻灯片的核心观点是，采取这种方法的<strong>最大优势在于其稳健性
(robustness) 和它提供的一个“安全网”</strong>。</p>
<ul>
<li><strong>性能下界 (Lower
Bound)</strong>：纳什均衡策略（尤其是在零和博弈中的极小化极大策略）本质上是一种<strong>防御性策略</strong>。它的设计初衷就是为了应对一个完美的、理性的、并且想尽办法针对你的对手。</li>
<li><strong>独立于对手策略</strong>：当你执行纳什均衡策略时，你等于为自己的表现设定了一个<strong>最坏情况下的保证</strong>。无论对手多么聪明，只要他也是理性的，他就无法让你得到比这个均衡价值更差的结果。从这个意义上说，你的表现变得“独立于”对手的具体策略，因为你已经为所有最坏的情况做好了准备。</li>
<li><strong>一个生动的例子</strong>：在“石头剪刀布”中，你的纳什均衡策略是完全随机地出招（各1/3概率）。当你这样做时，你为自己保证的“性能下界”是期望收益为0（不输不赢）。你的对手，无论他怎么出招，长期来看都无法让你输钱。但如果你放弃这个均衡策略，比如你决定100%出“石头”，那你的“安全网”就没了，对手可以100%出“布”来稳定地击败你。</li>
</ul>
<ol start="3" type="1">
<li><strong>局限与挑战</strong></li>
</ol>
<p>当然，这种方法也有其固有的缺点，这也是为什么它不是多智能体学习中唯一的范式：</p>
<ol type="1">
<li><strong>计算复杂度高</strong>：正如幻灯片和我们之前的讨论所指出的，计算纳什均衡（尤其是在非零和或多于两人的游戏中）是一个计算上非常困难的问题。这限制了均衡学习器在大型、复杂问题上的应用。</li>
<li><strong>对“理性对手”的强假设</strong>：这种方法最大的前提是，它假设你的对手也是一个完美的“均衡学习器”。但如果你的对手是一个新手，或者是一个非理性的、有固定套路的玩家呢？在这种情况下，你那套“万无一失”的均衡策略可能就显得过于保守了。一个更具攻击性、更具“剥削性”的策略，可能会让你获得远高于均衡价值的收益。</li>
<li><strong>多重均衡问题</strong>：在很多一般和博弈中，可能存在多个纳什均衡点。如果不同的学习器各自计算并收敛到了不同的均衡目标上，它们之间可能无法有效协调，导致一个糟糕的集体结果。</li>
</ol>
<p><strong>总结</strong>：均衡学习器是一种理论驱动的、非常强大的多智能体学习方法。它的核心优势在于<strong>稳健可靠，能提供最坏情况下的性能保证</strong>。它的主要缺点在于<strong>计算成本高昂，且在面对非理性或次优对手时可能过于保守</strong>。它在两人零和博弈这类结构清晰、均衡唯一的场景中最为有效。</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250708202817638.png" alt="7-17">
<figcaption aria-hidden="true">7-17</figcaption>
</figure>
<p>这张幻灯片将强化学习中另一个核心概念——<strong>Q值（或称状态-动作价值）</strong>——引入到了随机博弈的框架中。它揭示了Q值在多智能体环境下的定义，并含蓄地指出了由此带来的巨大挑战。</p>
<ol type="1">
<li><strong>Q值与V值的关系</strong></li>
</ol>
<p>首先，我们需要理解Q值和我们之前讨论的V值（状态价值）之间的关系。它们是同一枚硬币的两面：</p>
<ul>
<li><strong>V值 (State Value)</strong> Viπ(s)
回答的是：“在状态<code>s</code>下，长期来看我能得到多少好处？”</li>
<li><strong>Q值 (Action Value)</strong> Qiπ(s,a)
回答的是：“在状态<code>s</code>下，如果我们<strong>这次</strong>采取联合行动<code>a</code>，长期来看我能得到多少好处？”</li>
</ul>
<p>它们之间的关系非常清晰：</p>
<ul>
<li><p>从Q到V：一个状态的价值（V），等于在该状态下根据策略π所能采取的所有动作的Q值的期望。</p>
<p>Viπ(s)=∑aπ(a∣s)Qiπ(s,a)</p></li>
<li><p>从V到Q：一个动作的价值（Q），等于采取该动作后的即时回报，加上所有可能进入的下一个状态的折扣V值的期望。</p>
<p>Qiπ(s,a)=ri(s,a)+γ∑s′p(s′∣s,a)Viπ(s′)</p>
<p>(注：这里假设了回报r是关于(s,a)的函数)</p></li>
</ul>
<p>幻灯片上的推导过程正是展示了这层关系。</p>
<ol start="2" type="1">
<li><strong>多智能体Q值的“灾难”：联合行动空间</strong></li>
</ol>
<p>幻灯片的最后一点“个体的Q值也取决于所有参与者的行动”是理解多智能体学习困难的关键。这看起来是一句废话，但其背后隐藏着<strong>指数级的复杂性</strong>。</p>
<ul>
<li><p>在单智能体中 (MDP)：</p>
<p>Q函数是
Q(s,a)，其中a是我个人的行动。要找到最优动作，我只需要在我的几个可选动作中，找到那个能使
Q(s,a) 最大的a即可。这很简单。</p></li>
<li><p>在多智能体中 (Stochastic Game)：</p>
<p>Q函数是 Qi(s,a)，其中 a=(a1,a2,…,an) 是一个联合行动向量。</p></li>
</ul>
<p>这就带来了两个灾难性的问题：</p>
<ol type="1">
<li><strong>维数灾难 (Curse of
Dimensionality)</strong>：我（智能体<code>i</code>）的Q值，取决于我、你、他、所有人共同做了什么。如果每个智能体有10个动作，总共有5个智能体，那么联合行动空间的大小就是
105=10万。要为每一个联合行动都计算和存储一个Q值，在计算和存储上都是一个巨大的挑战。</li>
<li><strong>不确定性与协调问题</strong>：即使我能计算出所有可能的
Qi(s,a)，在做决策时，我只知道我自己的动作 ai。为了选择最优的
ai，我必须<strong>预测或假设</strong>其他所有参与者 a−i
将会采取什么行动。
<ul>
<li>我怎么知道他们会做什么？</li>
<li>他们也在同时预测我将做什么。</li>
<li>这就形成了一个复杂的“鸡生蛋，蛋生鸡”的循环。我需要找到一个我的最佳行动
ai∗，它是我对他们最佳行动 a−i∗ 的最佳回应；而他们的 a−i∗
也必须是他们对我 ai∗ 的最佳回应。</li>
<li>这恰恰就是<strong>寻找当前状态下这个“子博弈”的纳什均衡</strong>的过程。</li>
</ul></li>
</ol>
<p>总结：</p>
<p>这张幻灯片定义了多智能体环境下的Q值。虽然其贝尔曼方程在形式上与单智能体情况相似，但其核心变量从个体行动变成了联合行动。这一根本性的转变，使得基于Q学习的多智能体算法面临着联合行动空间爆炸和智能体间决策高度耦合两大挑战，这也是多智能体强化学习（MARL）领域需要不断探索和解决的核心难题。</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250708204114402.png" alt="7-18">
<figcaption aria-hidden="true">7-18</figcaption>
</figure>
<p><strong>标题：均衡学习器 (Equilibrium Learners)</strong></p>
<ul>
<li><p>通常，一个均衡学习器的解，是以下方程组在 π∗=(πi∗,π−i∗)
上的一个不动点 (fixed point)：</p>
<p>∀i=1…nQi∗(s,a)=ri(s,a)+γs′∑p(s′∣s,a)Vi∗(s′)</p>
<ul>
<li>— 其中 Vi∗(s′) 代表当所有人都执行纳什均衡联合策略 π∗ 时，智能体 i
的<strong>均衡价值</strong>，并且</li>
<li>— 它是根据 <strong>Q值</strong> 来计算的。</li>
<li>—
这与<strong>贝尔曼最优方程</strong>很相似，区别在于<strong>状态价值函数的计算方式</strong>。</li>
</ul></li>
</ul>
<hr>
<p>这张幻灯片为我们揭示了“均衡学习器”求解随机博弈的最终数学形式，并一针见血地指出了它与我们熟悉的单智能体强化学习（如Q-learning）最根本的区别。</p>
<ol type="1">
<li><strong>核心思想：一个“自洽”的均衡解</strong></li>
</ol>
<p>这个方程组的核心思想是寻找一个<strong>“自洽的”
(self-consistent)</strong> 或 <strong>“不动点” (fixed point)</strong>
的解。这是什么意思呢？</p>
<p>一个策略组合 π∗
之所以被称为纳什均衡，是因为当所有人都采用这个策略时，它所产生的长期价值
(Q∗ 和 V∗)，反过来又会证明，继续沿用策略 π∗
对每个参与者来说都是最优的。没有任何人有单方面偏离的动机。</p>
<p>换句话说，策略 π∗ 产生了价值 V∗，而价值 V∗ 又反过来支撑了策略 π∗
的最优性。当策略和价值完美地相互印证、循环嵌套时，我们就找到了一个稳定的不动点，也就是纳什均衡。</p>
<ol start="2" type="1">
<li><strong>最关键的区别：如何从Q值计算V值</strong></li>
</ol>
<p>幻灯片的最后一句是理解多智能体学习复杂性的关键。它指出，这个方程组和单智能体最优方程（如Q-learning中的）非常像，唯一的区别在于<strong>如何从Q值计算出V值</strong>。</p>
<p>让我们做一个清晰的对比：</p>
<ul>
<li><strong>在单智能体MDP中 (Single-Agent MDP):</strong>
<ul>
<li>一个状态的价值
V∗(s)，等于在该状态下采取<strong>最优可能动作</strong>的Q值。</li>
<li><strong>计算方式</strong>: V∗(s)=maxaQ∗(s,a)</li>
<li><strong>逻辑</strong>:
“我是唯一的玩家，我当然会选择那个能让我长期收益最大化的动作。所以这个状态的价值就是我所有可选动作里最好的那个的价值。”
这是一个简单的<strong>最大化</strong>运算。</li>
</ul></li>
<li><strong>在多智能体随机博弈中 (Multi-Agent Stochastic Game):</strong>
<ul>
<li>一个状态的价值
Vi∗(s)，等于在该状态下，所有参与者玩“Q值博弈”所产生的<strong>纳什均衡结果</strong>中，参与者
<code>i</code> 能获得的期望收益。</li>
<li><strong>计算方式</strong>:
Vi∗(s)=NashEquilibriumValuei(Qi∗(s,⋅))</li>
<li><strong>逻辑</strong>:
“我只是众多玩家之一。我的收益不仅取决于我的动作，还取决于其他所有人的动作。在当前状态<code>s</code>下，所有可能的联合行动的Q值
Qi∗(s,a)
构成了一个新的‘即时’矩阵博弈。我不能简单地选一个收益最大的动作，因为那不一定是最终结果。这个状态的真实价值，是我在所有人都理性地进行这个‘Q值博弈’后，所能期望得到的均衡收益。”</li>
</ul></li>
</ul>
<ol start="3" type="1">
<li><strong>“嵌套”的复杂性</strong></li>
</ol>
<p>这个区别导致了多智能体学习在计算上的“嵌套式”困难。</p>
<ul>
<li>在单智能体的价值迭代中，每一步更新V值，只需要做一个简单的
<code>max</code> 运算。</li>
<li>但在多智能体的价值迭代中，为了更新一次V值，你需要在<strong>每一个状态
<code>s</code></strong>
下，都去<strong>求解一个完整的矩阵博弈</strong>（这个博弈的支付由上一轮迭代的Q值定义）。
<ul>
<li>如果是零和博弈，这个“内部循环”就是解一个线性规划。</li>
<li>如果是一般和博弈，这个“内部循环”就是解一个更复杂的LCP问题。</li>
</ul></li>
</ul>
<p>这使得整个算法的复杂度远高于单智能体的情况。</p>
<p><strong>总结：</strong></p>
<p>这张幻灯片给出了求解随机博弈均衡的最终数学形式。它看起来像我们熟悉的贝尔曼方程，但其核心的价值更新步骤——从Q值到V值的计算——隐藏着天壤之别。从一个简单的“取最大值”操作，升级为了一个复杂的“求解纳什均衡”操作，这正是多智能体系统复杂性的根源所在。</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250708205136340.png" alt="7-19">
<figcaption aria-hidden="true">7-19</figcaption>
</figure>
<p><strong>标题：Nash-Q：一种通用均衡学习器 (Nash-Q: general equilibrium
learner)</strong></p>
<ul>
<li><strong>Nash-Q</strong> 算法处理的是<strong>两人一般和博弈
(two-player general-sum games)</strong>。
<ul>
<li>— 使用<strong>二次规划 (quadratic programming)</strong>
来计算一般和均衡。</li>
<li>— 理论上仅限于<strong>单一均衡 (single equilibrium)</strong>
的情况。</li>
</ul></li>
</ul>
<hr>
<p>这张幻灯片介绍了一个在多智能体强化学习（MARL）领域具有里程碑意义的算法——<strong>Nash-Q
Learning</strong>。它是将单智能体的Q-learning成功扩展到多智能体博弈论场景的首次重要尝试之一，是典型的“均衡学习器”。</p>
<ol type="1">
<li><strong>Nash-Q 的核心思想：Q学习与纳什均衡的结合</strong></li>
</ol>
<p>Nash-Q算法的基本思路，是将我们之前讨论的两个概念——单智能体的Q值更新和博弈论的纳什均衡求解——无缝地结合起来。其学习更新的循环过程如下：</p>
<ol type="1">
<li><p>在当前状态 <code>s</code>，所有智能体采取一个联合行动
<code>a</code>。</p></li>
<li><p>系统转移到下一个状态 <code>s'</code>，并且每个智能体
<code>i</code> 获得一个即时回报 <code>rᵢ</code>。</p></li>
<li><p><strong>（纳什部分）</strong>：在新的状态
<code>s'</code>，智能体们会“在脑中”求解一个当前的“Q值博弈”。这个博弈的支付矩阵，就是它们当前估算的Q函数
Q(s′,⋅)。通过求解这个矩阵博弈的<strong>纳什均衡</strong>，它们能得到一个均衡状态下的期望收益，也就是我们上一张幻灯片里提到的均衡状态价值
Vi∗(s′)。</p></li>
<li><p>（Q学习部分）：每个智能体 i 使用这个计算出的均衡价值
Vi∗(s′)，来更新它在上一步的Q值
Qi(s,a)。更新公式与标准的Q-learning非常相似：</p>
<p>Qi(s,a)←(1−α)Qi(s,a)+α⋅[ri+γ⋅Vi∗(s′)]</p></li>
</ol>
<p>通过这个过程，智能体学习到的Q值，不仅仅反映了即时的回报，更反映了未来在理性博弈下的长期价值。</p>
<ol start="2" type="1">
<li><strong>数学工具的升级：从线性规划到二次规划</strong></li>
</ol>
<ul>
<li>在<strong>零和博弈</strong>中，双方利益完全对立，求解纳什均衡（即极小化极大值点）可以被优美地转化为一个<strong>线性规划
(LP)</strong> 问题。</li>
<li>但在<strong>一般和博弈</strong>中，双方有各自独立的目标（最大化自己的收益），利益不再完全对立。这种更复杂的关系使得问题无法再用线性规划来描述。</li>
<li><strong>二次规划 (Quadratic Programming, QP)</strong>
是一个更强大的优化工具，它可以用来求解某些特定形式的非线性问题。研究表明，寻找两人一般和博弈的纳什均衡，可以被转化为一个二次规划问题来求解。因此，Nash-Q算法在“纳什部分”的计算，就需要动用QP这个更高级的工具。</li>
</ul>
<ol start="3" type="1">
<li><strong>理论的“阿喀琉斯之踵”：单一均衡假设</strong></li>
</ol>
<p>这是Nash-Q算法最核心、也是最致命的局限。</p>
<ul>
<li><strong>问题所在</strong>：我们知道，一般和博弈（例如“性别大战”）经常拥有<strong>多个纳什均衡</strong>。</li>
<li><strong>算法的困境</strong>：在第3步更新时，算法需要一个<strong>唯一的</strong>均衡价值
Vi∗(s′)
来代入公式。如果求解“Q值博弈”后发现了多个纳什均衡，算法就“懵了”——它不知道应该用哪个均衡点的价值来进行下一步的Q值更新。
<ul>
<li>比如，如果智能体1选择了均衡A的价值来更新，而智能体2选择了均衡B的价值来更新，他们的学习目标就不一致，整个学习过程会变得不稳定，甚至无法收敛。</li>
</ul></li>
<li><strong>无奈的假设</strong>：为了保证理论上的收敛性，最初的Nash-Q算法不得不做出一个非常强的假设：<strong>在学习过程中的每一个阶段，所遇到的所有“Q值博弈”都恰好只有一个纳什均衡</strong>。这个假设在现实中很少成立，极大地限制了Nash-Q的直接应用范围。</li>
</ul>
<p>总结：</p>
<p>Nash-Q
是一个开创性的算法，它成功地搭建了从单智能体Q学习到多智能体博弈均衡学习的桥梁。它的主要贡献是提供了一套清晰的、理论驱动的MARL框架。然而，它的主要局限——“单一均衡假设”——也恰恰揭示了多智能体学习中一个最核心的难题：当存在多个可能的稳定状态时，一群独立的学习者如何能够相互协调，共同收敛到同一个目标上？
这个问题至今仍是MARL领域的研究热点。</p>
<ol start="4" type="1">
<li><strong>每个状态下，每个智能体的支付矩阵是不是不一样？</strong></li>
</ol>
<p><strong>简单直接的回答是：是的，在一般和博弈（General-Sum
Game）中，通常情况下，每个状态下每个智能体的支付矩阵都是不一样的。</strong></p>
<p>下面是更详细的解释：</p>
<ol type="1">
<li><strong>为什么会不一样？—— 利益不完全对立</strong></li>
</ol>
<p>在一个战略博弈中，每个参与者都有自己独立的支付函数（Payoff
Function），这个函数决定了在各种可能的结果下，他能获得多少收益。我们可以把这个函数想象成他的“记分牌”或“利益所在”。</p>
<ul>
<li><strong>在零和博弈中</strong>：情况非常特殊。双方的利益是<strong>完全对立</strong>的。你赢的，恰好就是我输的。所以，参与者2的支付矩阵（我们称之为<code>R₂</code>），恰好是参与者1支付矩阵（<code>R₁</code>）的负数，即
R2=−R1。从这个角度看，它们的矩阵虽然数值不同，但信息是完全对称的，知道一个就知道了另一个。</li>
<li><strong>在一般和博弈中</strong>：情况更为普遍和现实。参与者们的利益<strong>既不完全对立，也不完全一致</strong>。我赢得多，不一定意味着你输得多。我们可能双赢，也可能双输。</li>
</ul>
<ol start="2" type="1">
<li><strong>一个经典的例子：协调博弈（或性别大战）</strong></li>
</ol>
<p>让我们回顾一下之前幻灯片里的那个“协调博弈”的例子：</p>
<ul>
<li><p>参与者1 (行玩家) 的支付矩阵:</p>
<p>R1=(2001)</p></li>
<li><p>参与者2 (列玩家) 的支付矩阵:</p>
<p>R2=(1002)</p></li>
</ul>
<p>我们来看一下左上角的那个结果 <code>(行1, 列1)</code>:</p>
<ul>
<li>参与者1得到了 <strong>2</strong> 的收益。</li>
<li>参与者2得到了 <strong>1</strong> 的收益。</li>
</ul>
<p>他们的收益<strong>不相等</strong>，加起来也不等于零。这意味着，他们各自的支付矩阵
<code>R₁</code> 和 <code>R₂</code>
是<strong>两个完全不同的矩阵</strong>。这反映了他们有共同的目标（都想协调成功，避免得到0），但又有不同的偏好（参与者1更喜欢结果(2,1)，参与者2更喜欢结果(1,2)）。</p>
<ol start="3" type="1">
<li><strong>在随机博弈的框架下</strong></li>
</ol>
<p>问题是“每个状态下…”，</p>
<p>在一个<strong>一般和随机博弈 (General-Sum Stochastic Game)</strong>
中，每一个状态 <code>s</code> 本身就是一个子博弈。因此：</p>
<ul>
<li>在<strong>状态s₁</strong>下，参与者1和参与者2面对的是一组支付矩阵
(R1(s1,⋅),R2(s1,⋅))。</li>
<li>当他们转移到<strong>状态s₂</strong>后，他们面对的可能是另一组完全不同的支付矩阵
(R1(s2,⋅),R2(s2,⋅))。</li>
</ul>
<p>在每一种状态下，这两个矩阵 <code>R₁</code> 和 <code>R₂</code>
通常都是不同的，反映了在该特定情境下，双方复杂的、既有合作又有冲突的利益关系。</p>
<p><strong>总结：</strong></p>
<p>“每个智能体的支付矩阵不一样”这正是“一般和博弈”的核心特征。它打破了零和博弈中“我得即你失”的简单对称性，引入了更复杂的、更贴近现实世界的战略互动，也正是这种复杂性，使得求解一般和博弈的纳什均衡比求解零和博弈要困难得多。</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250709000352490.png" alt="7-20">
<figcaption aria-hidden="true">7-20</figcaption>
</figure>
<p><strong>上图：Nash-Q：一种通用均衡学习器</strong></p>
<ul>
<li><p>Q函数可以通过一个与标准Q学习非常相似的随机近似过程来估计：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">初始化 Q(s, a) // a是联合行动</span><br><span class="line">初始化 s</span><br><span class="line">循环：</span><br><span class="line">    aᵢ ← 从Q(s,a)导出的纳什策略的概率性结果 (混合了探索策略)</span><br><span class="line"></span><br><span class="line">    为每个玩家i采取行动aᵢ，观察回报rᵢ，下一状态s&#x27;以及其他玩家的联合行动a₋ᵢ</span><br><span class="line"></span><br><span class="line">    对于 i = 1...n:</span><br><span class="line">        Qᵢ(s,(aᵢ,a₋ᵢ)) ← (1-α)Qᵢ(s,(aᵢ,a₋ᵢ)) + α(rᵢ + γVᵢ(s&#x27;))</span><br><span class="line"></span><br><span class="line">    其中 V(s&#x27;) = Nash([Q(s&#x27;,a&#x27;)]) // V是Q值博弈的纳什均衡价值</span><br><span class="line"></span><br><span class="line">    s ← s&#x27;</span><br><span class="line">结束循环</span><br></pre></td></tr></table></figure>
<p><em>(左侧注释)</em>: 不再像Q学习中那样取“max”。</p></li>
</ul>
<p><strong>下图：Minimax-Q</strong></p>
<ul>
<li><p>Minimax-Q 被设计用于<strong>两人零和随机博弈</strong>。</p>
<ul>
<li>— 在零和博弈中，只有一个均衡。</li>
<li>— 它可以使用<strong>线性规划</strong>来找到。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">初始化 Q(s, &lt;a, o&gt;) 和 π(s)  // a是自己的行动, o是对手的行动</span><br><span class="line">循环：</span><br><span class="line">    a ← π(s) 的概率性结果 &#123;混合了探索策略&#125;</span><br><span class="line"></span><br><span class="line">    采取行动a，观察回报r，下一状态s&#x27;和对手的行动o</span><br><span class="line"></span><br><span class="line">    Q(s, (a, o)) ← (1-α)Q(s,(a,o)) + α(r + γV(s&#x27;))</span><br><span class="line"></span><br><span class="line">    其中 V(s&#x27;) = max_&#123;π&#x27;∈PD(A)&#125; min_&#123;o&#x27;∈O&#125; Σ_&#123;a&#x27;∈A&#125; π&#x27;(a&#x27;|s&#x27;) Q(s&#x27;, (a&#x27;, o&#x27;))</span><br><span class="line"></span><br><span class="line">    π(s) ← arg max_&#123;π&#x27;∈PD(A)&#125; min_&#123;o&#x27;∈O&#125; Σ_&#123;a&#x27;∈A&#125; π&#x27;(a&#x27;|s) Q(s, (a&#x27;, o&#x27;))</span><br><span class="line"></span><br><span class="line">    s ← s&#x27;</span><br><span class="line">结束循环</span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<p>这两张幻灯片介绍了多智能体强化学习（MARL）中两种奠基石级别的“均衡学习器”算法：<strong>Nash-Q</strong>
和
<strong>Minimax-Q</strong>。它们的核心思想都是一致的：<strong>将单智能体Q学习中的
<code>max</code> 操作，替换为一次博弈论的“均衡求解”操作</strong>。</p>
<p>尽管思想一致，但它们分别针对不同类型的博弈，这导致了它们在实现和特性上的巨大差异。</p>
<ol type="1">
<li><strong>Nash-Q：更通用的“多面手”</strong></li>
</ol>
<ul>
<li><strong>适用领域</strong>：两人<strong>一般和博弈</strong>。这是它的优势，因为它能处理更普遍的、合作与冲突并存的场景。</li>
<li><strong>核心计算</strong>：在每一步更新时，它都需要求解当前Q值矩阵的<strong>纳什均衡</strong>。这通常需要使用像<strong>二次规划
(QP)</strong> 这样的复杂优化工具。</li>
<li><strong>主要挑战</strong>：它最大的软肋在于，它<strong>假设</strong>在学习的每一步中，Q值博弈都恰好有<strong>唯一的纳什均衡</strong>。然而，一般和博弈经常出现多重均衡，这会让Nash-Q“不知所措”——它不知道应该用哪个均衡的价值来更新Q函数，从而可能导致学习过程不稳定或失败。</li>
</ul>
<ol start="2" type="1">
<li><strong>Minimax-Q：更稳健的“专家”</strong></li>
</ol>
<ul>
<li><strong>适用领域</strong>：两人<strong>零和博弈</strong>。这是它的局限，它只能用于利益完全对立的纯冲突场景。</li>
<li><strong>核心计算</strong>：在每一步更新时，它求解的是<strong>极大化极小值
(Maximin)</strong> 的解。这可以被高效地转化为一个<strong>线性规划
(LP)</strong> 问题来解决，计算上比二次规划更简单。</li>
<li><strong>主要优势</strong>：由于零和博弈的特性，其均衡价值（即博弈的价值V）是<strong>唯一且确定的</strong>。这彻底避免了Nash-Q遇到的多重均衡问题，使得Minimax-Q的理论基础更坚实，学习过程也更稳定。同时，它不仅更新Q值，还显式地更新了当前状态下的最优混合策略
<code>π(s)</code>。</li>
</ul>
<ol start="3" type="1">
<li><strong>对比总结</strong></li>
</ol>
<p>我们可以用一个表格来清晰地对比二者：</p>
<table>
<colgroup>
<col style="width: 21%">
<col style="width: 43%">
<col style="width: 34%">
</colgroup>
<thead>
<tr>
<th>特点 / Feature</th>
<th>Nash-Q</th>
<th>Minimax-Q</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>适用领域</strong></td>
<td>两人一般和博弈 (更通用)</td>
<td>两人零和博弈 (更专门)</td>
</tr>
<tr>
<td><strong>核心计算</strong></td>
<td>求解纳什均衡</td>
<td>求解极大化极小值</td>
</tr>
<tr>
<td><strong>计算工具</strong></td>
<td>二次规划 (QP)</td>
<td>线性规划 (LP)</td>
</tr>
<tr>
<td><strong>均衡唯一性</strong></td>
<td><strong>不保证</strong> (主要弱点)</td>
<td><strong>保证</strong> (主要优点)</td>
</tr>
<tr>
<td><strong>优点</strong></td>
<td>适用范围广</td>
<td>理论稳健，计算相对简单</td>
</tr>
<tr>
<td><strong>缺点</strong></td>
<td>依赖单一均衡假设，可能不稳定</td>
<td>只能用于纯冲突场景</td>
</tr>
</tbody>
</table>
<p>结论：</p>
<p>Nash-Q 和 Minimax-Q 体现了多智能体学习中一个经典的权衡：通用性
vs. 稳健性。</p>
<ul>
<li><strong>Nash-Q</strong>
像一把瑞士军刀，尝试解决所有问题，但因为它所依赖的“单一均衡”假设过于理想化，所以在很多复杂情况下会“卡壳”。</li>
<li><strong>Minimax-Q</strong>
像一把手术刀，只专注于零和博弈这一特定领域，但在该领域内，它的表现非常出色，理论保证强，结果稳定。</li>
</ul>
<p>这两种早期的算法为后来的多智能体强化学习研究奠定了基础，许多现代算法都是在试图克服它们的局限（尤其是Nash-Q的多重均衡问题）的道路上发展起来的。</p>
<h3 id="最佳对策">7.4 最佳对策</h3>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250709003711705.png" alt="7-21">
<figcaption aria-hidden="true">7-21</figcaption>
</figure>
<p><strong>标题：多智能体学习器的期望性质 (Desired Properties of
Multi-agent Learners)</strong></p>
<ul>
<li><p><strong>理性
(Rationality)</strong>：如果其他参与者的策略收敛到固定的策略，那么该学习算法也应收敛到一个作为对其他参与者策略的<strong>最佳应对
(best-response)</strong> 的策略。</p></li>
<li><p><strong>收敛性
(Convergence)</strong>：学习器必须能收敛到一个固定的策略。</p>
<ul>
<li><p>— 定义：一个针对参与者 i 的学习算法是收敛的，当且仅当对于任何 ε
&gt; 0，都存在一个时间 T &gt; 0，使得：</p>
<p>对于所有 t &gt; T, aᵢ ∈ Aᵢ, s ∈ S, 只要 P(s,t) &gt; 0，就有
|P(aᵢ|s,t) - π(s,aᵢ)| &lt; ε</p></li>
<li><p>— 其中 <code>P(s,t)</code> 是在时间t游戏处于状态s的概率，而
<code>P(aᵢ|s,t)</code>
是在时间t，给定游戏状态为s的条件下，该算法选择行动 <code>aᵢ</code>
的概率。</p></li>
</ul></li>
</ul>
<hr>
<p>这张幻灯片为我们评估一个多智能体学习（MARL）算法的好坏，提出了两个至关重要的标准或“期望的品质”。这可以看作是一份给智能体学习算法的“成绩单”，我们希望它既“聪明”（理性），又“稳重”（收敛）。</p>
<ol type="1">
<li><strong>理性 (Rationality)：抓住机会的能力</strong></li>
</ol>
<ul>
<li><strong>它是什么？</strong>
“理性”指的是一个学习算法<strong>利用对手弱点的能力</strong>。这个性质保证了，如果你的对手们不再学习和改变，而是开始使用一套固定的、可预测的（甚至可能是很差的）策略，那么你的算法应该足够聪明，能够发现这一点，并最终学会那套能最大化自身利益的<strong>完美反制策略</strong>。</li>
<li><strong>为什么重要？</strong>
这是一个智能体“智商”的基本体现。如果一个学习算法，连一个一成不变的、有明显弱点的对手都无法战胜，那它就算不上是一个好的学习算法。这个性质确保了智能体不会因为过于保守而错失良机。</li>
<li><strong>一个例子</strong>：在“石头剪刀布”中，如果你发现对手是一个只会出“石头”的“铁头娃”（一个固定的策略），那么一个具备理性的学习算法，其策略应该能很快地收敛到“永远出布”这个最佳应对上。</li>
</ul>
<ol start="2" type="1">
<li><strong>收敛性 (Convergence)：保持稳重的能力</strong></li>
</ol>
<ul>
<li><strong>它是什么？</strong>
“收敛性”指的是，<strong>无论对手们在做什么</strong>（哪怕他们在混乱地学习、策略变来变去），我们自己的学习算法的策略也应该能<strong>最终稳定下来</strong>，而不是永远地摇摆不定或无限循环下去。</li>
<li><strong>为什么重要？</strong>
这是<strong>稳定性和可预测性</strong>的保证。一个策略永远在振荡的智能体是不可靠的。我们希望我们的智能体，无论环境多么复杂，最终都能形成一套自洽的、固定的行为模式。没有收敛性，学习过程可能就只是一团毫无意义的混乱。</li>
</ul>
<ol start="3" type="1">
<li><strong>“理性”与“收敛性”之间的核心矛盾</strong></li>
</ol>
<p>在多智能体学习的设计中，这两个看似都非常理想的性质，实际上经常是<strong>相互冲突</strong>的。设计一个能同时完美满足这两点的算法是极其困难的，这也是MARL领域的核心挑战之一。</p>
<ul>
<li><p>过于追求“理性”的风险：</p>
<p>一个极度追求理性的智能体，会像一个“墙头草”，不断地去寻找对其他玩家当前策略的“最佳应对”。但问题是，其他玩家也在学习和变化。当你刚学会针对他们A策略的方法时，他们已经换成了B策略。这种永无休止的“追逐移动靶”，很容易导致策略的无限振荡，无法收敛。</p></li>
<li><p>过于追求“收敛性”的风险：</p>
<p>一个极度追求收敛的智能体，可能会像一个“顽固派”。它可能很快就锁定在一个“安全”的策略上（比如我们之前讨论的Minimax均衡策略），然后就一成不变，以确保自身的稳定。但这样做的代价是，即使它后来发现对手其实是个“铁头娃”，它也可能因为过于“稳重”而不愿意调整策略去利用这个弱点，从而失去了获得更高收益的机会，即丧失了理性。</p></li>
</ul>
<p>总结：</p>
<p>“理性”和“收敛性”构成了评估多智能体学习算法的一对核心指标。一个理想的算法，应该像一个经验丰富的拳击手，既有自己稳定扎实的基本功（收敛性），又能敏锐地发现并利用对手的破绽（理性）。在MARL的研究中，如何在二者之间找到精妙的平衡，是设计出更强大、更通用的智能体的关键所在。</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250709004802516.png" alt="7-22">
<figcaption aria-hidden="true">7-22</figcaption>
</figure>
<p>*<strong>（红色方框内的新增注释）*</strong></p>
<p><strong>收敛性</strong>通常是<strong>有条件的 (conditioned
on)</strong>，其条件与其他参与者的学习算法有关。例如，<strong>相对于理性参与者的收敛</strong>，或是在<strong>自我博弈（self-play）</strong>（所有参与者都使用相同的算法）中的收敛。</p>
<hr>
<p>这张幻灯片在上一张的基础上，对“收敛性”这个期望的性质增加了一个非常重要且符合现实的<strong>限定条件</strong>。它告诉我们，在复杂的多智能体世界中，我们通常无法奢求一个算法在任何情况下都保证收敛，而是会去证明它在<strong>某些特定的、合理的条件下</strong>是收敛的。</p>
<ol type="1">
<li><strong>为什么“无条件收敛”过于苛刻？</strong></li>
</ol>
<p>上一张幻灯片中定义的“学习器<strong>必须</strong>收敛到一个固定策略”是一个非常强的、近乎理想化的要求。</p>
<ul>
<li>想象一下，你要设计一个学习算法，并保证它<strong>无论和谁博弈</strong>都能最终稳定下来。</li>
<li>但如果你的对手是一个“捣乱者”，它的策略就是完全随机、毫无规律，甚至是故意要让你无法收敛呢？或者，如果你的对手也在学习，并且你们的互动陷入了一种类似“石头剪刀布”的无尽循环中呢？</li>
<li>在这种情况下，强求你的算法单方面地“稳定下来”是不现实的，甚至可能是有害的（因为它会变得僵化，容易被利用）。</li>
</ul>
<p>因此，在多智能体学习的研究中，学者们通常会退一步，去追求更有意义的<strong>“有条件收敛”</strong>。</p>
<ol start="2" type="1">
<li><strong>“有条件收敛”的两种典型情况</strong></li>
</ol>
<p>幻灯片中列举了两种最常见、也最重要的条件：</p>
<p><strong>A. 相对于理性参与者的收敛 (Convergence with respect to
rational players)</strong></p>
<ul>
<li><strong>含义</strong>：这个保证是说，“我的学习算法，在和一群同样是‘讲道理’、‘有理性’的玩家一起博弈时，是能够收敛的。”</li>
<li><strong>重要性</strong>：这证明了算法在一个“文明的”、可分析的多智能体环境中的稳定性。它排除了那些完全不可预测或恶意捣乱的对手，在一个更现实的“理性人”假设下，验证算法的可靠性。</li>
</ul>
<p><strong>B. 在自我博弈中的收敛 (Convergence in self-play)</strong></p>
<ul>
<li><strong>含义</strong>：这是目前多智能体强化学习研究中<strong>最常用</strong>的设定。“我的算法，在和它自己的<strong>克隆体</strong>进行博弈时，能够收敛到一个稳定的策略均衡。”</li>
<li><strong>重要性</strong>：这是一种“自我检验”。如果一个算法连和自己都玩不到一块儿去，学习过程都无法稳定，那很难说它是一个好的、可靠的算法。自我博弈创造了一个<strong>完美的、对称的</strong>实验环境，排除了因对手算法不同而带来的额外复杂性，让我们能纯粹地研究算法本身的动态特性。像
AlphaGo、AlphaStar 等许多著名的AI，其核心训练过程都是基于自我博弈。</li>
</ul>
<ol start="3" type="1">
<li><strong>总结：一个更务实的研究范式</strong></li>
</ol>
<p>这张幻灯片通过增加这个注释，向我们展示了科学研究中一个务实的思想：当一个问题在最普适的情况下难以解决时，我们可以先<strong>限定问题范围</strong>，在更有意义的特定条件下进行分析和证明。</p>
<ul>
<li><strong>无条件收敛</strong>：是“圣杯”，非常理想，但可能无法实现。</li>
<li><strong>有条件收敛</strong>：是一个更实际、更有价值的“里程碑”。</li>
</ul>
<p>在评估一个多智能体学习算法时，我们通常会问：“它能在自我博弈中收敛吗？”、“它能和理性学习者一起收敛吗？”、“它能收敛到好的均衡点上吗？”。一个算法能够提供的保证越强、越广泛，它就被认为越优秀。</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250709011441787.png" alt="7-23">
<figcaption aria-hidden="true">7-23</figcaption>
</figure>
<p><strong>标题：独立学习器（ILs）的困难 (Difficulty in Independent
learners (ILs))</strong></p>
<ul>
<li>假设其他智能体不在学习是不现实的。</li>
<li>如果参与者1正在执行均衡策略，另一方可能会选择一个确定性策略并获得相同的回报。</li>
<li>然而，一旦参与者2偏离了均衡，一个学习中的参与者1便可以利用这一事实，并采取某种能降低参与者2回报的策略。</li>
</ul>
<p><strong>图示文字翻译:</strong></p>
<ul>
<li><strong>右上角文字框</strong>:
参与者1利用了参与者2的确定性策略（注意 R2 = -R1）。</li>
<li><strong>右下角文字框</strong>:
参与者2偏离均衡，改为确定性策略，同时保持回报不变。</li>
<li><strong>图中</strong>: 标有“Nash”的点代表纳什均衡。</li>
</ul>
<hr>
<p>这张幻灯片非常深刻地揭示了为什么“独立学习器”（即每个智能体只顾自己学习，把对手当成环境背景）这种看似简单的方法，在实际中往往会失败。它描绘了一个<strong>“聪明的学习者”之间无法维持稳定均衡的动态过程</strong>。</p>
<p>我们可以把图中的动态过程分解为两步来理解：</p>
<p><strong>第一步：参与者2的“安全”偏离</strong></p>
<ol type="1">
<li><strong>初始状态</strong>：我们从“Nash”点开始。这是一个混合策略纳什均衡。在这个均衡点，参与者1采取他的最优混合策略（比如50%出招A，50%出招B）。</li>
<li><strong>无差异的诱惑</strong>：根据我们之前学到的“无差异原则”，当参与者1采取这个最优混合策略时，参与者2会发现，无论他选择“列1”还是“列2”，他的期望回报都是<strong>完全相同</strong>的。</li>
<li><strong>参与者2的决策</strong>：他心想：“既然我怎么选，回报都一样，那我何必费劲去搞一个复杂的随机策略呢？我就一直出‘列1’（一个确定性策略）好了，省事而且结果也一样。”</li>
<li><strong>图示</strong>：这对应了图中从“Nash”点出发的第一段横向箭头。参与者2的策略从混合策略变成了确定性策略，但因为无差异原则，他自己的回报（以及参与者1的回报）在<strong>这一刻</strong>保持不变。这看起来像是一次安全的、无害的简化。</li>
</ol>
<p><strong>第二步：参与者1的“理性”利用</strong></p>
<ol type="1">
<li><strong>发现机会</strong>：参与者1也是一个<strong>学习者</strong>，他会观察对手的行为。他很快就发现：“嘿！参与者2不再随机出招了，他现在只会出‘列1’，他的行为变得<strong>完全可预测</strong>了！”</li>
<li><strong>调整策略</strong>：此时，参与者1会反思：“既然我已经知道他只会出‘列1’，那我原来的那个50/50混合策略还是最优的吗？”
答案显然是“不是”。参与者1会立刻放弃他原来的均衡策略，转而采取一个专门针对“列1”的、能让自己收益最大化的<strong>最佳应对策略</strong>。</li>
<li><strong>图示</strong>：这对应了图中那条向上的、攀升的箭头。参与者1正在“爬”他的收益曲面，寻找针对参与者2这个固定策略的最高点。这个行动会<strong>极大地提高</strong>参与者1自己的回报<code>R1</code>。</li>
</ol>
<p><strong>后果：均衡的崩溃</strong></p>
<p>由于这是一个零和博弈（<code>R2 = -R1</code>），当参与者1的收益<code>R1</code>达到新的高峰时，就意味着参与者2的收益<code>R2</code>跌入了谷底。参与者2会发现，他最初那个看似“安全”的偏离行为，最终给自己带来了灾难性的后果。</p>
<p>接下来，一个学习中的参与者2会立刻放弃他那个被剥削的确定性策略，从而开始新一轮的策略调整。</p>
<p>总结：</p>
<p>这张幻灯片描绘了一个“信任的瓦解”过程。独立学习器无法形成稳定的合作（这里的合作指共同维持纳什均衡），因为：</p>
<ol type="1">
<li>纳什均衡的“无差异”特性，为其中一方“偷懒”或“偏离”提供了最初的动机。</li>
<li>独立学习器天生就具有“利用”对手的倾向，它会把对手任何可预测的行为都当作环境中的一个可利用的“漏洞”。</li>
<li>这种“偏离-利用-反制”的循环，使得双方的策略在纳什均衡点附近不停地追逐和振荡，但永远无法真正稳定下来。</li>
</ol>
<p>这正是独立学习范式的根本缺陷，也解释了为什么需要发展那些能明确处理智能体间互动的、更复杂的<strong>联合行动学习器（如Nash-Q）</strong>。</p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250709013039785.png" alt="7-24">
<figcaption aria-hidden="true">7-24</figcaption>
</figure>
<hr>
<p>这张幻灯片进一步深入探讨了<strong>联合行动学习器 (Joint Action
Learners, JALs)</strong>
的核心机制以及它们在现实中所面临的关键挑战。</p>
<ol type="1">
<li><strong>JAL 的核心思想回顾</strong></li>
</ol>
<p>JAL
的基本哲学是“直面博弈”，它承认在一个多智能体系统中，任何结果都由所有人的<strong>联合行动</strong>决定。因此，它的核心数据结构是一个基于联合行动的Q表格：Qi(s,a)，其中
a=(ai,a−i) 包含了自己和他人的行动。</p>
<ol start="2" type="1">
<li><strong>JAL 的两大实践挑战</strong></li>
</ol>
<p>幻灯片指出了将JAL付诸实践的两个主要障碍：</p>
<p><strong>挑战一：完全可观测性的要求</strong></p>
<ul>
<li><strong>问题</strong>：为了更新 Qi(s,(ai,a−i)) 这个值，智能体
<code>i</code> 在行动之后，不仅需要知道自己做了什么
(<code>aᵢ</code>)，还需要<strong>准确地观察到所有其他参与者刚才做了什么</strong>
(<code>a₋ᵢ</code>)。</li>
<li><strong>现实</strong>：在许多真实场景中，这个要求过于苛刻。例如，在商业竞争中，你可能知道自己的营销策略，也知道最终的市场份额变化，但你很难知道每一个竞争对手具体采取了什么内部策略。这种信息的缺失（即<strong>部分可观测性</strong>）使得最基础的JAL算法难以应用。</li>
</ul>
<p><strong>挑战二：参与者的异质性 (Heterogeneity)</strong></p>
<ul>
<li><strong>问题</strong>：像 Nash-Q
这样的均衡学习器，往往隐含地假设“我的对手也和我一样聪明，也在试图计算并执行纳什均衡”。</li>
<li><strong>现实</strong>：你的对手可能是五花八门的。他可能是一个正在学习的新手，一个遵循固定规则的“机器人”，或者一个完全非理性的玩家。你无法保证所有人都处在同一个“频道”上。当对手的行为模式不符合你的“理性人”假设时，你计算出的均衡策略可能就不是最优的。</li>
</ul>
<ol start="3" type="1">
<li><strong>核心决策机制：对他人建模</strong></li>
</ol>
<p>面对这些挑战，JAL
在做决策时通常采用一种更务实的方法，也就是幻灯片里给出的那个公式。这个公式背后的思想是<strong>“对其他参与者建模
(Opponent Modeling)”</strong>。</p>
<p>EV(ai)=a−i∈A−i∑Q(⟨ai,a−i⟩)j=i∏π^j(a−i[j])</p>
<p>我们可以这样解读这个决策过程：</p>
<ol type="1">
<li><strong>“如果我选择行动<code>aᵢ</code>…”</strong>：智能体
<code>i</code> 会在心里盘算自己的每一个可选行动 <code>aᵢ</code>。</li>
<li><strong>“…其他人可能会怎么做？”</strong>：它会根据自己对其他人的观察，为每一个对手
<code>j</code> 建立一个<strong>策略模型
π^j</strong>。这个模型预测了“对手<code>j</code>有多大概率会出什么招”。这可能通过统计他们过去的行为频率来学习。</li>
<li><strong>“…那么我的期望收益是多少？”</strong>：它会遍历所有其他人可能的行动组合
<code>a₋ᵢ</code>，用自己学到的Q值 Q(s,(ai,a−i))
乘以自己预测的“这种情况发生的概率” ∏π^j，然后求和。这就得到了采取行动
<code>aᵢ</code> 的总期望价值 EV(ai)。</li>
<li><strong>最终决策</strong>：计算出所有自己可选行动的EV值后，选择那个<strong>期望价值最高的行动</strong>来执行。</li>
</ol>
<p>总结：</p>
<p>JALs
提供了一个理论上更完备的MARL框架，但代价是面临着观测和扩展性的巨大挑战。幻灯片中的公式揭示了一种核心的实践方法：通过学习对手的模型来指导自己的决策。<strong>这使得
JAL
的研究分化为两个主要方向：一部分继续研究如何高效地进行均衡计算（如Nash-Q），另一部分则专注于如何更准确、更高效地进行对手建模，这两者共同推动着多智能体学习领域的发展。</strong></p>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250709014313654.png" alt="7-25">
<figcaption aria-hidden="true">7-25</figcaption>
</figure>
<p>这张幻灯片介绍了一类与“均衡学习器”（如Nash-Q）思想相对的、更具适应性的多智能体学习方法——<strong>对手建模
(Opponent Modeling)</strong>。这种方法的经典形式也被称为<strong>虚拟博弈
(Fictitious Play)</strong>。</p>
<ol type="1">
<li><strong>核心思想：“经验主义” vs. “理论主义”</strong></li>
</ol>
<p>我们可以把这两类学习器的哲学思想做一个对比：</p>
<ul>
<li><strong>均衡学习器
(如Nash-Q)</strong>：这是一个“理论主义者”。它假设对手是完全理性的博弈论家，其目标是计算出游戏抽象的、理论上的“纳什均衡”，然后去执行它。</li>
<li><strong>对手建模学习器</strong>：这是一个“经验主义者”。它不对对手做过高的理性假设，而是采取一种更务实的态度：“我不管你理论上该怎么玩，我就看你实际上是怎么玩的。”
它的目标是：
<ol type="1">
<li>通过观察，<strong>学习</strong>并建立一个关于对手行为模式的<strong>统计模型</strong>。</li>
<li>计算并执行针对这个“模型”的<strong>最佳应对策略</strong>。</li>
</ol></li>
</ul>
<figure>
<img src="/2025/06/22/Multi-Agent-AI/image-20250709014504249.png" alt="7-26">
<figcaption aria-hidden="true">7-26</figcaption>
</figure>
<ol start="3" type="1">
<li><strong>“巨型对手”假设的含义</strong></li>
</ol>
<p>红色注释指出了这个特定算法实现的一个简化之处：它把所有其他参与者
<code>a₋ᵢ</code>
打包看作是一个“巨型对手”。它统计的是对手们“集体上”做了什么，而不是去为每一个对手
<code>j</code> 单独建模。</p>
<ul>
<li><strong>优点</strong>：简化了问题，需要维护的计数器更少。</li>
<li><strong>缺点</strong>：模型较为粗糙，无法捕捉不同对手之间的个性化差异或他们之间可能存在的联动关系。更高级的对手建模算法会为每一个对手分别建立策略模型。</li>
</ul>
<ol start="4" type="1">
<li><strong>优缺点分析</strong></li>
</ol>
<ul>
<li><strong>优点</strong>：
<ul>
<li><strong>满足“理性”</strong>：这种方法的核心就是计算最佳应对，所以它天生就满足我们之前讨论的“理性”性质。如果对手采取一个固定的、有漏洞的策略，这个算法能很快地学习到并加以利用。</li>
<li><strong>计算更简单</strong>：在决策时，它只需要做一次期望值计算，而不需要像Nash-Q那样在每一步都去求解一个复杂的均衡（如二次规划），计算上更高效。</li>
</ul></li>
<li><strong>缺点</strong>：
<ul>
<li><strong>不保证“收敛”</strong>：这是它最大的问题。它假设对手的策略是固定的，但如果对手也是一个同样的学习者，双方就会陷入“你根据我的历史来预测我，我根据你的历史来预测你”的循环。在很多非零和博弈中（比如石头剪刀布），这种相互预测和应对会导致双方的策略永远振荡，无法收敛到纳什均衡。</li>
<li><strong>学习滞后</strong>：如果对手的策略变化很快，这种基于历史频率的建模方法会存在明显的滞后，导致它总是在应对一个“过去的”对手，从而表现不佳。</li>
</ul></li>
</ul>
<p>总结：</p>
<p>对手建模/虚拟博弈是多智能体学习中一种非常重要且直观的方法。它放弃了均衡学习器对“完美理性”的强假设，转而采用一种数据驱动、经验主义的方式来学习和适应。它的优势在于能够利用对手的弱点（满足理性），但代价是牺牲了收敛性的保证。</p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/05/15/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRESENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/" rel="prev" title="ATTENTION-GUIDED CONTRASTIVE ROLE REPRESENTATIONS FOR MULTI-AGENT REINFORCEMENT LEARNING">
                  <i class="fa fa-angle-left"></i> ATTENTION-GUIDED CONTRASTIVE ROLE REPRESENTATIONS FOR MULTI-AGENT REINFORCEMENT LEARNING
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/07/15/TRPO/" rel="next" title="TRPO">
                  TRPO <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">GGBond</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"mhchem":false,"cdn":"https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML","src":"custom_mathjax_source","js":{"url":"//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
